"""
æ•°æ®åˆ†ææ¨¡å— - å‘¨æŠ¥ç»Ÿè®¡åˆ†æ
"""
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import sqlite3
from .db import load_data_from_db
from .config import DB_PATH


# ç™¾åº¦å®˜æ–¹æ¨¡å‹è¯†åˆ«è§„åˆ™
OFFICIAL_RULES = {
    'Hugging Face': 'baidu',
    'AI Studio': 'PaddlePaddle',
    'ModelScope': 'é£æ¡¨PaddlePaddle',
    'GitCode': 'é£æ¡¨PaddlePaddle',
    'é²¸æ™º': 'PaddlePaddle',
    'é­”ä¹ Modelers': 'PaddlePaddle',
    'Gitee': 'PaddlePaddle'
}

# æ¨¡å‹é¡ºåºï¼ˆæŒ‰é‡è¦æ€§æ’åˆ—ï¼‰
MODEL_ORDER = [
    'ERNIE-4.5-VL-424B-A47B-Paddle',
    'ERNIE-4.5-VL-424B-A47B-PT',
    'ERNIE-4.5-VL-424B-A47B-Base-Paddle',
    'ERNIE-4.5-VL-424B-A47B-Base-PT',
    'ERNIE-4.5-300B-A47B-Paddle',
    'ERNIE-4.5-300B-A47B-PT',
    'ERNIE-4.5-300B-A47B-Base-Paddle',
    'ERNIE-4.5-300B-A47B-Base-PT',
    'ERNIE-4.5-300B-A47B-W4A8C8-TP4-Paddle',
    'ERNIE-4.5-300B-A47B-FP8-Paddle',
    'ERNIE-4.5-300B-A47B-2Bits-Paddle',
    'ERNIE-4.5-300B-A47B-2Bits-TP2-Paddle',
    'ERNIE-4.5-300B-A47B-2Bits-TP4-Paddle',
    'ERNIE-4.5-VL-28B-A3B-Paddle',
    'ERNIE-4.5-VL-28B-A3B-PT',
    'ERNIE-4.5-VL-28B-A3B-Thinking',
    'ERNIE-4.5-VL-28B-A3B-Base-Paddle',
    'ERNIE-4.5-VL-28B-A3B-Base-PT',
    'ERNIE-4.5-21B-A3B-Paddle',
    'ERNIE-4.5-21B-A3B-PT',
    'ERNIE-4.5-21B-A3B-Thinking',
    'ERNIE-4.5-21B-A3B-Base-Paddle',
    'ERNIE-4.5-21B-A3B-Base-PT',
    'ERNIE-4.5-0.3B-Paddle',
    'ERNIE-4.5-0.3B-PT',
    'ERNIE-4.5-0.3B-Base-Paddle',
    'ERNIE-4.5-0.3B-Base-PT'
]

PADDLEOCR_VL_MODEL_ORDER = [
    'PaddleOCR-VL'
]

# å¹³å°é¡ºåº
REPO_ORDER = ['Hugging Face', 'AI Studio', 'ModelScope', 'GitCode', 'å…¶ä»–']

# è¯¦ç»†å¹³å°é¡ºåºï¼ˆä¸åˆå¹¶"å…¶ä»–"ï¼‰
REPO_ORDER_DETAILED = ['Hugging Face', 'AI Studio', 'ModelScope', 'GitCode', 'é­”ä¹ Modelers', 'é²¸æ™º', 'Gitee']


def get_last_friday(current_date=None):
    """
    è·å–ä¸Šå‘¨äº”çš„æ—¥æœŸ

    Args:
        current_date: å½“å‰æ—¥æœŸï¼Œé»˜è®¤ä¸ºä»Šå¤©

    Returns:
        str: ä¸Šå‘¨äº”çš„æ—¥æœŸå­—ç¬¦ä¸² (YYYY-MM-DD)
    """
    if current_date is None:
        current_date = datetime.now()
    elif isinstance(current_date, str):
        current_date = datetime.strptime(current_date, '%Y-%m-%d')

    # è·å–å½“å‰æ˜¯æ˜ŸæœŸå‡  (0=Monday, 6=Sunday)
    current_weekday = current_date.weekday()

    # è®¡ç®—åˆ°ä¸Šå‘¨äº”çš„å¤©æ•°
    # å¦‚æœä»Šå¤©æ˜¯å‘¨ä¸€(0)ï¼Œä¸Šå‘¨äº”æ˜¯3å¤©å‰
    # å¦‚æœä»Šå¤©æ˜¯å‘¨äº”(4)ï¼Œä¸Šå‘¨äº”æ˜¯7å¤©å‰
    if current_weekday >= 4:  # å‘¨äº”ã€å‘¨å…­ã€å‘¨æ—¥
        days_to_last_friday = current_weekday - 4 + 7
    else:  # å‘¨ä¸€åˆ°å‘¨å››
        days_to_last_friday = current_weekday + 3

    last_friday = current_date - timedelta(days=days_to_last_friday)
    return last_friday.strftime('%Y-%m-%d')


def get_available_dates():
    """
    è·å–æ•°æ®åº“ä¸­æ‰€æœ‰å¯ç”¨çš„æ—¥æœŸ

    Returns:
        list: æ—¥æœŸåˆ—è¡¨
    """
    conn = sqlite3.connect(DB_PATH)
    query = "SELECT DISTINCT date FROM model_downloads ORDER BY date DESC"
    df = pd.read_sql_query(query, conn)
    conn.close()
    return df['date'].tolist()


def normalize_model_names(data):
    """
    æ ‡å‡†åŒ–æ¨¡å‹åç§°ï¼šç§»é™¤ model_name ä¸­çš„ publisher å‰ç¼€

    ä¾‹å¦‚ï¼š'paddlepaddle/ERNIE-4.5-0.3B-PT' -> 'ERNIE-4.5-0.3B-PT'

    è¿™ç¡®ä¿äº†å³ä½¿æ•°æ®åº“ä¸­å­˜å‚¨çš„æ¨¡å‹åç§°æ ¼å¼ä¸ä¸€è‡´ï¼Œ
    åœ¨åˆ†ææ—¶ä¹Ÿèƒ½æ­£ç¡®åŒ¹é…å’Œæ¯”è¾ƒã€‚
    """
    data = data.copy()

    def remove_publisher_prefix(row):
        model_name = str(row['model_name']).strip()
        publisher = str(row['publisher']).strip()

        # å¦‚æœæ¨¡å‹åç§°ä»¥ "publisher/" å¼€å¤´ï¼ˆå¿½ç•¥å¤§å°å†™ï¼‰ï¼Œç§»é™¤å‰ç¼€
        if publisher and publisher.lower() != 'nan' and '/' in model_name:
            parts = model_name.split('/', 1)
            if len(parts) == 2 and parts[0].lower() == publisher.lower():
                return parts[1]

        return model_name

    data['model_name'] = data.apply(remove_publisher_prefix, axis=1)
    return data


def mark_official_models(data):
    """
    æ ‡è®°å®˜æ–¹æ¨¡å‹ã€‚
    å¦‚æœ publisher åŒ…å« 'ç™¾åº¦', 'baidu', æˆ– 'Paddle' (ä¸åŒºåˆ†å¤§å°å†™)ï¼Œåˆ™è§†ä¸ºå®˜æ–¹æ¨¡å‹ã€‚
    """
    data = data.copy()
    # ç¡®ä¿ publisher åˆ—æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œä»¥ä¾¿è¿›è¡Œæ–‡æœ¬æ“ä½œ
    data['publisher'] = data['publisher'].astype(str)

    keywords = ['ç™¾åº¦', 'baidu', 'Paddle', 'yiyan', 'ä¸€è¨€']
    # åˆ›å»ºä¸€ä¸ªæ­£åˆ™è¡¨è¾¾å¼ï¼Œç”¨ | (OR) è¿æ¥å…³é”®å­—
    pattern = '|'.join(keywords)

    # ä½¿ç”¨ str.contains è¿›è¡Œä¸åŒºåˆ†å¤§å°å†™çš„åŒ¹é…
    data['is_official'] = data['publisher'].str.contains(pattern, case=False, na=False)

    return data


def create_pivot_table(data, repo_order=None, model_order=None, group_by_publisher=False, merge_other=True):
    """
    åˆ›å»ºæ•°æ®é€è§†è¡¨

    Args:
        data: DataFrame
        repo_order: å¹³å°é¡ºåºåˆ—è¡¨
        model_order: æ¨¡å‹é¡ºåºåˆ—è¡¨. å¦‚æœä¸º None, åˆ™ä¸æŒ‰ç‰¹å®šæ¨¡å‹é¡ºåºå¤„ç†.
        group_by_publisher: æ˜¯å¦æŒ‰ publisher åˆ†ç»„ï¼ˆç”¨äºè¡ç”Ÿæ¨¡å‹ï¼‰ã€‚
                           å¦‚æœä¸º Trueï¼Œç´¢å¼•ä¸º (model_name, publisher)ï¼›
                           å¦‚æœä¸º Falseï¼Œç´¢å¼•ä»…ä¸º model_nameï¼ˆç”¨äºå®˜æ–¹æ¨¡å‹ï¼‰
        merge_other: æ˜¯å¦åˆå¹¶é­”ä¹ Modelersã€é²¸æ™ºã€Giteeä¸º"å…¶ä»–"ï¼ˆé»˜è®¤Trueï¼‰

    Returns:
        DataFrame: é€è§†è¡¨
    """
    if repo_order is None:
        repo_order = REPO_ORDER if merge_other else REPO_ORDER_DETAILED

    # ç¡®ä¿ download_count æ˜¯æ•°å€¼ç±»å‹
    data = data.copy()
    data['download_count'] = pd.to_numeric(data['download_count'], errors='coerce').fillna(0)

    # åˆå¹¶å¹³å°ï¼ˆä»…å½“ merge_other=True æ—¶ï¼‰
    if merge_other:
        data['repo'] = data['repo'].replace(['é­”ä¹ Modelers', 'é²¸æ™º', 'Gitee'], 'å…¶ä»–')

    # æ ¹æ® group_by_publisher å†³å®šç´¢å¼•
    if group_by_publisher:
        # è¡ç”Ÿæ¨¡å‹ï¼šä½¿ç”¨ (model_name, publisher) ä½œä¸ºç´¢å¼•
        pivot_df = pd.pivot_table(
            data,
            values='download_count',
            index=['model_name', 'publisher'],
            columns='repo',
            aggfunc='sum',
            fill_value=0
        )
    else:
        # å®˜æ–¹æ¨¡å‹ï¼šä½¿ç”¨ model_name ä½œä¸ºç´¢å¼•
        pivot_df = pd.pivot_table(
            data,
            values='download_count',
            index='model_name',
            columns='repo',
            aggfunc='sum',
            fill_value=0
        )

    # ç¡®ä¿æ‰€æœ‰å¹³å°éƒ½åœ¨åˆ—ä¸­
    for repo in repo_order:
        if repo not in pivot_df.columns:
            pivot_df[repo] = 0

    # æŒ‰æŒ‡å®šé¡ºåºæ’åˆ—åˆ—
    available_repos = [repo for repo in repo_order if repo in pivot_df.columns]
    pivot_df = pivot_df[available_repos]

    # å¦‚æœæä¾›äº† model_orderï¼Œåˆ™æŒ‰å…¶å¤„ç†
    if model_order:
        # ç¡®ä¿æ‰€æœ‰æŒ‡å®šæ¨¡å‹éƒ½åœ¨ç´¢å¼•ä¸­
        for model in model_order:
            if model not in pivot_df.index:
                pivot_df.loc[model] = [0] * len(available_repos)

        # æŒ‰æŒ‡å®šé¡ºåºæ’åˆ—è¡Œ
        available_models = [model for model in model_order if model in pivot_df.index]
        pivot_df = pivot_df.reindex(available_models)

    return pivot_df


def get_all_new_models(current_date, previous_date, model_series='ERNIE-4.5'):
    """
    è·å–æœ¬å‘¨æ–°å¢çš„æ‰€æœ‰æ¨¡å‹ï¼ˆå®Œæ•´åˆ—è¡¨ï¼‰

    ğŸ”§ ä¿®å¤ï¼šç›´æ¥ä»æ•°æ®åº“åŠ è½½æ•°æ®ï¼Œç¡®ä¿ä¸ get_weekly_new_finetune_adapters() ä½¿ç”¨ç›¸åŒçš„ç­›é€‰é€»è¾‘

    Args:
        current_date: å½“å‰æ—¥æœŸ (YYYY-MM-DD)
        previous_date: å¯¹æ¯”æ—¥æœŸ (YYYY-MM-DD)
        model_series: æ¨¡å‹ç³»åˆ— ('ERNIE-4.5' æˆ– 'PaddleOCR-VL')

    Returns:
        dict: åŒ…å«æ–°å¢æ¨¡å‹ä¿¡æ¯çš„å­—å…¸
    """
    try:
        # ä»æ•°æ®åº“åŠ è½½æ•°æ®ï¼ˆä¸ get_weekly_new_finetune_adapters ä½¿ç”¨ç›¸åŒçš„åŠ è½½æ–¹å¼ï¼‰
        current_data = load_data_from_db(date_filter=current_date)
        previous_data = load_data_from_db(date_filter=previous_date)

        if current_data.empty:
            return {
                'new_models_list': [],
                'total_new': 0,
                'summary': f'æœ¬å‘¨æ²¡æœ‰æ–°å¢{model_series}æ¨¡å‹'
            }

        # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ä¸ get_weekly_new_finetune_adapters() ç›¸åŒçš„ç­›é€‰é€»è¾‘
        # æ ¹æ® model_series ç¡®å®šè¦ç­›é€‰çš„ model_category
        if model_series == 'ERNIE-4.5':
            target_category = 'ernie-4.5'
        elif model_series == 'PaddleOCR-VL':
            target_category = 'paddleocr-vl'
        else:
            target_category = 'ernie-4.5'

        # ğŸ”´ å…³é”®ä¿®å¤ï¼šå…ˆæŒ‰ model_category ç­›é€‰æ¨¡å‹ç³»åˆ—ï¼Œå†åˆ¤æ–­æ–°å¢
        # æ–°å¢åˆ¤æ–­åªçœ‹ (repo, publisher, model_name) ä¸‰å…ƒç»„ï¼Œä¸å— model_category ç¼ºå¤±å½±å“
        # ç­›é€‰ç­–ç•¥ï¼šmodel_category æ­£ç¡® OR model_name åŒ…å«å…³é”®è¯
        if model_series == 'ERNIE-4.5':
            name_pattern = 'ERNIE-4.5'
        else:  # PaddleOCR-VL
            name_pattern = 'PaddleOCR-VL'

        # ä½¿ç”¨ model_category OR model_name ç­›é€‰ï¼Œç¡®ä¿ä¸é—æ¼å›  model_category ç¼ºå¤±çš„æ¨¡å‹
        hf_current = current_data[
            (current_data['repo'] == 'Hugging Face') & (
                (current_data['model_category'] == target_category) |
                (current_data['model_name'].str.contains(name_pattern, case=False, na=False))
            )
        ].copy()

        if previous_data.empty:
            hf_previous = pd.DataFrame()
        else:
            # ğŸ”´ å…³é”®ä¿®å¤ï¼šprevious_date ä½¿ç”¨ç›¸åŒçš„ç­›é€‰é€»è¾‘
            # è¿™æ ·å³ä½¿ä¹‹å‰çš„æ•°æ® model_category ä¸ºç©ºï¼Œä¹Ÿèƒ½é€šè¿‡ model_name åŒ¹é…è¯†åˆ«å·²å­˜åœ¨çš„æ¨¡å‹
            hf_previous = previous_data[
                (previous_data['repo'] == 'Hugging Face') & (
                    (previous_data['model_category'] == target_category) |
                    (previous_data['model_name'].str.contains(name_pattern, case=False, na=False))
                )
            ].copy()

        # æ‰¾å‡ºåœ¨å½“å‰æ•°æ®ä¸­ä½†ä¸åœ¨å¯¹æ¯”æ•°æ®ä¸­çš„æ¨¡å‹ï¼ˆæŒ‰ publisher+model_name å»é‡ï¼‰
        if hf_previous.empty:
            new_models = hf_current.copy()
        else:
            previous_keys = set(zip(hf_previous['publisher'], hf_previous['model_name']))
            current_keys = set(zip(hf_current['publisher'], hf_current['model_name']))
            new_keys = current_keys - previous_keys
            new_models = hf_current[hf_current.apply(lambda r: (r['publisher'], r['model_name']) in new_keys, axis=1)].copy()

        if new_models.empty:
            return {
                'new_models_list': [],
                'total_new': 0,
                'summary': f'æœ¬å‘¨æ²¡æœ‰æ–°å¢{model_series}æ¨¡å‹'
            }

        # æŒ‰å¹³å°åˆ†ç»„ï¼Œæ¯ä¸ªæ¨¡å‹åªä¿ç•™ä¸€æ¡è®°å½•ï¼ˆé€‰æ‹©ä¸‹è½½é‡æœ€å¤§çš„å¹³å°ï¼‰
        new_models_dedup = new_models.sort_values('download_count', ascending=False).drop_duplicates(
            subset=['publisher', 'model_name'], keep='first'
        )

        # æ ¼å¼åŒ–æ¨¡å‹åˆ—è¡¨ï¼ŒåŒ…å«æ›´å¤šä¿¡æ¯
        models_list = []
        for _, row in new_models_dedup.iterrows():
            model_info = {
                'model_name': row['model_name'],
                'publisher': row['publisher'],
                'repo': row['repo'],
                'download_count': int(row['download_count']),
            }

            # æ·»åŠ å¯é€‰å­—æ®µ
            if 'model_type' in row and pd.notna(row['model_type']):
                model_info['model_type'] = row['model_type']
            if 'model_category' in row and pd.notna(row['model_category']):
                model_info['model_category'] = row['model_category']
            if 'base_model' in row and pd.notna(row['base_model']):
                model_info['base_model'] = row['base_model']

            models_list.append(model_info)

        # æŒ‰ä¸‹è½½é‡é™åºæ’åº
        models_list = sorted(models_list, key=lambda x: x['download_count'], reverse=True)

        return {
            'new_models_list': models_list,
            'total_new': len(models_list),
            'summary': f'æœ¬å‘¨å…±å‘ç° {len(models_list)} ä¸ªæ–°å¢{model_series}æ¨¡å‹'
        }

    except Exception as e:
        print(f"è·å–æœ¬å‘¨æ–°å¢æ¨¡å‹å®Œæ•´åˆ—è¡¨å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {
            'new_models_list': [],
            'total_new': 0,
            'summary': f'è·å–æ•°æ®æ—¶å‡ºé”™: {e}'
        }


def calculate_weekly_report(current_date=None, previous_date=None, model_order=None, model_series='ERNIE-4.5'):
    """
    è®¡ç®—å‘¨æŠ¥æ•°æ®

    Args:
        current_date: å½“å‰æ—¥æœŸ (YYYY-MM-DD)ï¼Œé»˜è®¤ä¸ºä»Šå¤©
        previous_date: å¯¹æ¯”æ—¥æœŸ (YYYY-MM-DD)ï¼Œé»˜è®¤ä¸ºä¸Šå‘¨äº”
        model_order: æ¨¡å‹é¡ºåºåˆ—è¡¨
        model_series: æ¨¡å‹ç³»åˆ— ('ERNIE-4.5' æˆ– 'PaddleOCR-VL')

    Returns:
        dict: åŒ…å«å„ç§ç»Ÿè®¡æ•°æ®çš„å­—å…¸
    """
    if model_order is None:
        model_order = MODEL_ORDER if model_series == 'ERNIE-4.5' else PADDLEOCR_VL_MODEL_ORDER

    # è®¾ç½®æ—¥æœŸ
    if current_date is None:
        current_date = datetime.now().strftime('%Y-%m-%d')
    if previous_date is None:
        previous_date = get_last_friday(current_date)

    # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ load_data_from_db() è·å–å»é‡åçš„æ•°æ®
    # è¿™ç¡®ä¿äº†é‡å¤è®°å½•åªå–æœ€å¤§ä¸‹è½½é‡ï¼Œé¿å…é‡å¤è®¡ç®—
    # å®˜æ–¹/éå®˜æ–¹çš„å½“æ—¥ç»Ÿè®¡éƒ½åº”ä½¿ç”¨å½“å¤©è®°å½•ï¼Œä¸åšâ€œå–æœ€è¿‘æœ‰å€¼â€å›å¡«
    current_data = load_data_from_db(date_filter=current_date, last_value_per_model=False)
    previous_data = load_data_from_db(date_filter=previous_date, last_value_per_model=False)

    # è´Ÿå¢é•¿æ£€æµ‹ä½¿ç”¨çœŸå®çš„å½“æ—¥è®°å½•ï¼ˆä¸å¸¦ last_value_per_modelï¼‰ï¼Œå•ç‹¬åŠ è½½
    warn_current_raw = load_data_from_db(date_filter=current_date, last_value_per_model=False)
    warn_previous_raw = load_data_from_db(date_filter=previous_date, last_value_per_model=False)

    # ğŸ”´ å…³é”®ä¿®å¤ï¼šåœ¨åˆå¹¶å’Œè¿›ä¸€æ­¥å¤„ç†ä¹‹å‰ï¼Œå¯¹æ•°æ®è¿›è¡Œå¼ºåˆ¶æ ‡å‡†åŒ–å’ŒäºŒæ¬¡å»é‡
    # ç¡®ä¿å³ä½¿æ•°æ®åº“ä¸­å­˜åœ¨ä¸ä¸€è‡´ï¼Œä¹Ÿèƒ½åœ¨åˆ†ææ—¶å¾—åˆ°ä¿®æ­£
    def enforce_deduplication_and_standardization(df):
        if df.empty:
            return df
        
        # 1. æ ‡å‡†åŒ– publisher åç§°ï¼ˆç»Ÿä¸€å¤§å°å†™ï¼‰
        df['publisher'] = df['publisher'].astype(str).apply(lambda x: x.title() if x.lower() != 'nan' else x)
        
        # 2. æ ‡å‡†åŒ–æ¨¡å‹åç§°ï¼ˆç§»é™¤ publisher å‰ç¼€ï¼‰
        df = normalize_model_names(df)
        
        # 3. å†æ¬¡å»é‡ï¼Œç¡®ä¿åŒä¸€ (date, repo, publisher, model_name) åªæœ‰ä¸€æ¡è®°å½•ï¼Œä¸”ä¸‹è½½é‡æœ€å¤§
        # æŒ‰ç…§ download_count é™åºæ’åºï¼Œç„¶åä¿ç•™æ¯ä¸ªåˆ†ç»„çš„ç¬¬ä¸€ä¸ª
        df['download_count'] = pd.to_numeric(df['download_count'], errors='coerce').fillna(0)
        df = df.sort_values(by='download_count', ascending=False).drop_duplicates(
            subset=['date', 'repo', 'publisher', 'model_name'], keep='first'
        )
        return df

    current_data = enforce_deduplication_and_standardization(current_data)
    previous_data = enforce_deduplication_and_standardization(previous_data)
    warn_current_raw = enforce_deduplication_and_standardization(warn_current_raw)
    warn_previous_raw = enforce_deduplication_and_standardization(warn_previous_raw)

    # åˆå¹¶ä¸¤ä¸ªæ—¥æœŸçš„æ•°æ®
    data = pd.concat([current_data, previous_data], ignore_index=True)

    if data.empty:
        return None

    # ğŸ”´ é‡è¦ï¼šæ ‡å‡†åŒ–æ¨¡å‹åç§°ï¼ˆç§»é™¤ publisher å‰ç¼€ï¼‰
    # è¿™ä¸€æ­¥åœ¨ enforce_deduplication_and_standardization ä¸­å·²ç»å®Œæˆï¼Œæ­¤å¤„å¯ä»¥ç§»é™¤æˆ–ä¿ç•™ä½œä¸ºå†—ä½™æ£€æŸ¥
    # ä¸ºäº†é¿å…é‡å¤å¤„ç†ï¼Œæ­¤å¤„ä¸å†é‡å¤è°ƒç”¨ normalize_model_names
    # data = normalize_model_names(data)

    def filter_by_series(df):
        """æŒ‰ç³»åˆ—è¿‡æ»¤æ•°æ®ï¼Œç”¨äºå®˜æ–¹ä¸è¡ç”Ÿå…±ç”¨çš„ç­›é€‰é€»è¾‘ã€‚"""
        if df.empty:
            return df
        if model_series == 'ERNIE-4.5':
            if 'model_category' in df.columns:
                condition = (
                    (df['model_category'] == 'ernie-4.5') |
                    (df['model_name'].str.contains('ERNIE-4.5', case=False, na=False))
                )
                return df[condition].copy()
            return df[df['model_name'].str.contains('ERNIE-4.5', case=False, na=False)].copy()
        if model_series == 'PaddleOCR-VL':
            if 'model_category' in df.columns:
                condition = (
                    (df['model_category'] == 'paddleocr-vl') |
                    (df['model_name'].str.contains('PaddleOCR-VL', case=False, na=False))
                )
                return df[condition].copy()
            return df[df['model_name'].str.contains('PaddleOCR-VL', case=False, na=False)].copy()
        return df

    # ğŸ”§ ä¿®å¤ï¼šæ ¹æ® model_series ä½¿ç”¨ model_category å­—æ®µ **æˆ–** æ¨¡å‹åç§°ç­›é€‰
    # è¿™æ ·æ—¢èƒ½åŒ…å«æ­£ç¡®åˆ†ç±»çš„è¡ç”Ÿæ¨¡å‹ï¼Œä¹Ÿèƒ½åŒ…å«å…¶ä»–å¹³å°çš„å®˜æ–¹æ¨¡å‹
    data = filter_by_series(data)

    if data.empty:
        print(f"è­¦å‘Š: åœ¨é€‰å®šæ—¥æœŸå†…æœªæ‰¾åˆ° {model_series} ç³»åˆ—çš„æ¨¡å‹æ•°æ®ã€‚")
        return None

    # ç¡®ä¿ 'download_count' æ˜¯æ•°å€¼ç±»å‹
    data['download_count'] = pd.to_numeric(data['download_count'], errors='coerce').fillna(0)

    # æ ‡è®°å®˜æ–¹æ¨¡å‹
    data = mark_official_models(data)
    # è´Ÿå¢é•¿æ£€æµ‹ç”¨çš„åŸå§‹å½“æ—¥æ•°æ®ä¹Ÿéœ€è¦å®˜æ–¹æ ‡è®°
    warn_current_raw = mark_official_models(warn_current_raw)
    warn_previous_raw = mark_official_models(warn_previous_raw)

    # ç­›é€‰å®˜æ–¹æ¨¡å‹
    official_data = data[data['is_official'] == True].copy()

    if official_data.empty:
        print("è­¦å‘Š: åœ¨é€‰å®šæ—¥æœŸå†…æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„å®˜æ–¹æ¨¡å‹æ•°æ®ã€‚")
        return None

    # --- å…¨é‡æ•°æ®é€è§† (ç”¨äºå¹³å°æ€»è§ˆå’Œè¯¦ç»†æ•°æ®) ---
    all_current_data = data[data['date'] == current_date]
    all_previous_data = data[data['date'] == previous_date]
    current_pivot = create_pivot_table(all_current_data, model_order=model_order, merge_other=True)
    previous_pivot = create_pivot_table(all_previous_data, model_order=model_order, merge_other=True)
    growth_pivot = current_pivot - previous_pivot

    # --- å®˜æ–¹æ¨¡å‹æ•°æ®é€è§†ï¼ˆè¯¦ç»†å¹³å°ï¼Œä¸åˆå¹¶"å…¶ä»–"ï¼‰ ---
    # ç”¨äºæ˜¾ç¤ºè¯¦ç»†çš„å„å¹³å°æ¨¡å‹ä¸‹è½½é‡è¯¦æƒ…è¡¨æ ¼
    current_official_data = official_data[official_data['date'] == current_date]
    previous_official_data = official_data[official_data['date'] == previous_date]
    current_official_pivot = create_pivot_table(current_official_data, model_order=model_order, merge_other=False, repo_order=REPO_ORDER_DETAILED)
    previous_official_pivot = create_pivot_table(previous_official_data, model_order=model_order, merge_other=False, repo_order=REPO_ORDER_DETAILED)
    growth_official_pivot = current_official_pivot - previous_official_pivot

    # è®¡ç®—å®˜æ–¹æ¨¡å‹çš„æ€»è®¡ (ç”¨äºTop Næ’å)
    current_totals = current_official_pivot.sum(axis=1).sort_values(ascending=False)
    growth_totals = growth_official_pivot.sum(axis=1).sort_values(ascending=False)

    # Top 5 å¢é•¿æœ€é«˜çš„æ¨¡å‹
    top5_growth = growth_totals.head(5)

    # Top 3 æ€»ä¸‹è½½é‡æœ€é«˜çš„æ¨¡å‹
    top3_downloads = current_totals.head(3)

    # --- è¡ç”Ÿæ¨¡å‹æ•°æ® ---
    derivative_data = data[data['is_official'] == False].copy()
    current_derivative_data = derivative_data[derivative_data['date'] == current_date]
    previous_derivative_data = derivative_data[derivative_data['date'] == previous_date]
    # æ³¨æ„ï¼šæ­¤å¤„ model_order=Noneï¼Œä»¥åŒ…å«æ‰€æœ‰è¡ç”Ÿæ¨¡å‹
    # ğŸ”´ é‡è¦ï¼šä½¿ç”¨ group_by_publisher=True æ¥åŒºåˆ†ä¸åŒ publisher çš„åŒåæ¨¡å‹
    current_derivative_pivot = create_pivot_table(current_derivative_data, model_order=None, group_by_publisher=True)
    previous_derivative_pivot = create_pivot_table(previous_derivative_data, model_order=None, group_by_publisher=True)
    
    # ç¡®ä¿ä¸¤ä¸ªé€è§†è¡¨æœ‰ç›¸åŒçš„ç´¢å¼•å’Œåˆ—ï¼Œä»¥ä¾¿ç›¸å‡
    all_derivative_models = current_derivative_pivot.index.union(previous_derivative_pivot.index)
    current_derivative_pivot = current_derivative_pivot.reindex(index=all_derivative_models, columns=REPO_ORDER, fill_value=0)
    previous_derivative_pivot = previous_derivative_pivot.reindex(index=all_derivative_models, columns=REPO_ORDER, fill_value=0)
    
    growth_derivative_pivot = current_derivative_pivot - previous_derivative_pivot

    # å„å¹³å°ä¸‹è½½é‡æœ€é«˜å’Œå¢é•¿æœ€é«˜çš„æ¨¡å‹ (è¯¦ç»†ç‰ˆ)
    def _get_top_models(repo, current_pivot, growth_pivot, data_source):
        """è¾…åŠ©å‡½æ•°ï¼Œç”¨äºè·å–æŒ‡å®šå¹³å°å’Œæ•°æ®ç±»å‹çš„é¡¶å°–æ¨¡å‹"""
        if repo not in current_pivot.columns or current_pivot[repo].sum() == 0:
            return {
                'top_download_model': 'N/A', 'top_download_publisher': '', 'top_download_count': 0, 'top_download_growth': 0,
                'top_growth_model': 'N/A', 'top_growth_publisher': '', 'top_growth_count': 0, 'top_growth_current': 0,
            }

        # æ£€æŸ¥ pivot ç´¢å¼•æ˜¯å¦ä¸ºå¤šå±‚ç´¢å¼•ï¼ˆè¡ç”Ÿæ¨¡å‹ï¼‰
        has_multiindex = isinstance(current_pivot.index, pd.MultiIndex)

        # ä¸‹è½½é‡æœ€é«˜
        top_download_idx = current_pivot[repo].idxmax()
        top_download_count = current_pivot.loc[top_download_idx, repo]
        top_download_growth = growth_pivot.loc[top_download_idx, repo] if repo in growth_pivot.columns and top_download_idx in growth_pivot.index else 0

        if has_multiindex:
            # å¤šå±‚ç´¢å¼•ï¼š(model_name, publisher)
            top_download_model, top_download_publisher = top_download_idx
        else:
            # å•å±‚ç´¢å¼•ï¼šmodel_name
            top_download_model = top_download_idx
            # ğŸ”§ ä¿®å¤ï¼šä» data_source ä¸­ç­›é€‰å‡ºå¯¹åº” repo çš„æ•°æ®å†æŸ¥æ‰¾ publisher
            filtered_data_source = data_source[data_source['repo'] == repo]
            top_download_publisher = filtered_data_source.loc[filtered_data_source['model_name'] == top_download_model, 'publisher'].iloc[0] if not filtered_data_source.loc[filtered_data_source['model_name'] == top_download_model].empty else ''

        # å¢é•¿æœ€é«˜
        if repo not in growth_pivot.columns or growth_pivot[repo].sum() == 0:
            top_growth_model, top_growth_publisher, top_growth_count, top_growth_current = 'N/A', '', 0, 0
        else:
            top_growth_idx = growth_pivot[repo].idxmax()
            top_growth_count = growth_pivot.loc[top_growth_idx, repo]
            top_growth_current = current_pivot.loc[top_growth_idx, repo]

            if has_multiindex:
                # å¤šå±‚ç´¢å¼•ï¼š(model_name, publisher)
                top_growth_model, top_growth_publisher = top_growth_idx
            else:
                # å•å±‚ç´¢å¼•ï¼šmodel_name
                top_growth_model = top_growth_idx
                # ğŸ”§ ä¿®å¤ï¼šä» data_source ä¸­ç­›é€‰å‡ºå¯¹åº” repo çš„æ•°æ®å†æŸ¥æ‰¾ publisher
                filtered_data_source = data_source[data_source['repo'] == repo]
                top_growth_publisher = filtered_data_source.loc[filtered_data_source['model_name'] == top_growth_model, 'publisher'].iloc[0] if not filtered_data_source.loc[filtered_data_source['model_name'] == top_growth_model].empty else ''

        return {
            'top_download_model': top_download_model, 'top_download_publisher': top_download_publisher,
            'top_download_count': int(top_download_count), 'top_download_growth': int(top_download_growth),
            'top_growth_model': top_growth_model, 'top_growth_publisher': top_growth_publisher,
            'top_growth_count': int(top_growth_count), 'top_growth_current': int(top_growth_current),
        }

    # --- ä¸ºâ€œå„å¹³å°æ¦œé¦–æ¨¡å‹â€å‡†å¤‡åˆå¹¶åçš„æ•°æ® ---
    # åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„ current_official_dataï¼Œå°† 'é­”ä¹ Modelers', 'é²¸æ™º', 'Gitee' åˆå¹¶åˆ° 'å…¶ä»–'
    temp_current_official_data_merged = current_official_data.copy()
    temp_current_official_data_merged['repo'] = temp_current_official_data_merged['repo'].replace(['é­”ä¹ Modelers', 'é²¸æ™º', 'Gitee'], 'å…¶ä»–')
    current_official_pivot_merged = create_pivot_table(temp_current_official_data_merged, model_order=model_order, merge_other=True)

    # åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„ previous_official_dataï¼Œå°† 'é­”ä¹ Modelers', 'é²¸æ™º', 'Gitee' åˆå¹¶åˆ° 'å…¶ä»–'
    temp_previous_official_data_merged = previous_official_data.copy()
    temp_previous_official_data_merged['repo'] = temp_previous_official_data_merged['repo'].replace(['é­”ä¹ Modelers', 'é²¸æ™º', 'Gitee'], 'å…¶ä»–')
    previous_official_pivot_merged = create_pivot_table(temp_previous_official_data_merged, model_order=model_order, merge_other=True)

    growth_official_pivot_merged = current_official_pivot_merged - previous_official_pivot_merged

    platform_top_models = []
    
    for repo in REPO_ORDER: # éå† REPO_ORDERï¼ŒåŒ…å« 'å…¶ä»–'
        if repo in ['Hugging Face', 'AI Studio', 'ModelScope', 'GitCode']: # è¿™äº›å¹³å°ä¿æŒç‹¬ç«‹
            official_tops = _get_top_models(repo, current_official_pivot, growth_official_pivot, current_official_data)
            # åªæœ‰ Hugging Face å’Œ ModelScope æœ‰è¡ç”Ÿæ¨¡å‹
            derivative_tops = None
            if repo in ['Hugging Face', 'ModelScope']:
                derivative_tops = _get_top_models(repo, current_derivative_pivot, growth_derivative_pivot, current_derivative_data)
            platform_top_models.append({
                'platform': repo,
                'official_tops': official_tops,
                'derivative_tops': derivative_tops
            })
        elif repo == 'å…¶ä»–': # 'å…¶ä»–'å¹³å°ä½¿ç”¨åˆå¹¶åçš„æ•°æ®
            official_tops = _get_top_models(repo, current_official_pivot_merged, growth_official_pivot_merged, temp_current_official_data_merged)
            # 'å…¶ä»–'å¹³å°ç›®å‰æ²¡æœ‰åŒºåˆ†è¡ç”Ÿæ¨¡å‹
            platform_top_models.append({'platform': repo, 'official_tops': official_tops, 'derivative_tops': None})

    # å„å¹³å°æ€»ä¸‹è½½é‡å’Œå¢é•¿ (åŸºäºå…¨é‡æ•°æ®)
    # ğŸ”§ ä¿®å¤ï¼šä¸ºäº†åœ¨å¹³å°æ±‡æ€»ä¸­æ˜¾ç¤ºâ€œå…¶ä»–â€çš„æ±‡æ€»æ•°æ®ï¼Œéœ€è¦å°† Gitee, Modelers, é²¸æ™ºåˆå¹¶ä¸ºâ€œå…¶ä»–â€
    all_current_data_with_other = data[data['date'] == current_date].copy()
    all_current_data_with_other['repo'] = all_current_data_with_other['repo'].replace(['é­”ä¹ Modelers', 'é²¸æ™º', 'Gitee'], 'å…¶ä»–')
    current_platform_totals = all_current_data_with_other.groupby('repo')['download_count'].sum()

    all_previous_data_with_other = data[data['date'] == previous_date].copy()
    all_previous_data_with_other['repo'] = all_previous_data_with_other['repo'].replace(['é­”ä¹ Modelers', 'é²¸æ™º', 'Gitee'], 'å…¶ä»–')
    previous_platform_totals = all_previous_data_with_other.groupby('repo')['download_count'].sum()

    # åˆå¹¶å¹¶ç¡®ä¿æ•°å€¼ç±»å‹ï¼Œé¿å…TypeError
    platform_summary = pd.DataFrame({
        'current_total': current_platform_totals,
        'previous_total': previous_platform_totals
    }).reindex(REPO_ORDER).fillna(0).astype(int) # ğŸ”§ ä¿®å¤ï¼šreindex ä½¿ç”¨ REPO_ORDER

    platform_summary['growth_total'] = platform_summary['current_total'] - platform_summary['previous_total']
    platform_summary = platform_summary[['current_total', 'growth_total']]

    # å¢åŠ æ€»ä½“ç»Ÿè®¡
    # å®˜æ–¹æ¨¡å‹ï¼šç›´æ¥ç”¨æœ€åä¸€å¤©
    official_current_total = official_data[official_data['date'] == current_date]['download_count'].sum()
    official_previous_total = official_data[official_data['date'] == previous_date]['download_count'].sum()
    official_growth = official_current_total - official_previous_total

    # è¡ç”Ÿæ¨¡å‹ï¼šä½¿ç”¨â€œå†å²æœ€å¤§å€¼â€é€»è¾‘ï¼ˆæŒ‰ repo/publisher/model_name å–æˆªæ­¢æ—¥æœŸå‰çš„æœ€å¤§ä¸‹è½½é‡ï¼‰
    # é‡æ–°åŠ è½½å…¨é‡æ•°æ®ï¼Œç¡®ä¿å†å²å³°å€¼è®¡ç®—è¦†ç›–æ‰€æœ‰æ—¥æœŸ
    full_data = load_data_from_db(date_filter=None, last_value_per_model=False)
    full_data = enforce_deduplication_and_standardization(full_data)
    full_data = filter_by_series(full_data)
    if not full_data.empty:
        full_data['download_count'] = pd.to_numeric(full_data['download_count'], errors='coerce').fillna(0)
        full_data = mark_official_models(full_data)
        # ä¾¿äºæ—¥æœŸæ¯”è¾ƒï¼Œè½¬æ¢ä¸º datetime
        full_data['date'] = pd.to_datetime(full_data['date'])
        current_dt = pd.to_datetime(current_date)
        previous_dt = pd.to_datetime(previous_date)

        def derivative_peak_total(df, cutoff_dt):
            subset = df[(df['is_official'] == False) & (df['date'] <= cutoff_dt)]
            if subset.empty:
                return 0
            peak_per_combo = subset.groupby(['repo', 'publisher', 'model_name'])['download_count'].max()
            return peak_per_combo.sum()

        derivative_current_total = derivative_peak_total(full_data, current_dt)
        derivative_previous_total = derivative_peak_total(full_data, previous_dt)
    else:
        derivative_current_total = 0
        derivative_previous_total = 0

    derivative_growth = derivative_current_total - derivative_previous_total

    # æ±‡æ€»æ€»æ•°ï¼ˆå®˜æ–¹=æœ€åä¸€å¤©ï¼Œè¡ç”Ÿ=å†å²å³°å€¼ï¼‰
    all_current_total = official_current_total + derivative_current_total
    all_previous_total = official_previous_total + derivative_previous_total
    all_growth = all_current_total - all_previous_total
    # è¡ç”Ÿæ¨¡å‹ï¼ˆæŒ‰ HFã€éå®˜æ–¹ã€publisher+model_name å»é‡çš„æ–°å‡ºç°æ•°é‡ï¼‰
    # ğŸ”´ ä¿®å¤ï¼šç§»é™¤å¤šä½™çš„æ—¥æœŸç­›é€‰ï¼Œå› ä¸º all_current_data/all_previous_data å·²ç»åªåŒ…å«å¯¹åº”æ—¥æœŸçš„æ•°æ®
    hf_curr_non_official = all_current_data[
        (all_current_data['repo'] == 'Hugging Face') &
        (all_current_data['is_official'] == False)
    ]
    hf_prev_non_official = all_previous_data[
        (all_previous_data['repo'] == 'Hugging Face') &
        (all_previous_data['is_official'] == False)
    ]
    curr_deriv_keys = set(zip(hf_curr_non_official['publisher'], hf_curr_non_official['model_name']))
    prev_deriv_keys = set(zip(hf_prev_non_official['publisher'], hf_prev_non_official['model_name']))
    new_derivative_keys = curr_deriv_keys - prev_deriv_keys
    derivative_new_models = len(new_derivative_keys)
    # åˆ—è¡¨æ˜ç»†ï¼ˆHFéå®˜æ–¹æ–°å¢å·®é›†ï¼‰
    derivative_new_models_list = []
    if derivative_new_models > 0:
        # æŒ‰ä¸‹è½½é‡é™åºï¼Œä¿æŒå”¯ä¸€
        hf_curr_non_official = hf_curr_non_official.sort_values('download_count', ascending=False)
        seen = set()
        for _, row in hf_curr_non_official.iterrows():
            key = (row['publisher'], row['model_name'])
            if key in new_derivative_keys and key not in seen:
                seen.add(key)
                derivative_new_models_list.append({
                    'model_name': row['model_name'],
                    'publisher': row['publisher'],
                    'download_count': int(row.get('download_count', 0) or 0),
                    'model_type': row.get('model_type'),
                    'model_category': row.get('model_category'),
                    'base_model': row.get('base_model'),
                    'repo': row.get('repo')
                })

    # ç¤¾åŒºç»´åº¦ & æ¨¡å‹ç»´åº¦
    platform_top_models_df = pd.DataFrame(platform_top_models)
    # 1. ç¤¾åŒºç»´åº¦ï¼šHFå¢é•¿æœ€é«˜ (åŸºäºå®˜æ–¹æ¨¡å‹)
    hf_row = platform_top_models_df.loc[platform_top_models_df['platform'] == 'Hugging Face']
    if not hf_row.empty:
        hf_official_tops = hf_row['official_tops'].iloc[0]
        hf_top_growth_model_name = hf_official_tops['top_growth_model']
        hf_top_model_growth = hf_official_tops['top_growth_count']
    else:
        hf_top_growth_model_name = "N/A"
        hf_top_model_growth = 0

    # 2. æ¨¡å‹ç»´åº¦ï¼šä¸‹è½½æ€»é‡å‰ä¸‰
    top3_downloads_details = current_totals.head(3)

    # 3. æ¨¡å‹ç»´åº¦ï¼šå¢é•¿æœ€å¿«å‰ä¸‰
    top3_growth_details = growth_totals.head(3)

    community_summary = {
        'hf_top_model_name': hf_top_growth_model_name,
        'hf_top_model_growth': hf_top_model_growth,
        'top3_downloads_details': top3_downloads_details.to_dict(),
        'top3_growth_details': top3_growth_details.to_dict(),
    }

    # è·å–æœ¬å‘¨æ–°å¢çš„Finetuneå’ŒAdapteræ¨¡å‹
    try:
        from .fetchers.fetchers_modeltree import get_weekly_new_finetune_adapters
        # ğŸ”§ ä¿®å¤ï¼šä¼ é€’ model_series å‚æ•°ä»¥ç²¾ç¡®ç­›é€‰æ¨¡å‹ç³»åˆ—
        new_models_info = get_weekly_new_finetune_adapters(current_date, previous_date, model_series=model_series)
    except Exception as e:
        print(f"è·å–æ–°å¢Finetune/Adapteræ¨¡å‹ä¿¡æ¯å¤±è´¥: {e}")
        new_models_info = {
            'new_finetune_models': [],
            'new_adapter_models': [],
            'new_lora_models': [],
            'total_new': 0,
            'summary': 'è·å–æ–°å¢æ¨¡å‹ä¿¡æ¯æ—¶å‡ºé”™'
        }

    # ğŸ†• è·å–æœ¬å‘¨æ–°å¢çš„æ‰€æœ‰æ¨¡å‹ï¼ˆå®Œæ•´åˆ—è¡¨ï¼‰
    # ğŸ”§ ä¿®å¤ï¼šç›´æ¥ä¼ é€’æ—¥æœŸï¼Œè®©å‡½æ•°è‡ªå·±åŠ è½½æ•°æ®ï¼ˆä¸ get_weekly_new_finetune_adapters ä¿æŒä¸€è‡´ï¼‰
    all_new_models_info = get_all_new_models(
        current_date=current_date,
        previous_date=previous_date,
        model_series=model_series
    )

    # ç»Ÿè®¡æ¨¡å‹æ•°é‡ï¼ˆæŒ‰ç±»åˆ«ã€æŒ‰æ˜¯å¦åŸå§‹ï¼‰â€”â€”è¡ç”Ÿæ¨¡å‹è®¡æ•°é‡‡ç”¨å›å¡«ï¼ˆå–å½“å‰æ—¥æœŸåŠä¹‹å‰çš„æœ€åä¸€æ¡ï¼‰
    backfill_for_count = load_data_from_db(date_filter=current_date, last_value_per_model=True)
    backfill_for_count = enforce_deduplication_and_standardization(backfill_for_count)
    backfill_for_count = filter_by_series(backfill_for_count)
    backfill_for_count = mark_official_models(backfill_for_count)
    derivative_current_total_models = len(
        backfill_for_count[
            (backfill_for_count['model_category'] == ('ernie-4.5' if model_series == 'ERNIE-4.5' else 'paddleocr-vl')) &
            (backfill_for_count['model_type'] != 'original')
        ]
    )

    summary_stats = {
        'all_current_total': all_current_total,
        'all_growth': all_growth,
        'official_current_total': official_current_total,
        'official_growth': official_growth,
        'derivative_current_total': derivative_current_total,
        'derivative_growth': derivative_growth,
        'derivative_current_total_models': derivative_current_total_models,
        'derivative_new_models': derivative_new_models,
        'derivative_new_models_list': derivative_new_models_list,
    }

    # ğŸ”´ è´Ÿå¢é•¿æ£€æµ‹ï¼šæ£€æŸ¥æ‰€æœ‰å¹³å°å’Œæ¨¡å‹çš„å¢é•¿æƒ…å†µ
    negative_growth_warnings = []

    # ğŸ”§ ä¿®å¤ï¼šåŸºäºåŸå§‹æ•°æ®æ£€æµ‹è´Ÿå¢é•¿ï¼Œè€Œä¸æ˜¯åŸºäº pivot è¡¨
    # è¿™æ ·èƒ½æ•è·æ‰€æœ‰æ¨¡å‹ï¼ˆåŒ…æ‹¬ä¸åœ¨ model_order ä¸­çš„è¡ç”Ÿæ¨¡å‹ï¼‰
    # ä¿®å¤ï¼šä½¿ç”¨ REPO_ORDER_DETAILED æ¥å•ç‹¬æ£€æµ‹ 'é­”ä¹ Modelers', 'é²¸æ™º', 'Gitee' çš„è´Ÿå¢é•¿
    for repo in REPO_ORDER_DETAILED: # ä¿®æ”¹ä¸º REPO_ORDER_DETAILED
        # è·å–è¯¥å¹³å°ä¸Šå‘¨å’Œæœ¬å‘¨çš„åŸå§‹å½“æ—¥æ•°æ®ï¼ˆä¸ä½¿ç”¨ last_value_per_modelï¼‰
        prev_platform_data = warn_previous_raw[warn_previous_raw['repo'] == repo].copy()
        curr_platform_data = warn_current_raw[warn_current_raw['repo'] == repo].copy()

        # æŒ‰æ¨¡å‹+å‘å¸ƒè€…èšåˆä¸‹è½½é‡ï¼Œé¿å…ä¸åŒå‘å¸ƒè€…çš„åŒåæ¨¡å‹è¢«åˆå¹¶
        prev_by_model = prev_platform_data.groupby(['model_name', 'publisher'])['download_count'].sum()
        curr_by_model = curr_platform_data.groupby(['model_name', 'publisher'])['download_count'].sum()

        # æ£€æŸ¥æ¯ä¸ªåœ¨ä¸Šå‘¨å­˜åœ¨çš„æ¨¡å‹
        for model_name in prev_by_model.index:
            previous_val = prev_by_model[model_name]
            current_val = curr_by_model.get(model_name, 0)  # å¦‚æœä¸å­˜åœ¨åˆ™ä¸º0
            growth_val = current_val - previous_val

            # åªæŠ¥å‘Šè´Ÿå¢é•¿
            if growth_val < 0:
                # åˆ¤æ–­æ˜¯å®˜æ–¹æ¨¡å‹è¿˜æ˜¯è¡ç”Ÿæ¨¡å‹
                model_name_only, publisher = model_name
                prev_rows = prev_platform_data[
                    (prev_platform_data['model_name'] == model_name_only) &
                    (prev_platform_data['publisher'] == publisher)
                ]
                curr_rows = curr_platform_data[
                    (curr_platform_data['model_name'] == model_name_only) &
                    (curr_platform_data['publisher'] == publisher)
                ]
                is_official_prev = (not prev_rows.empty) and prev_rows['is_official'].any()
                is_official_curr = (not curr_rows.empty) and curr_rows['is_official'].any()
                is_official = is_official_prev or is_official_curr
                model_type = 'å®˜æ–¹æ¨¡å‹' if is_official else 'è¡ç”Ÿæ¨¡å‹'

                negative_growth_warnings.append({
                    'platform': repo,
                    'model_name': model_name_only,
                    'publisher': publisher,
                    'model_type': model_type,
                    'previous': int(previous_val),
                    'current': int(current_val),
                    'growth': int(growth_val)
                })

    # å¦‚æœå‘ç°è´Ÿå¢é•¿ï¼Œæ‰“å°è­¦å‘Š
    if negative_growth_warnings:
        print("\n" + "=" * 80)
        print("âš ï¸  è­¦å‘Šï¼šæ£€æµ‹åˆ°è´Ÿå¢é•¿ï¼")
        print("=" * 80)
        for warning in negative_growth_warnings:
            print(f"å¹³å°: {warning['platform']}")
            print(f"æ¨¡å‹: {warning['model_name']} | å‘å¸ƒè€…: {warning['publisher']} | ç±»å‹: {warning['model_type']}")
            print(f"æ•°æ®: {warning['previous']:,} â†’ {warning['current']:,} (å˜åŒ–: {warning['growth']:,})")
            print(f"è¯´æ˜: è¯¥æ¨¡å‹åœ¨ {previous_date} æœ‰æ•°æ®ï¼Œä½†åœ¨ {current_date} æ•°æ®å‡å°‘æˆ–ç¼ºå¤±")
            print("-" * 80)

    return {
        'current_date': current_date,
        'summary_stats': summary_stats,
        'community_summary': community_summary,
        'previous_date': previous_date,
        'current_pivot': current_official_pivot,  # ä¿®æ”¹ï¼šä½¿ç”¨å®˜æ–¹æ¨¡å‹é€è§†è¡¨
        'previous_pivot': previous_official_pivot,  # ä¿®æ”¹ï¼šä½¿ç”¨å®˜æ–¹æ¨¡å‹é€è§†è¡¨
        'growth_pivot': growth_official_pivot,  # ä¿®æ”¹ï¼šä½¿ç”¨å®˜æ–¹æ¨¡å‹é€è§†è¡¨
        'top5_growth': top5_growth,
        'top3_downloads': top3_downloads,
        'platform_top_models': platform_top_models_df,
        'platform_summary': platform_summary,
        'new_models_info': new_models_info,  # æ–°å¢Finetune/Adapter/LoRAæ¨¡å‹ä¿¡æ¯
        'all_new_models_info': all_new_models_info,  # ğŸ†• æ‰€æœ‰æ–°å¢æ¨¡å‹å®Œæ•´åˆ—è¡¨
        'negative_growth_warnings': negative_growth_warnings  # è´Ÿå¢é•¿è­¦å‘Š
    }


def format_report_tables(report_data):
    """
    æ ¼å¼åŒ–æŠ¥è¡¨æ•°æ®ä¸ºå¯æ˜¾ç¤ºçš„è¡¨æ ¼

    Args:
        report_data: calculate_weekly_report è¿”å›çš„æ•°æ®

    Returns:
        dict: åŒ…å«æ ¼å¼åŒ–è¡¨æ ¼çš„å­—å…¸
    """
    if report_data is None:
        return None

    tables = {}

    # 1. & 2. åˆå¹¶ä¸‹è½½é‡å’Œå¢é•¿é‡
    current_pivot = report_data['current_pivot'].astype(int)
    growth_pivot = report_data['growth_pivot'].astype(int)

    interleaved_df = pd.DataFrame(index=current_pivot.index)

    # ä½¿ç”¨ REPO_ORDER_DETAILED ä¿è¯é¡ºåºï¼ˆæ˜¾ç¤ºè¯¦ç»†å¹³å°ï¼Œä¸åˆå¹¶"å…¶ä»–"ï¼‰
    for repo in REPO_ORDER_DETAILED:
        if repo in current_pivot.columns:
            interleaved_df[f'{repo} (æ€»)'] = current_pivot[repo]
            interleaved_df[f'{repo} (å‘¨å¢)'] = growth_pivot[repo]

    tables['combined_downloads_growth'] = interleaved_df

    # 3. Top 5 å¢é•¿æœ€é«˜çš„æ¨¡å‹
    top5_df = pd.DataFrame({
        'æ¨¡å‹åç§°': report_data['top5_growth'].index,
        'æœ¬å‘¨å¢é•¿': report_data['top5_growth'].values.astype(int)
    }).reset_index(drop=True)
    top5_df.index = top5_df.index + 1
    tables['top5_growth'] = top5_df

    # 4. Top 3 æ€»ä¸‹è½½é‡æœ€é«˜çš„æ¨¡å‹
    top3_df = pd.DataFrame({
        'æ¨¡å‹åç§°': report_data['top3_downloads'].index,
        'æ€»ä¸‹è½½é‡': report_data['top3_downloads'].values.astype(int)
    }).reset_index(drop=True)
    top3_df.index = top3_df.index + 1
    tables['top3_downloads'] = top3_df

    # 5. å„å¹³å°æ¦œé¦–æ¨¡å‹
    platform_top_df = report_data['platform_top_models'].copy()
    
    def format_model_info(tops, label, is_growth_col=False):
        if not tops:
            return ""
            
        model_name = tops['top_growth_model'] if is_growth_col else tops['top_download_model']
        if model_name == 'N/A':
            return ""

        publisher = tops['top_growth_publisher'] if is_growth_col else tops['top_download_publisher']
        total_downloads = tops['top_growth_current'] if is_growth_col else tops['top_download_count']
        weekly_growth = tops['top_growth_count'] if is_growth_col else tops['top_download_growth']

        return (
            f"{label}: {model_name} ({publisher})\n"
            f"  ({total_downloads:,}) (æœ¬å‘¨: +{weekly_growth:,})"
        )

    def format_combined_download(row):
        official_str = format_model_info(row['official_tops'], 'å®˜æ–¹', is_growth_col=False)
        derivative_str = format_model_info(row['derivative_tops'], 'è¡ç”Ÿ', is_growth_col=False)
        return "\n".join(filter(None, [official_str, derivative_str]))

    def format_combined_growth(row):
        official_str = format_model_info(row['official_tops'], 'å®˜æ–¹', is_growth_col=True)
        derivative_str = format_model_info(row['derivative_tops'], 'è¡ç”Ÿ', is_growth_col=True)
        return "\n".join(filter(None, [official_str, derivative_str]))

    platform_top_df['ä¸‹è½½é‡æœ€é«˜æ¨¡å‹'] = platform_top_df.apply(format_combined_download, axis=1)
    platform_top_df['å¢é•¿æœ€é«˜æ¨¡å‹'] = platform_top_df.apply(format_combined_growth, axis=1)
    
    # ç­›é€‰å¹¶é‡å‘½ååˆ—
    tables['platform_top_models'] = platform_top_df[['platform', 'ä¸‹è½½é‡æœ€é«˜æ¨¡å‹', 'å¢é•¿æœ€é«˜æ¨¡å‹']]

    # 6. å„å¹³å°æ±‡æ€»
    summary_df = report_data['platform_summary'].copy()
    summary_df.columns = ['å½“å‰æ€»ä¸‹è½½é‡', 'æœ¬å‘¨å¢é•¿']
    summary_df = summary_df.astype(int)
    tables['platform_summary'] = summary_df

    # 7. æ–°å¢Finetuneå’ŒAdapteræ¨¡å‹è¡¨æ ¼
    new_models_info = report_data.get('new_models_info', {})

    # æ ¼å¼åŒ–æ–°å¢Finetuneæ¨¡å‹
    finetune_data = new_models_info.get('new_finetune_models', [])
    if finetune_data:
        finetune_df = pd.DataFrame(finetune_data)
        finetune_df.index = finetune_df.index + 1
        finetune_df.columns = ['æ¨¡å‹åç§°', 'å‘å¸ƒè€…', 'ä¸‹è½½é‡']
        tables['new_finetune_models'] = finetune_df
    else:
        tables['new_finetune_models'] = pd.DataFrame(columns=['æ¨¡å‹åç§°', 'å‘å¸ƒè€…', 'ä¸‹è½½é‡'])

    # æ ¼å¼åŒ–æ–°å¢Adapteræ¨¡å‹
    adapter_data = new_models_info.get('new_adapter_models', [])
    if adapter_data:
        adapter_df = pd.DataFrame(adapter_data)
        adapter_df.index = adapter_df.index + 1
        adapter_df.columns = ['æ¨¡å‹åç§°', 'å‘å¸ƒè€…', 'ä¸‹è½½é‡']
        tables['new_adapter_models'] = adapter_df
    else:
        tables['new_adapter_models'] = pd.DataFrame(columns=['æ¨¡å‹åç§°', 'å‘å¸ƒè€…', 'ä¸‹è½½é‡'])

    # æ ¼å¼åŒ–æ–°å¢LoRAæ¨¡å‹
    lora_data = new_models_info.get('new_lora_models', [])
    if lora_data:
        lora_df = pd.DataFrame(lora_data)
        lora_df.index = lora_df.index + 1
        lora_df.columns = ['æ¨¡å‹åç§°', 'å‘å¸ƒè€…', 'ä¸‹è½½é‡']
        tables['new_lora_models'] = lora_df
    else:
        tables['new_lora_models'] = pd.DataFrame(columns=['æ¨¡å‹åç§°', 'å‘å¸ƒè€…', 'ä¸‹è½½é‡'])

    # æ–°å¢æ¨¡å‹æ±‡æ€»ä¿¡æ¯
    tables['new_models_summary'] = new_models_info.get('summary', 'æ— æ–°å¢æ¨¡å‹ä¿¡æ¯')

    # 8. Model Tree æ–°å¢è¡ç”Ÿæ¨¡å‹è¡¨æ ¼
    # ç§»é™¤ Model Tree æ–°å¢è¡ç”Ÿæ¨¡å‹æ¨¡å—
    tables['new_model_tree_models'] = pd.DataFrame(columns=['æ¨¡å‹åç§°', 'å‘å¸ƒè€…', 'ä¸‹è½½é‡', 'åŸºç¡€æ¨¡å‹', 'æ¨¡å‹ç±»å‹'])
    tables['model_tree_summary'] = 'ï¼ˆå·²ç§»é™¤ Model Tree æ–°å¢æ¨¡å—ï¼‰'

    # 9. è´Ÿå¢é•¿è­¦å‘Šè¡¨æ ¼
    negative_growth_warnings = report_data.get('negative_growth_warnings', [])
    if negative_growth_warnings:
        warnings_df = pd.DataFrame(negative_growth_warnings)
        warnings_df.index = warnings_df.index + 1
        warnings_df.columns = ['å¹³å°', 'æ¨¡å‹åç§°', 'å‘å¸ƒè€…', 'æ¨¡å‹ç±»å‹', 'ä¸Šå‘¨ä¸‹è½½é‡', 'æœ¬å‘¨ä¸‹è½½é‡', 'å‘¨å¢é•¿']
        tables['negative_growth_warnings'] = warnings_df
    else:
        tables['negative_growth_warnings'] = pd.DataFrame(columns=['å¹³å°', 'æ¨¡å‹åç§°', 'å‘å¸ƒè€…', 'æ¨¡å‹ç±»å‹', 'ä¸Šå‘¨ä¸‹è½½é‡', 'æœ¬å‘¨ä¸‹è½½é‡', 'å‘¨å¢é•¿'])

    # 10. ğŸ†• æ‰€æœ‰æ–°å¢æ¨¡å‹å®Œæ•´åˆ—è¡¨
    all_new_models_info = report_data.get('all_new_models_info', {})

    new_models_list = all_new_models_info.get('new_models_list', [])
    if new_models_list:
        all_new_df = pd.DataFrame(new_models_list)
        all_new_df.index = all_new_df.index + 1

        # æ ¹æ®åŒ…å«çš„åˆ—æ¥è®¾ç½®åˆ—å
        column_mapping = {
            'model_name': 'æ¨¡å‹åç§°',
            'publisher': 'å‘å¸ƒè€…',
            'repo': 'å¹³å°',
            'download_count': 'ä¸‹è½½é‡',
            'model_type': 'æ¨¡å‹ç±»å‹',
            'model_category': 'æ¨¡å‹åˆ†ç±»',
            'base_model': 'åŸºç¡€æ¨¡å‹'
        }

        # åªé‡å‘½åå­˜åœ¨çš„åˆ—
        rename_dict = {k: v for k, v in column_mapping.items() if k in all_new_df.columns}
        all_new_df = all_new_df.rename(columns=rename_dict)

        tables['all_new_models'] = all_new_df
    else:
        tables['all_new_models'] = pd.DataFrame(columns=['æ¨¡å‹åç§°', 'å‘å¸ƒè€…', 'å¹³å°', 'ä¸‹è½½é‡', 'æ¨¡å‹ç±»å‹'])

    # æ–°å¢æ¨¡å‹æ±‡æ€»ä¿¡æ¯
    tables['all_new_models_summary'] = all_new_models_info.get('summary', 'æ— æ–°å¢æ¨¡å‹')

    # 11. HF éå®˜æ–¹æ–°å¢è¡ç”Ÿæ¨¡å‹åˆ—è¡¨
    derivative_new_list = report_data['summary_stats'].get('derivative_new_models_list', [])
    if derivative_new_list:
        deriv_new_df = pd.DataFrame(derivative_new_list)
        deriv_new_df.index = deriv_new_df.index + 1
        rename_map = {
            'model_name': 'æ¨¡å‹åç§°',
            'publisher': 'å‘å¸ƒè€…',
            'download_count': 'ä¸‹è½½é‡',
            'model_type': 'æ¨¡å‹ç±»å‹',
            'model_category': 'æ¨¡å‹ç³»åˆ—',
            'base_model': 'åŸºç¡€æ¨¡å‹',
            'repo': 'å¹³å°'
        }
        deriv_new_df = deriv_new_df.rename(columns={k: v for k, v in rename_map.items() if k in deriv_new_df.columns})
        tables['derivative_new_models'] = deriv_new_df
    else:
        tables['derivative_new_models'] = pd.DataFrame(columns=['æ¨¡å‹åç§°', 'å‘å¸ƒè€…', 'ä¸‹è½½é‡', 'æ¨¡å‹ç±»å‹', 'æ¨¡å‹ç³»åˆ—', 'åŸºç¡€æ¨¡å‹', 'å¹³å°'])

    return tables


def calculate_paddleocr_vl_weekly_report(current_date=None, previous_date=None):
    """
    è®¡ç®— PaddleOCR-VL çš„å‘¨æŠ¥æ•°æ®
    """
    return calculate_weekly_report(
        current_date,
        previous_date,
        model_order=PADDLEOCR_VL_MODEL_ORDER,
        model_series='PaddleOCR-VL'
    )


def get_deleted_or_hidden_models(current_date, model_series='ERNIE-4.5'):
    """
    æ£€æµ‹å·²è¢«åˆ é™¤æˆ–éšè—çš„æ¨¡å‹

    é€»è¾‘ï¼š
    - ä½¿ç”¨å›å¡«æ¨¡å¼ï¼ˆlast_value_per_model=Trueï¼‰è·å–æˆªæ­¢åˆ°å½“å‰æ—¥æœŸçš„æ‰€æœ‰å†å²æ¨¡å‹
    - ä½¿ç”¨æ­£å¸¸æ¨¡å¼ï¼ˆlast_value_per_model=Falseï¼‰è·å–å½“å‰æ—¥æœŸçš„å®é™…æ•°æ®
    - å¯¹æ¯”ä¸¤è€…ï¼Œæ‰¾å‡ºåœ¨å†å²ä¸­å­˜åœ¨ä½†å½“å‰æ—¥æœŸä¸å­˜åœ¨çš„æ¨¡å‹

    Args:
        current_date: å½“å‰æ—¥æœŸ (YYYY-MM-DD)
        model_series: æ¨¡å‹ç³»åˆ— ('ERNIE-4.5' æˆ– 'PaddleOCR-VL')

    Returns:
        list: å·²åˆ é™¤/éšè—çš„æ¨¡å‹åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«:
            - model_name: æ¨¡å‹åç§°
            - publisher: å‘å¸ƒè€…
            - model_type: æ¨¡å‹ç±»å‹
            - model_category: æ¨¡å‹åˆ†ç±»
            - base_model: åŸºç¡€æ¨¡å‹
            - last_seen_date: æœ€åå‡ºç°æ—¥æœŸ
            - last_download_count: æœ€åè®°å½•çš„ä¸‹è½½é‡
            - repo: å¹³å°
    """
    try:
        # 1. è·å–æ‰€æœ‰å†å²æ¨¡å‹ï¼ˆå›å¡«æ¨¡å¼ï¼‰
        all_historical = load_data_from_db(date_filter=current_date, last_value_per_model=True)

        # 2. è·å–å½“å‰æ—¥æœŸçš„å®é™…æ•°æ®
        current_actual = load_data_from_db(date_filter=current_date, last_value_per_model=False)

        if all_historical.empty:
            return []

        # 3. ç­›é€‰ç›®æ ‡ç³»åˆ—çš„è¡ç”Ÿæ¨¡å‹
        target_category = 'ernie-4.5' if model_series == 'ERNIE-4.5' else 'paddleocr-vl'

        # å†å²ä¸­çš„è¡ç”Ÿæ¨¡å‹
        historical_derivatives = all_historical[
            (all_historical['model_category'] == target_category) &
            (all_historical['model_type'] != 'original')
        ].copy()

        # å½“å‰æ—¥æœŸçš„è¡ç”Ÿæ¨¡å‹
        current_derivatives = current_actual[
            (current_actual['model_category'] == target_category) &
            (current_actual['model_type'] != 'original')
        ].copy()

        if historical_derivatives.empty:
            return []

        # 3.5. åº”ç”¨ä¸å‘¨æŠ¥ç›¸åŒçš„æ ‡å‡†åŒ–é€»è¾‘
        # æ ‡å‡†åŒ– publisher åç§°
        historical_derivatives['publisher'] = historical_derivatives['publisher'].astype(str).apply(
            lambda x: x.title() if x.lower() != 'nan' else x
        )
        if not current_derivatives.empty:
            current_derivatives['publisher'] = current_derivatives['publisher'].astype(str).apply(
                lambda x: x.title() if x.lower() != 'nan' else x
            )

        # æ ‡å‡†åŒ–æ¨¡å‹åç§°
        historical_derivatives = normalize_model_names(historical_derivatives)
        if not current_derivatives.empty:
            current_derivatives = normalize_model_names(current_derivatives)

        # å»é‡ï¼ˆæŒ‰ä¸‹è½½é‡é™åºï¼Œä¿ç•™æœ€é«˜çš„ï¼‰
        historical_derivatives['download_count'] = pd.to_numeric(
            historical_derivatives['download_count'], errors='coerce'
        ).fillna(0)
        historical_derivatives = historical_derivatives.sort_values(
            by='download_count', ascending=False
        ).drop_duplicates(
            subset=['date', 'repo', 'publisher', 'model_name'], keep='first'
        )

        if not current_derivatives.empty:
            current_derivatives['download_count'] = pd.to_numeric(
                current_derivatives['download_count'], errors='coerce'
            ).fillna(0)
            current_derivatives = current_derivatives.sort_values(
                by='download_count', ascending=False
            ).drop_duplicates(
                subset=['date', 'repo', 'publisher', 'model_name'], keep='first'
            )

        # 4. åˆ›å»ºæ¨¡å‹å”¯ä¸€æ ‡è¯† (repo, publisher, model_name)
        historical_derivatives['model_key'] = (
            historical_derivatives['repo'] + '|||' +
            historical_derivatives['publisher'] + '|||' +
            historical_derivatives['model_name']
        )

        if not current_derivatives.empty:
            current_derivatives['model_key'] = (
                current_derivatives['repo'] + '|||' +
                current_derivatives['publisher'] + '|||' +
                current_derivatives['model_name']
            )
            current_keys = set(current_derivatives['model_key'].unique())
        else:
            current_keys = set()

        historical_keys = set(historical_derivatives['model_key'].unique())

        # 5. æ‰¾å‡ºå·²åˆ é™¤/éšè—çš„æ¨¡å‹
        deleted_keys = historical_keys - current_keys

        if not deleted_keys:
            return []

        # 6. è·å–å·²åˆ é™¤æ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯
        deleted_models = historical_derivatives[
            historical_derivatives['model_key'].isin(deleted_keys)
        ].copy()

        # 7. å¯¹äºæ¯ä¸ªå·²åˆ é™¤çš„æ¨¡å‹ï¼Œæ‰¾åˆ°å®ƒæœ€åå‡ºç°çš„æ—¥æœŸ
        deleted_models_info = []

        for _, row in deleted_models.iterrows():
            model_key_parts = row['model_key'].split('|||')
            repo = model_key_parts[0]
            publisher = model_key_parts[1]
            model_name = model_key_parts[2]

            # æŸ¥è¯¢è¯¥æ¨¡å‹åœ¨æ•°æ®åº“ä¸­æœ€åå‡ºç°çš„æ—¥æœŸ
            # ä½¿ç”¨ LOWER() è¿›è¡Œä¸åŒºåˆ†å¤§å°å†™çš„åŒ¹é…ï¼Œå› ä¸ºæ ‡å‡†åŒ–åçš„ publisher å¯èƒ½ä¸æ•°æ®åº“ä¸­çš„åŸå§‹å€¼å¤§å°å†™ä¸åŒ
            conn = sqlite3.connect(DB_PATH)
            query = """
                SELECT date, download_count
                FROM model_downloads
                WHERE repo = ? AND LOWER(publisher) = LOWER(?) AND model_name = ?
                ORDER BY date DESC
                LIMIT 1
            """
            result = pd.read_sql_query(query, conn, params=(repo, publisher, model_name))
            conn.close()

            if not result.empty:
                last_seen_date = result.iloc[0]['date']
                last_download_count = result.iloc[0]['download_count']
            else:
                last_seen_date = row.get('date', 'Unknown')
                last_download_count = row.get('download_count', 0)

            model_info = {
                'model_name': model_name,
                'publisher': publisher,
                'model_type': row.get('model_type', 'unknown'),
                'model_category': row.get('model_category', ''),
                'base_model': row.get('base_model', ''),
                'last_seen_date': last_seen_date,
                'last_download_count': int(last_download_count) if pd.notna(last_download_count) else 0,
                'repo': repo
            }

            deleted_models_info.append(model_info)

        # 8. æŒ‰æœ€åå‡ºç°æ—¥æœŸé™åºæ’åº
        deleted_models_info = sorted(
            deleted_models_info,
            key=lambda x: x['last_seen_date'],
            reverse=True
        )

        return deleted_models_info

    except Exception as e:
        print(f"æ£€æµ‹å·²åˆ é™¤/éšè—æ¨¡å‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return []


def analyze_derivative_models_all_platforms(df, selected_series=None):
    """
    åˆ†æå…¨å¹³å°çš„è¡ç”Ÿæ¨¡å‹ç”Ÿæ€ï¼ˆåŸºäº is_official æ ‡è®°ï¼‰

    Args:
        df: åŒ…å«å…¨å¹³å°æ•°æ®çš„ DataFrame
        selected_series: è¦åˆ†æçš„ç³»åˆ—åˆ—è¡¨ï¼Œå¦‚ ['ERNIE-4.5', 'PaddleOCR-VL']

    Returns:
        dict: åŒ…å«åˆ†æç»“æœçš„å­—å…¸
    """
    if df.empty:
        return {
            'total_models': 0,
            'total_derivative_models': 0,
            'total_official_models': 0,
            'derivative_rate': 0,
            'by_platform': {},
            'by_series': {},
            'derivative_models_df': pd.DataFrame()
        }

    # ç¡®ä¿å¿…è¦çš„åˆ—å­˜åœ¨
    df = df.copy()

    # ğŸ”´ æ ‡å‡†åŒ–å’Œå»é‡ï¼ˆä¸ calculate_weekly_report ä¿æŒä¸€è‡´ï¼‰
    # 1. æ ‡å‡†åŒ– publisher åç§°ï¼ˆç»Ÿä¸€å¤§å°å†™ï¼‰
    df['publisher'] = df['publisher'].astype(str).apply(lambda x: x.title() if x.lower() != 'nan' else x)

    # 2. æ ‡å‡†åŒ–æ¨¡å‹åç§°ï¼ˆç§»é™¤ publisher å‰ç¼€ï¼‰
    df = normalize_model_names(df)

    # 3. å†æ¬¡å»é‡ï¼Œç¡®ä¿åŒä¸€ (date, repo, publisher, model_name) åªæœ‰ä¸€æ¡è®°å½•ï¼Œä¸”ä¸‹è½½é‡æœ€å¤§
    df['download_count'] = pd.to_numeric(df['download_count'], errors='coerce').fillna(0)
    df = df.sort_values(by='download_count', ascending=False).drop_duplicates(
        subset=['date', 'repo', 'publisher', 'model_name'], keep='first'
    )

    # æ ‡è®°å®˜æ–¹æ¨¡å‹ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ is_official åˆ—ï¼‰
    if 'is_official' not in df.columns:
        df = mark_official_models(df)

    # æŒ‰ç³»åˆ—ç­›é€‰ï¼ˆæ‰€æœ‰è®°å½•ç°åœ¨éƒ½æœ‰ model_category å­—æ®µï¼‰
    if selected_series:
        series_mapping = {
            "ERNIE-4.5": "ernie-4.5",
            "PaddleOCR-VL": "paddleocr-vl",
            "å…¶ä»–ERNIE": "other-ernie"
        }

        selected_categories = [series_mapping.get(s, s) for s in selected_series]
        df = df[df['model_category'].isin(selected_categories)].copy()

    # ç»Ÿè®¡æ€»æ•°
    total_models = len(df)
    official_models_df = df[df['is_official'] == True]
    derivative_models_df = df[df['is_official'] == False]

    total_official_models = len(official_models_df)
    total_derivative_models = len(derivative_models_df)
    derivative_rate = (total_derivative_models / total_models * 100) if total_models > 0 else 0

    # æŒ‰å¹³å°ç»Ÿè®¡
    by_platform = {}
    for platform in df['repo'].unique():
        platform_df = df[df['repo'] == platform]
        platform_derivative_df = derivative_models_df[derivative_models_df['repo'] == platform]

        # è®¡ç®—ä¸‹è½½é‡ï¼ˆè½¬æ¢ä¸ºæ•°å€¼ï¼‰
        platform_derivative_df['download_count_num'] = pd.to_numeric(
            platform_derivative_df['download_count'], errors='coerce'
        ).fillna(0)

        total_downloads = int(platform_derivative_df['download_count_num'].sum())

        # æ‰¾å‡ºä¸‹è½½é‡æœ€é«˜çš„æ¨¡å‹ï¼ˆTop 5ï¼‰
        top_models = platform_derivative_df.nlargest(5, 'download_count_num')[
            ['model_name', 'publisher', 'download_count']
        ].to_dict('records')

        # ğŸ”§ æ–°å¢ï¼šæŒ‰ç³»åˆ—ç»Ÿè®¡ï¼ˆå¦‚æœé€‰æ‹©äº†å¤šä¸ªç³»åˆ—ï¼‰
        by_series_stats = {}
        if selected_series and 'model_category' in platform_derivative_df.columns:
            series_mapping = {
                "ERNIE-4.5": "ernie-4.5",
                "PaddleOCR-VL": "paddleocr-vl",
                "å…¶ä»–ERNIE": "other-ernie"
            }

            for series in selected_series:
                category = series_mapping.get(series, series)
                series_df = platform_derivative_df[platform_derivative_df['model_category'] == category]
                series_downloads = int(series_df['download_count_num'].sum())

                by_series_stats[category] = {
                    'count': len(series_df),
                    'downloads': series_downloads
                }

        by_platform[platform] = {
            'total_models': len(platform_df),
            'derivative_models': len(platform_derivative_df),
            'official_models': len(platform_df[platform_df['is_official'] == True]),
            'total_downloads': total_downloads,
            'derivative_rate': (len(platform_derivative_df) / len(platform_df) * 100) if len(platform_df) > 0 else 0,
            'top_models': top_models,
            'by_series': by_series_stats  # æ–°å¢ï¼šæŒ‰ç³»åˆ—ç»Ÿè®¡
        }

    # æŒ‰ç³»åˆ—ç»Ÿè®¡ï¼ˆå¦‚æœæœ‰ model_category å­—æ®µï¼‰
    by_series = {}
    if 'model_category' in df.columns:
        for category in df['model_category'].dropna().unique():
            category_df = df[df['model_category'] == category]
            category_derivative_df = derivative_models_df[derivative_models_df['model_category'] == category]

            by_series[category] = {
                'total_models': len(category_df),
                'derivative_models': len(category_derivative_df),
                'official_models': len(category_df[category_df['is_official'] == True]),
                'derivative_rate': (len(category_derivative_df) / len(category_df) * 100) if len(category_df) > 0 else 0
            }

    return {
        'total_models': total_models,
        'total_derivative_models': total_derivative_models,
        'total_official_models': total_official_models,
        'derivative_rate': derivative_rate,
        'by_platform': by_platform,
        'by_series': by_series,
        'derivative_models_df': derivative_models_df
    }


def get_quarter_start_date(current_date):
    """
    è·å–å½“å‰æ—¥æœŸæ‰€åœ¨å­£åº¦çš„å¼€å§‹æ—¥æœŸ

    Args:
        current_date: å½“å‰æ—¥æœŸ (datetime æˆ– str)

    Returns:
        str: å­£åº¦å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
    """
    if isinstance(current_date, str):
        current_date = datetime.strptime(current_date, '%Y-%m-%d')

    year = current_date.year
    month = current_date.month

    # è®¡ç®—å­£åº¦
    if month <= 3:
        quarter_start = datetime(year, 1, 1)
    elif month <= 6:
        quarter_start = datetime(year, 4, 1)
    elif month <= 9:
        quarter_start = datetime(year, 7, 1)
    else:
        quarter_start = datetime(year, 10, 1)

    return quarter_start.strftime('%Y-%m-%d')


def get_current_quarter_name(current_date):
    """
    è·å–å½“å‰å­£åº¦åç§°

    Args:
        current_date: å½“å‰æ—¥æœŸ (datetime æˆ– str)

    Returns:
        str: å­£åº¦åç§°ï¼Œå¦‚ "2026Q1"
    """
    if isinstance(current_date, str):
        current_date = datetime.strptime(current_date, '%Y-%m-%d')

    year = current_date.year
    month = current_date.month

    # è®¡ç®—å­£åº¦
    if month <= 3:
        quarter = 1
    elif month <= 6:
        quarter = 2
    elif month <= 9:
        quarter = 3
    else:
        quarter = 4

    return f"{year}Q{quarter}"


def calculate_periodic_stats(current_date, selected_series=None):
    """
    è®¡ç®—å‘¨æœŸæ€§ç»Ÿè®¡æ•°æ®ï¼ˆæœ¬å‘¨ã€å½“å‰å­£åº¦æ–°å¢ç­‰ï¼‰

    Args:
        current_date: åˆ†ææ—¥æœŸ (YYYY-MM-DD)
        selected_series: æ¨¡å‹ç³»åˆ—åˆ—è¡¨ï¼Œå¦‚ ['ERNIE-4.5', 'PaddleOCR-VL']

    Returns:
        dict: åŒ…å«å‘¨æœŸæ€§ç»Ÿè®¡çš„å­—å…¸
    """
    # è®¡ç®—æ—¶é—´ç‚¹
    current_date_dt = datetime.strptime(current_date, '%Y-%m-%d')
    last_week_date = (current_date_dt - timedelta(days=7)).strftime('%Y-%m-%d')
    quarter_start_date = get_quarter_start_date(current_date)
    quarter_name = get_current_quarter_name(current_date)

    # åŠ è½½æ•°æ®ï¼ˆä½¿ç”¨å›å¡«é€»è¾‘ï¼‰
    current_data = load_data_from_db(date_filter=current_date, last_value_per_model=True)
    last_week_data = load_data_from_db(date_filter=last_week_date, last_value_per_model=True)
    quarter_start_data = load_data_from_db(date_filter=quarter_start_date, last_value_per_model=True)

    # æ ‡å‡†åŒ–å’Œå»é‡
    def standardize(df):
        if df.empty:
            return df
        df = df.copy()
        df['publisher'] = df['publisher'].astype(str).apply(lambda x: x.title() if x.lower() != 'nan' else x)
        df = normalize_model_names(df)
        df['download_count'] = pd.to_numeric(df['download_count'], errors='coerce').fillna(0)
        df = df.sort_values(by='download_count', ascending=False).drop_duplicates(
            subset=['date', 'repo', 'publisher', 'model_name'], keep='first'
        )
        return df

    current_data = standardize(current_data)
    last_week_data = standardize(last_week_data)
    quarter_start_data = standardize(quarter_start_data)

    # æ ‡è®°å®˜æ–¹æ¨¡å‹
    current_data = mark_official_models(current_data)
    last_week_data = mark_official_models(last_week_data)
    quarter_start_data = mark_official_models(quarter_start_data)

    # æŒ‰ç³»åˆ—ç­›é€‰
    def filter_series(df):
        if df.empty or not selected_series:
            return df
        series_mapping = {
            "ERNIE-4.5": "ernie-4.5",
            "PaddleOCR-VL": "paddleocr-vl",
            "å…¶ä»–ERNIE": "other-ernie"
        }
        selected_categories = [series_mapping.get(s, s) for s in selected_series]
        return df[df['model_category'].isin(selected_categories)].copy()

    current_data = filter_series(current_data)
    last_week_data = filter_series(last_week_data)
    quarter_start_data = filter_series(quarter_start_data)

    # è·å–è¡ç”Ÿæ¨¡å‹
    current_derivatives = current_data[current_data['is_official'] == False].copy()
    last_week_derivatives = last_week_data[last_week_data['is_official'] == False].copy()
    quarter_start_derivatives = quarter_start_data[quarter_start_data['is_official'] == False].copy()

    # ç´¯è®¡æ•°é‡
    total_count = len(current_derivatives)

    # æœ¬å‘¨æ–°å¢ï¼šåœ¨å½“å‰æ—¥æœŸå­˜åœ¨ä½†ä¸Šå‘¨ä¸å­˜åœ¨çš„æ¨¡å‹
    current_keys = set(zip(current_derivatives['repo'], current_derivatives['publisher'], current_derivatives['model_name']))
    last_week_keys = set(zip(last_week_derivatives['repo'], last_week_derivatives['publisher'], last_week_derivatives['model_name']))
    weekly_new_keys = current_keys - last_week_keys
    weekly_new_count = len(weekly_new_keys)

    # å­£åº¦æ–°å¢
    quarter_start_keys = set(zip(quarter_start_derivatives['repo'], quarter_start_derivatives['publisher'], quarter_start_derivatives['model_name']))
    quarter_new_keys = current_keys - quarter_start_keys
    quarter_new_count = len(quarter_new_keys)

    # æœ¬å‘¨æ–°å¢æ¨¡å‹åˆ—è¡¨
    weekly_new_models = []
    for repo, publisher, model_name in weekly_new_keys:
        model_row = current_derivatives[
            (current_derivatives['repo'] == repo) &
            (current_derivatives['publisher'] == publisher) &
            (current_derivatives['model_name'] == model_name)
        ].iloc[0]

        weekly_new_models.append({
            'repo': repo,
            'publisher': publisher,
            'model_name': model_name,
            'download_count': int(model_row.get('download_count', 0)),
            'model_category': model_row.get('model_category', ''),
            'model_type': model_row.get('model_type', '')
        })

    # æŒ‰ä¸‹è½½é‡æ’åº
    weekly_new_models = sorted(weekly_new_models, key=lambda x: x['download_count'], reverse=True)

    # æŒ‰ç³»åˆ—ç»Ÿè®¡
    stats_by_series = {}
    if 'model_category' in current_data.columns:
        for category in current_data['model_category'].dropna().unique():
            cat_current = current_derivatives[current_derivatives['model_category'] == category]
            cat_last_week = last_week_derivatives[last_week_derivatives['model_category'] == category]
            cat_quarter_start = quarter_start_derivatives[quarter_start_derivatives['model_category'] == category]

            cat_current_keys = set(zip(cat_current['repo'], cat_current['publisher'], cat_current['model_name']))
            cat_last_week_keys = set(zip(cat_last_week['repo'], cat_last_week['publisher'], cat_last_week['model_name']))
            cat_quarter_start_keys = set(zip(cat_quarter_start['repo'], cat_quarter_start['publisher'], cat_quarter_start['model_name']))

            stats_by_series[category] = {
                'total_count': len(cat_current),
                'weekly_new_count': len(cat_current_keys - cat_last_week_keys),
                'quarter_new_count': len(cat_current_keys - cat_quarter_start_keys)
            }

    return {
        'current_date': current_date,
        'total_count': total_count,
        'weekly_new_count': weekly_new_count,
        'quarter_new_count': quarter_new_count,
        'quarter_name': quarter_name,
        'weekly_new_models': weekly_new_models,
        'stats_by_series': stats_by_series
    }


def get_deleted_derivative_models_all_platforms(current_date, selected_series=None):
    """
    æ£€æµ‹å…¨å¹³å°å·²åˆ é™¤çš„è¡ç”Ÿæ¨¡å‹ï¼ˆåŸºäº is_official æ ‡è®°ï¼‰

    é€»è¾‘ï¼š
    - ä½¿ç”¨å›å¡«æ¨¡å¼è·å–æˆªæ­¢åˆ°å½“å‰æ—¥æœŸçš„æ‰€æœ‰å†å²æ¨¡å‹
    - ä½¿ç”¨æ­£å¸¸æ¨¡å¼è·å–å½“å‰æ—¥æœŸçš„å®é™…æ•°æ®
    - å¯¹æ¯”ä¸¤è€…ï¼Œæ‰¾å‡ºåœ¨å†å²ä¸­å­˜åœ¨ä½†å½“å‰æ—¥æœŸä¸å­˜åœ¨çš„è¡ç”Ÿæ¨¡å‹

    Args:
        current_date: å½“å‰æ—¥æœŸ (YYYY-MM-DD)
        selected_series: å¯é€‰çš„ç³»åˆ—åˆ—è¡¨ï¼Œå¦‚ ['ERNIE-4.5', 'PaddleOCR-VL']

    Returns:
        list: å·²åˆ é™¤çš„è¡ç”Ÿæ¨¡å‹åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«:
            - model_name: æ¨¡å‹åç§°
            - publisher: å‘å¸ƒè€…
            - model_category: æ¨¡å‹åˆ†ç±»
            - last_seen_date: æœ€åå‡ºç°æ—¥æœŸ
            - last_download_count: æœ€åè®°å½•çš„ä¸‹è½½é‡
            - repo: å¹³å°
    """
    try:
        # 1. è·å–æ‰€æœ‰å†å²æ¨¡å‹ï¼ˆå›å¡«æ¨¡å¼ï¼‰
        all_historical = load_data_from_db(date_filter=current_date, last_value_per_model=True)

        # 2. è·å–å½“å‰æ—¥æœŸçš„å®é™…æ•°æ®
        current_actual = load_data_from_db(date_filter=current_date, last_value_per_model=False)

        if all_historical.empty:
            return []

        # 3. åº”ç”¨æ ‡å‡†åŒ–å’Œå»é‡é€»è¾‘
        def standardize_and_deduplicate(df):
            if df.empty:
                return df
            df = df.copy()
            # æ ‡å‡†åŒ– publisher
            df['publisher'] = df['publisher'].astype(str).apply(
                lambda x: x.title() if x.lower() != 'nan' else x
            )
            # æ ‡å‡†åŒ–æ¨¡å‹åç§°
            df = normalize_model_names(df)
            # è½¬æ¢ä¸‹è½½é‡ä¸ºæ•°å­—
            df['download_count'] = pd.to_numeric(df['download_count'], errors='coerce').fillna(0)
            # å»é‡ï¼ˆæŒ‰ä¸‹è½½é‡é™åºï¼Œä¿ç•™æœ€é«˜çš„ï¼‰
            df = df.sort_values(by='download_count', ascending=False).drop_duplicates(
                subset=['date', 'repo', 'publisher', 'model_name'], keep='first'
            )
            return df

        all_historical = standardize_and_deduplicate(all_historical)
        current_actual = standardize_and_deduplicate(current_actual)

        # 4. æ ‡è®°å®˜æ–¹æ¨¡å‹
        all_historical = mark_official_models(all_historical)
        current_actual = mark_official_models(current_actual)

        # 5. ç­›é€‰è¡ç”Ÿæ¨¡å‹ï¼ˆéå®˜æ–¹ï¼‰
        historical_derivatives = all_historical[all_historical['is_official'] == False].copy()
        current_derivatives = current_actual[current_actual['is_official'] == False].copy()

        # 6. æŒ‰ç³»åˆ—ç­›é€‰ï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if selected_series:
            series_mapping = {"ERNIE-4.5": "ernie-4.5", "PaddleOCR-VL": "paddleocr-vl"}
            selected_categories = [series_mapping.get(s, s) for s in selected_series]
            historical_derivatives = historical_derivatives[
                historical_derivatives['model_category'].isin(selected_categories)
            ].copy()
            current_derivatives = current_derivatives[
                current_derivatives['model_category'].isin(selected_categories)
            ].copy()

        if historical_derivatives.empty:
            return []

        # 7. åˆ›å»ºæ¨¡å‹å”¯ä¸€æ ‡è¯† (repo, publisher, model_name)
        historical_derivatives['model_key'] = (
            historical_derivatives['repo'] + '|||' +
            historical_derivatives['publisher'] + '|||' +
            historical_derivatives['model_name']
        )

        if not current_derivatives.empty:
            current_derivatives['model_key'] = (
                current_derivatives['repo'] + '|||' +
                current_derivatives['publisher'] + '|||' +
                current_derivatives['model_name']
            )
            current_keys = set(current_derivatives['model_key'].unique())
        else:
            current_keys = set()

        historical_keys = set(historical_derivatives['model_key'].unique())

        # 8. æ‰¾å‡ºå·²åˆ é™¤çš„æ¨¡å‹
        deleted_keys = historical_keys - current_keys

        if not deleted_keys:
            return []

        # 9. è·å–å·²åˆ é™¤æ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯
        deleted_models = historical_derivatives[
            historical_derivatives['model_key'].isin(deleted_keys)
        ].copy()

        # 10. å¯¹äºæ¯ä¸ªå·²åˆ é™¤çš„æ¨¡å‹ï¼Œæ‰¾åˆ°å®ƒæœ€åå‡ºç°çš„æ—¥æœŸ
        deleted_models_info = []

        for _, row in deleted_models.iterrows():
            model_key_parts = row['model_key'].split('|||')
            repo = model_key_parts[0]
            publisher = model_key_parts[1]
            model_name = model_key_parts[2]

            # æŸ¥è¯¢è¯¥æ¨¡å‹åœ¨æ•°æ®åº“ä¸­æœ€åå‡ºç°çš„æ—¥æœŸ
            conn = sqlite3.connect(DB_PATH)
            query = """
                SELECT date, download_count
                FROM model_downloads
                WHERE repo = ? AND LOWER(publisher) = LOWER(?) AND model_name = ?
                ORDER BY date DESC
                LIMIT 1
            """
            result = pd.read_sql_query(query, conn, params=(repo, publisher, model_name))
            conn.close()

            if not result.empty:
                last_seen_date = result.iloc[0]['date']
                last_download_count = result.iloc[0]['download_count']
            else:
                last_seen_date = row.get('date', 'Unknown')
                last_download_count = row.get('download_count', 0)

            model_info = {
                'model_name': model_name,
                'publisher': publisher,
                'model_category': row.get('model_category', ''),
                'last_seen_date': last_seen_date,
                'last_download_count': int(last_download_count) if pd.notna(last_download_count) else 0,
                'repo': repo
            }

            deleted_models_info.append(model_info)

        # 11. æŒ‰æœ€åå‡ºç°æ—¥æœŸé™åºæ’åº
        deleted_models_info = sorted(
            deleted_models_info,
            key=lambda x: x['last_seen_date'],
            reverse=True
        )

        return deleted_models_info

    except Exception as e:
        print(f"æ£€æµ‹å·²åˆ é™¤è¡ç”Ÿæ¨¡å‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return []


def get_models_needing_backfill(current_date, selected_series=None):
    """
    æ£€æµ‹éœ€è¦å›å¡«çš„æ¨¡å‹ï¼ˆæœ€åä¸€å¤©ä¸‹è½½é‡ä¸æ˜¯å†å²æœ€å¤§å€¼ï¼‰

    é€»è¾‘ï¼š
    - å¯¹äºå½“å‰æ—¥æœŸå­˜åœ¨çš„æ¯ä¸ªæ¨¡å‹
    - æŸ¥è¯¢è¯¥æ¨¡å‹çš„å†å²æœ€å¤§ä¸‹è½½é‡
    - å¦‚æœå½“å‰æ—¥æœŸçš„ä¸‹è½½é‡ < å†å²æœ€å¤§å€¼ï¼Œåˆ™è¯¥æ¨¡å‹éœ€è¦å›å¡«

    Args:
        current_date: å½“å‰æ—¥æœŸ (YYYY-MM-DD)
        selected_series: å¯é€‰çš„ç³»åˆ—åˆ—è¡¨ï¼Œå¦‚ ['ERNIE-4.5', 'PaddleOCR-VL']

    Returns:
        list: éœ€è¦å›å¡«çš„æ¨¡å‹åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«:
            - model_name: æ¨¡å‹åç§°
            - publisher: å‘å¸ƒè€…
            - model_category: æ¨¡å‹åˆ†ç±»
            - repo: å¹³å°
            - current_download_count: å½“å‰æ—¥æœŸä¸‹è½½é‡
            - max_download_count: å†å²æœ€å¤§ä¸‹è½½é‡
            - max_download_date: å†å²æœ€å¤§ä¸‹è½½é‡çš„æ—¥æœŸ
    """
    try:
        # 1. è·å–å½“å‰æ—¥æœŸçš„å®é™…æ•°æ®
        current_data = load_data_from_db(date_filter=current_date, last_value_per_model=False)

        if current_data.empty:
            return []

        # 2. åº”ç”¨æ ‡å‡†åŒ–å’Œå»é‡
        current_data = current_data.copy()
        current_data['publisher'] = current_data['publisher'].astype(str).apply(
            lambda x: x.title() if x.lower() != 'nan' else x
        )
        current_data = normalize_model_names(current_data)
        current_data['download_count'] = pd.to_numeric(current_data['download_count'], errors='coerce').fillna(0)
        current_data = current_data.sort_values(by='download_count', ascending=False).drop_duplicates(
            subset=['date', 'repo', 'publisher', 'model_name'], keep='first'
        )

        # 3. æ ‡è®°å®˜æ–¹æ¨¡å‹å¹¶ç­›é€‰è¡ç”Ÿæ¨¡å‹
        current_data = mark_official_models(current_data)
        current_derivatives = current_data[current_data['is_official'] == False].copy()

        # 4. æŒ‰ç³»åˆ—ç­›é€‰ï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if selected_series:
            series_mapping = {"ERNIE-4.5": "ernie-4.5", "PaddleOCR-VL": "paddleocr-vl"}
            selected_categories = [series_mapping.get(s, s) for s in selected_series]
            current_derivatives = current_derivatives[
                current_derivatives['model_category'].isin(selected_categories)
            ].copy()

        if current_derivatives.empty:
            return []

        # 5. å¯¹äºæ¯ä¸ªæ¨¡å‹ï¼ŒæŸ¥è¯¢å†å²æœ€å¤§ä¸‹è½½é‡
        models_needing_backfill = []

        conn = sqlite3.connect(DB_PATH)

        for _, row in current_derivatives.iterrows():
            repo = row['repo']
            publisher = row['publisher']
            model_name = row['model_name']
            current_download = row['download_count']

            # æŸ¥è¯¢å†å²æœ€å¤§ä¸‹è½½é‡
            query = """
                SELECT MAX(download_count) as max_count, date
                FROM model_downloads
                WHERE repo = ? AND LOWER(publisher) = LOWER(?) AND model_name = ?
                GROUP BY repo, publisher, model_name
                ORDER BY max_count DESC
                LIMIT 1
            """
            result = pd.read_sql_query(query, conn, params=(repo, publisher, model_name))

            if not result.empty:
                max_download = pd.to_numeric(result.iloc[0]['max_count'], errors='coerce')
                if pd.notna(max_download) and max_download > 0:
                    # å¦‚æœå½“å‰ä¸‹è½½é‡ < å†å²æœ€å¤§å€¼ï¼Œåˆ™éœ€è¦å›å¡«
                    if current_download < max_download:
                        # æŸ¥è¯¢æœ€å¤§ä¸‹è½½é‡çš„æ—¥æœŸ
                        date_query = """
                            SELECT date
                            FROM model_downloads
                            WHERE repo = ? AND LOWER(publisher) = LOWER(?)
                                  AND model_name = ? AND download_count = ?
                            ORDER BY date DESC
                            LIMIT 1
                        """
                        date_result = pd.read_sql_query(
                            date_query, conn,
                            params=(repo, publisher, model_name, max_download)
                        )
                        max_date = date_result.iloc[0]['date'] if not date_result.empty else 'Unknown'

                        model_info = {
                            'model_name': model_name,
                            'publisher': publisher,
                            'model_category': row.get('model_category', ''),
                            'repo': repo,
                            'current_download_count': int(current_download),
                            'max_download_count': int(max_download),
                            'max_download_date': max_date
                        }

                        models_needing_backfill.append(model_info)

        conn.close()

        # 6. æŒ‰å·®å€¼æ’åºï¼ˆå·®å€¼è¶Šå¤§æ’åœ¨è¶Šå‰é¢ï¼‰
        models_needing_backfill = sorted(
            models_needing_backfill,
            key=lambda x: x['max_download_count'] - x['current_download_count'],
            reverse=True
        )

        return models_needing_backfill

    except Exception as e:
        print(f"æ£€æµ‹éœ€è¦å›å¡«çš„æ¨¡å‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return []
