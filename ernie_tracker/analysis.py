"""
æ•°æ®åˆ†ææ¨¡å— - å‘¨æŠ¥ç»Ÿè®¡åˆ†æ
"""
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import sqlite3
from .db import load_data_from_db
from .config import DB_PATH


# ç™¾åº¦å®˜æ–¹æ¨¡å‹è¯†åˆ«è§„åˆ™
OFFICIAL_RULES = {
    'Hugging Face': 'baidu',
    'AI Studio': 'PaddlePaddle',
    'ModelScope': 'é£æ¡¨PaddlePaddle',
    'GitCode': 'é£æ¡¨PaddlePaddle',
    'é²¸æ™º': 'PaddlePaddle',
    'é­”ä¹ Modelers': 'PaddlePaddle',
    'Gitee': 'PaddlePaddle'
}

# æ¨¡å‹é¡ºåºï¼ˆæŒ‰é‡è¦æ€§æ’åˆ—ï¼‰
MODEL_ORDER = [
    'ERNIE-4.5-VL-424B-A47B-Paddle',
    'ERNIE-4.5-VL-424B-A47B-PT',
    'ERNIE-4.5-VL-424B-A47B-Base-Paddle',
    'ERNIE-4.5-VL-424B-A47B-Base-PT',
    'ERNIE-4.5-300B-A47B-Paddle',
    'ERNIE-4.5-300B-A47B-PT',
    'ERNIE-4.5-300B-A47B-Base-Paddle',
    'ERNIE-4.5-300B-A47B-Base-PT',
    'ERNIE-4.5-300B-A47B-W4A8C8-TP4-Paddle',
    'ERNIE-4.5-300B-A47B-FP8-Paddle',
    'ERNIE-4.5-300B-A47B-2Bits-Paddle',
    'ERNIE-4.5-300B-A47B-2Bits-TP2-Paddle',
    'ERNIE-4.5-300B-A47B-2Bits-TP4-Paddle',
    'ERNIE-4.5-VL-28B-A3B-Paddle',
    'ERNIE-4.5-VL-28B-A3B-PT',
    'ERNIE-4.5-VL-28B-A3B-Thinking',
    'ERNIE-4.5-VL-28B-A3B-Base-Paddle',
    'ERNIE-4.5-VL-28B-A3B-Base-PT',
    'ERNIE-4.5-21B-A3B-Paddle',
    'ERNIE-4.5-21B-A3B-PT',
    'ERNIE-4.5-21B-A3B-Thinking',
    'ERNIE-4.5-21B-A3B-Base-Paddle',
    'ERNIE-4.5-21B-A3B-Base-PT',
    'ERNIE-4.5-0.3B-Paddle',
    'ERNIE-4.5-0.3B-PT',
    'ERNIE-4.5-0.3B-Base-Paddle',
    'ERNIE-4.5-0.3B-Base-PT'
]

PADDLEOCR_VL_MODEL_ORDER = [
    'PaddleOCR-VL'
]

# å¹³å°é¡ºåº
REPO_ORDER = ['Hugging Face', 'AI Studio', 'ModelScope', 'GitCode', 'å…¶ä»–']

# è¯¦ç»†å¹³å°é¡ºåºï¼ˆä¸åˆå¹¶"å…¶ä»–"ï¼‰
REPO_ORDER_DETAILED = ['Hugging Face', 'AI Studio', 'ModelScope', 'GitCode', 'é­”ä¹ Modelers', 'é²¸æ™º', 'Gitee']


def get_last_friday(current_date=None):
    """
    è·å–ä¸Šå‘¨äº”çš„æ—¥æœŸ

    Args:
        current_date: å½“å‰æ—¥æœŸï¼Œé»˜è®¤ä¸ºä»Šå¤©

    Returns:
        str: ä¸Šå‘¨äº”çš„æ—¥æœŸå­—ç¬¦ä¸² (YYYY-MM-DD)
    """
    if current_date is None:
        current_date = datetime.now()
    elif isinstance(current_date, str):
        current_date = datetime.strptime(current_date, '%Y-%m-%d')

    # è·å–å½“å‰æ˜¯æ˜ŸæœŸå‡  (0=Monday, 6=Sunday)
    current_weekday = current_date.weekday()

    # è®¡ç®—åˆ°ä¸Šå‘¨äº”çš„å¤©æ•°
    # å¦‚æœä»Šå¤©æ˜¯å‘¨ä¸€(0)ï¼Œä¸Šå‘¨äº”æ˜¯3å¤©å‰
    # å¦‚æœä»Šå¤©æ˜¯å‘¨äº”(4)ï¼Œä¸Šå‘¨äº”æ˜¯7å¤©å‰
    if current_weekday >= 4:  # å‘¨äº”ã€å‘¨å…­ã€å‘¨æ—¥
        days_to_last_friday = current_weekday - 4 + 7
    else:  # å‘¨ä¸€åˆ°å‘¨å››
        days_to_last_friday = current_weekday + 3

    last_friday = current_date - timedelta(days=days_to_last_friday)
    return last_friday.strftime('%Y-%m-%d')


def get_available_dates():
    """
    è·å–æ•°æ®åº“ä¸­æ‰€æœ‰å¯ç”¨çš„æ—¥æœŸ

    Returns:
        list: æ—¥æœŸåˆ—è¡¨
    """
    conn = sqlite3.connect(DB_PATH)
    query = "SELECT DISTINCT date FROM model_downloads ORDER BY date DESC"
    df = pd.read_sql_query(query, conn)
    conn.close()
    return df['date'].tolist()


def normalize_model_names(data):
    """
    æ ‡å‡†åŒ–æ¨¡å‹åç§°ï¼šç§»é™¤ model_name ä¸­çš„ publisher å‰ç¼€

    ä¾‹å¦‚ï¼š'paddlepaddle/ERNIE-4.5-0.3B-PT' -> 'ERNIE-4.5-0.3B-PT'

    è¿™ç¡®ä¿äº†å³ä½¿æ•°æ®åº“ä¸­å­˜å‚¨çš„æ¨¡å‹åç§°æ ¼å¼ä¸ä¸€è‡´ï¼Œ
    åœ¨åˆ†ææ—¶ä¹Ÿèƒ½æ­£ç¡®åŒ¹é…å’Œæ¯”è¾ƒã€‚
    """
    data = data.copy()

    def remove_publisher_prefix(row):
        model_name = str(row['model_name']).strip()
        publisher = str(row['publisher']).strip()

        # å¦‚æœæ¨¡å‹åç§°ä»¥ "publisher/" å¼€å¤´ï¼ˆå¿½ç•¥å¤§å°å†™ï¼‰ï¼Œç§»é™¤å‰ç¼€
        if publisher and publisher.lower() != 'nan' and '/' in model_name:
            parts = model_name.split('/', 1)
            if len(parts) == 2 and parts[0].lower() == publisher.lower():
                return parts[1]

        return model_name

    data['model_name'] = data.apply(remove_publisher_prefix, axis=1)
    return data


def mark_official_models(data):
    """
    æ ‡è®°å®˜æ–¹æ¨¡å‹ã€‚
    å¦‚æœ publisher åŒ…å« 'ç™¾åº¦', 'baidu', æˆ– 'Paddle' (ä¸åŒºåˆ†å¤§å°å†™)ï¼Œåˆ™è§†ä¸ºå®˜æ–¹æ¨¡å‹ã€‚
    """
    data = data.copy()
    # ç¡®ä¿ publisher åˆ—æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œä»¥ä¾¿è¿›è¡Œæ–‡æœ¬æ“ä½œ
    data['publisher'] = data['publisher'].astype(str)

    keywords = ['ç™¾åº¦', 'baidu', 'Paddle', 'yiyan', 'ä¸€è¨€']
    # åˆ›å»ºä¸€ä¸ªæ­£åˆ™è¡¨è¾¾å¼ï¼Œç”¨ | (OR) è¿æ¥å…³é”®å­—
    pattern = '|'.join(keywords)

    # ä½¿ç”¨ str.contains è¿›è¡Œä¸åŒºåˆ†å¤§å°å†™çš„åŒ¹é…
    data['is_official'] = data['publisher'].str.contains(pattern, case=False, na=False)

    return data


def create_pivot_table(data, repo_order=None, model_order=None, group_by_publisher=False, merge_other=True):
    """
    åˆ›å»ºæ•°æ®é€è§†è¡¨

    Args:
        data: DataFrame
        repo_order: å¹³å°é¡ºåºåˆ—è¡¨
        model_order: æ¨¡å‹é¡ºåºåˆ—è¡¨. å¦‚æœä¸º None, åˆ™ä¸æŒ‰ç‰¹å®šæ¨¡å‹é¡ºåºå¤„ç†.
        group_by_publisher: æ˜¯å¦æŒ‰ publisher åˆ†ç»„ï¼ˆç”¨äºè¡ç”Ÿæ¨¡å‹ï¼‰ã€‚
                           å¦‚æœä¸º Trueï¼Œç´¢å¼•ä¸º (model_name, publisher)ï¼›
                           å¦‚æœä¸º Falseï¼Œç´¢å¼•ä»…ä¸º model_nameï¼ˆç”¨äºå®˜æ–¹æ¨¡å‹ï¼‰
        merge_other: æ˜¯å¦åˆå¹¶é­”ä¹ Modelersã€é²¸æ™ºã€Giteeä¸º"å…¶ä»–"ï¼ˆé»˜è®¤Trueï¼‰

    Returns:
        DataFrame: é€è§†è¡¨
    """
    if repo_order is None:
        repo_order = REPO_ORDER if merge_other else REPO_ORDER_DETAILED

    # ç¡®ä¿ download_count æ˜¯æ•°å€¼ç±»å‹
    data = data.copy()
    data['download_count'] = pd.to_numeric(data['download_count'], errors='coerce').fillna(0)

    # åˆå¹¶å¹³å°ï¼ˆä»…å½“ merge_other=True æ—¶ï¼‰
    if merge_other:
        data['repo'] = data['repo'].replace(['é­”ä¹ Modelers', 'é²¸æ™º', 'Gitee'], 'å…¶ä»–')

    # æ ¹æ® group_by_publisher å†³å®šç´¢å¼•
    if group_by_publisher:
        # è¡ç”Ÿæ¨¡å‹ï¼šä½¿ç”¨ (model_name, publisher) ä½œä¸ºç´¢å¼•
        pivot_df = pd.pivot_table(
            data,
            values='download_count',
            index=['model_name', 'publisher'],
            columns='repo',
            aggfunc='sum',
            fill_value=0
        )
    else:
        # å®˜æ–¹æ¨¡å‹ï¼šä½¿ç”¨ model_name ä½œä¸ºç´¢å¼•
        pivot_df = pd.pivot_table(
            data,
            values='download_count',
            index='model_name',
            columns='repo',
            aggfunc='sum',
            fill_value=0
        )

    # ç¡®ä¿æ‰€æœ‰å¹³å°éƒ½åœ¨åˆ—ä¸­
    for repo in repo_order:
        if repo not in pivot_df.columns:
            pivot_df[repo] = 0

    # æŒ‰æŒ‡å®šé¡ºåºæ’åˆ—åˆ—
    available_repos = [repo for repo in repo_order if repo in pivot_df.columns]
    pivot_df = pivot_df[available_repos]

    # å¦‚æœæä¾›äº† model_orderï¼Œåˆ™æŒ‰å…¶å¤„ç†
    if model_order:
        # ç¡®ä¿æ‰€æœ‰æŒ‡å®šæ¨¡å‹éƒ½åœ¨ç´¢å¼•ä¸­
        for model in model_order:
            if model not in pivot_df.index:
                pivot_df.loc[model] = [0] * len(available_repos)

        # æŒ‰æŒ‡å®šé¡ºåºæ’åˆ—è¡Œ
        available_models = [model for model in model_order if model in pivot_df.index]
        pivot_df = pivot_df.reindex(available_models)

    return pivot_df


def get_all_new_models(current_date, previous_date, model_series='ERNIE-4.5'):
    """
    è·å–æœ¬å‘¨æ–°å¢çš„æ‰€æœ‰æ¨¡å‹ï¼ˆå®Œæ•´åˆ—è¡¨ï¼‰

    ğŸ”§ ä¿®å¤ï¼šç›´æ¥ä»æ•°æ®åº“åŠ è½½æ•°æ®ï¼Œç¡®ä¿ä¸ get_weekly_new_finetune_adapters() ä½¿ç”¨ç›¸åŒçš„ç­›é€‰é€»è¾‘

    Args:
        current_date: å½“å‰æ—¥æœŸ (YYYY-MM-DD)
        previous_date: å¯¹æ¯”æ—¥æœŸ (YYYY-MM-DD)
        model_series: æ¨¡å‹ç³»åˆ— ('ERNIE-4.5' æˆ– 'PaddleOCR-VL')

    Returns:
        dict: åŒ…å«æ–°å¢æ¨¡å‹ä¿¡æ¯çš„å­—å…¸
    """
    try:
        # ä»æ•°æ®åº“åŠ è½½æ•°æ®ï¼ˆä¸ get_weekly_new_finetune_adapters ä½¿ç”¨ç›¸åŒçš„åŠ è½½æ–¹å¼ï¼‰
        current_data = load_data_from_db(date_filter=current_date)
        previous_data = load_data_from_db(date_filter=previous_date)

        if current_data.empty:
            return {
                'new_models_list': [],
                'total_new': 0,
                'summary': f'æœ¬å‘¨æ²¡æœ‰æ–°å¢{model_series}æ¨¡å‹'
            }

        # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ä¸ get_weekly_new_finetune_adapters() ç›¸åŒçš„ç­›é€‰é€»è¾‘
        # æ ¹æ® model_series ç¡®å®šè¦ç­›é€‰çš„ model_category
        if model_series == 'ERNIE-4.5':
            target_category = 'ernie-4.5'
        elif model_series == 'PaddleOCR-VL':
            target_category = 'paddleocr-vl'
        else:
            target_category = 'ernie-4.5'

        # ç­›é€‰Hugging Faceå¹³å°çš„æŒ‡å®šç³»åˆ—æ¨¡å‹ï¼ˆä½¿ç”¨ model_category å­—æ®µï¼‰
        hf_current = current_data[
            (current_data['repo'] == 'Hugging Face') &
            (current_data['model_category'] == target_category)
        ].copy()

        if previous_data.empty:
            hf_previous = pd.DataFrame()
        else:
            hf_previous = previous_data[
                (previous_data['repo'] == 'Hugging Face') &
                (previous_data['model_category'] == target_category)
            ].copy()

        # æ‰¾å‡ºåœ¨å½“å‰æ•°æ®ä¸­ä½†ä¸åœ¨å¯¹æ¯”æ•°æ®ä¸­çš„æ¨¡å‹ï¼ˆæŒ‰ publisher+model_name å»é‡ï¼‰
        if hf_previous.empty:
            new_models = hf_current.copy()
        else:
            previous_keys = set(zip(hf_previous['publisher'], hf_previous['model_name']))
            current_keys = set(zip(hf_current['publisher'], hf_current['model_name']))
            new_keys = current_keys - previous_keys
            new_models = hf_current[hf_current.apply(lambda r: (r['publisher'], r['model_name']) in new_keys, axis=1)].copy()

        if new_models.empty:
            return {
                'new_models_list': [],
                'total_new': 0,
                'summary': f'æœ¬å‘¨æ²¡æœ‰æ–°å¢{model_series}æ¨¡å‹'
            }

        # æŒ‰å¹³å°åˆ†ç»„ï¼Œæ¯ä¸ªæ¨¡å‹åªä¿ç•™ä¸€æ¡è®°å½•ï¼ˆé€‰æ‹©ä¸‹è½½é‡æœ€å¤§çš„å¹³å°ï¼‰
        new_models_dedup = new_models.sort_values('download_count', ascending=False).drop_duplicates(
            subset=['publisher', 'model_name'], keep='first'
        )

        # æ ¼å¼åŒ–æ¨¡å‹åˆ—è¡¨ï¼ŒåŒ…å«æ›´å¤šä¿¡æ¯
        models_list = []
        for _, row in new_models_dedup.iterrows():
            model_info = {
                'model_name': row['model_name'],
                'publisher': row['publisher'],
                'repo': row['repo'],
                'download_count': int(row['download_count']),
            }

            # æ·»åŠ å¯é€‰å­—æ®µ
            if 'model_type' in row and pd.notna(row['model_type']):
                model_info['model_type'] = row['model_type']
            if 'model_category' in row and pd.notna(row['model_category']):
                model_info['model_category'] = row['model_category']
            if 'base_model' in row and pd.notna(row['base_model']):
                model_info['base_model'] = row['base_model']

            models_list.append(model_info)

        # æŒ‰ä¸‹è½½é‡é™åºæ’åº
        models_list = sorted(models_list, key=lambda x: x['download_count'], reverse=True)

        return {
            'new_models_list': models_list,
            'total_new': len(models_list),
            'summary': f'æœ¬å‘¨å…±å‘ç° {len(models_list)} ä¸ªæ–°å¢{model_series}æ¨¡å‹'
        }

    except Exception as e:
        print(f"è·å–æœ¬å‘¨æ–°å¢æ¨¡å‹å®Œæ•´åˆ—è¡¨å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {
            'new_models_list': [],
            'total_new': 0,
            'summary': f'è·å–æ•°æ®æ—¶å‡ºé”™: {e}'
        }


def calculate_weekly_report(current_date=None, previous_date=None, model_order=None, model_series='ERNIE-4.5'):
    """
    è®¡ç®—å‘¨æŠ¥æ•°æ®

    Args:
        current_date: å½“å‰æ—¥æœŸ (YYYY-MM-DD)ï¼Œé»˜è®¤ä¸ºä»Šå¤©
        previous_date: å¯¹æ¯”æ—¥æœŸ (YYYY-MM-DD)ï¼Œé»˜è®¤ä¸ºä¸Šå‘¨äº”
        model_order: æ¨¡å‹é¡ºåºåˆ—è¡¨
        model_series: æ¨¡å‹ç³»åˆ— ('ERNIE-4.5' æˆ– 'PaddleOCR-VL')

    Returns:
        dict: åŒ…å«å„ç§ç»Ÿè®¡æ•°æ®çš„å­—å…¸
    """
    if model_order is None:
        model_order = MODEL_ORDER if model_series == 'ERNIE-4.5' else PADDLEOCR_VL_MODEL_ORDER

    # è®¾ç½®æ—¥æœŸ
    if current_date is None:
        current_date = datetime.now().strftime('%Y-%m-%d')
    if previous_date is None:
        previous_date = get_last_friday(current_date)

    # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ load_data_from_db() è·å–å»é‡åçš„æ•°æ®
    # è¿™ç¡®ä¿äº†é‡å¤è®°å½•åªå–æœ€å¤§ä¸‹è½½é‡ï¼Œé¿å…é‡å¤è®¡ç®—
    current_data = load_data_from_db(date_filter=current_date, last_value_per_model=True)
    previous_data = load_data_from_db(date_filter=previous_date, last_value_per_model=True)

    # è´Ÿå¢é•¿æ£€æµ‹ä½¿ç”¨çœŸå®çš„å½“æ—¥è®°å½•ï¼ˆä¸å¸¦ last_value_per_modelï¼‰ï¼Œå•ç‹¬åŠ è½½
    warn_current_raw = load_data_from_db(date_filter=current_date, last_value_per_model=False)
    warn_previous_raw = load_data_from_db(date_filter=previous_date, last_value_per_model=False)

    # ğŸ”´ å…³é”®ä¿®å¤ï¼šåœ¨åˆå¹¶å’Œè¿›ä¸€æ­¥å¤„ç†ä¹‹å‰ï¼Œå¯¹æ•°æ®è¿›è¡Œå¼ºåˆ¶æ ‡å‡†åŒ–å’ŒäºŒæ¬¡å»é‡
    # ç¡®ä¿å³ä½¿æ•°æ®åº“ä¸­å­˜åœ¨ä¸ä¸€è‡´ï¼Œä¹Ÿèƒ½åœ¨åˆ†ææ—¶å¾—åˆ°ä¿®æ­£
    def enforce_deduplication_and_standardization(df):
        if df.empty:
            return df
        
        # 1. æ ‡å‡†åŒ– publisher åç§°ï¼ˆç»Ÿä¸€å¤§å°å†™ï¼‰
        df['publisher'] = df['publisher'].astype(str).apply(lambda x: x.title() if x.lower() != 'nan' else x)
        
        # 2. æ ‡å‡†åŒ–æ¨¡å‹åç§°ï¼ˆç§»é™¤ publisher å‰ç¼€ï¼‰
        df = normalize_model_names(df)
        
        # 3. å†æ¬¡å»é‡ï¼Œç¡®ä¿åŒä¸€ (date, repo, publisher, model_name) åªæœ‰ä¸€æ¡è®°å½•ï¼Œä¸”ä¸‹è½½é‡æœ€å¤§
        # æŒ‰ç…§ download_count é™åºæ’åºï¼Œç„¶åä¿ç•™æ¯ä¸ªåˆ†ç»„çš„ç¬¬ä¸€ä¸ª
        df['download_count'] = pd.to_numeric(df['download_count'], errors='coerce').fillna(0)
        df = df.sort_values(by='download_count', ascending=False).drop_duplicates(
            subset=['date', 'repo', 'publisher', 'model_name'], keep='first'
        )
        return df

    current_data = enforce_deduplication_and_standardization(current_data)
    previous_data = enforce_deduplication_and_standardization(previous_data)
    warn_current_raw = enforce_deduplication_and_standardization(warn_current_raw)
    warn_previous_raw = enforce_deduplication_and_standardization(warn_previous_raw)

    # åˆå¹¶ä¸¤ä¸ªæ—¥æœŸçš„æ•°æ®
    data = pd.concat([current_data, previous_data], ignore_index=True)

    if data.empty:
        return None

    # ğŸ”´ é‡è¦ï¼šæ ‡å‡†åŒ–æ¨¡å‹åç§°ï¼ˆç§»é™¤ publisher å‰ç¼€ï¼‰
    # è¿™ä¸€æ­¥åœ¨ enforce_deduplication_and_standardization ä¸­å·²ç»å®Œæˆï¼Œæ­¤å¤„å¯ä»¥ç§»é™¤æˆ–ä¿ç•™ä½œä¸ºå†—ä½™æ£€æŸ¥
    # ä¸ºäº†é¿å…é‡å¤å¤„ç†ï¼Œæ­¤å¤„ä¸å†é‡å¤è°ƒç”¨ normalize_model_names
    # data = normalize_model_names(data)

    # ğŸ”§ ä¿®å¤ï¼šæ ¹æ® model_series ä½¿ç”¨ model_category å­—æ®µ **æˆ–** æ¨¡å‹åç§°ç­›é€‰
    # è¿™æ ·æ—¢èƒ½åŒ…å«æ­£ç¡®åˆ†ç±»çš„è¡ç”Ÿæ¨¡å‹ï¼Œä¹Ÿèƒ½åŒ…å«å…¶ä»–å¹³å°çš„å®˜æ–¹æ¨¡å‹
    if model_series == 'ERNIE-4.5':
        # ç­›é€‰æ¡ä»¶ï¼šmodel_category == 'ernie-4.5' æˆ– model_name åŒ…å« 'ERNIE-4.5'
        if 'model_category' in data.columns:
            condition = (
                (data['model_category'] == 'ernie-4.5') |  # HuggingFace æ­£ç¡®åˆ†ç±»çš„æ¨¡å‹
                (data['model_name'].str.contains('ERNIE-4.5', case=False, na=False))  # å…¶ä»–å¹³å°çš„å®˜æ–¹æ¨¡å‹
            )
            data = data[condition].copy()
        else:
            # å›é€€åˆ°åç§°æœç´¢ï¼ˆå…¼å®¹æ—§æ•°æ®ï¼‰
            data = data[data['model_name'].str.contains('ERNIE-4.5', case=False, na=False)].copy()
    elif model_series == 'PaddleOCR-VL':
        # ç­›é€‰æ¡ä»¶ï¼šmodel_category == 'paddleocr-vl' æˆ– model_name åŒ…å« 'PaddleOCR-VL'
        if 'model_category' in data.columns:
            condition = (
                (data['model_category'] == 'paddleocr-vl') |  # HuggingFace æ­£ç¡®åˆ†ç±»çš„æ¨¡å‹
                (data['model_name'].str.contains('PaddleOCR-VL', case=False, na=False))  # å…¶ä»–å¹³å°çš„å®˜æ–¹æ¨¡å‹
            )
            data = data[condition].copy()
        else:
            # å›é€€åˆ°åç§°æœç´¢ï¼ˆå…¼å®¹æ—§æ•°æ®ï¼‰
            data = data[data['model_name'].str.contains('PaddleOCR-VL', case=False, na=False)].copy()

    if data.empty:
        print(f"è­¦å‘Š: åœ¨é€‰å®šæ—¥æœŸå†…æœªæ‰¾åˆ° {model_series} ç³»åˆ—çš„æ¨¡å‹æ•°æ®ã€‚")
        return None

    # ç¡®ä¿ 'download_count' æ˜¯æ•°å€¼ç±»å‹
    data['download_count'] = pd.to_numeric(data['download_count'], errors='coerce').fillna(0)

    # æ ‡è®°å®˜æ–¹æ¨¡å‹
    data = mark_official_models(data)
    # è´Ÿå¢é•¿æ£€æµ‹ç”¨çš„åŸå§‹å½“æ—¥æ•°æ®ä¹Ÿéœ€è¦å®˜æ–¹æ ‡è®°
    warn_current_raw = mark_official_models(warn_current_raw)
    warn_previous_raw = mark_official_models(warn_previous_raw)

    # ç­›é€‰å®˜æ–¹æ¨¡å‹
    official_data = data[data['is_official'] == True].copy()

    if official_data.empty:
        print("è­¦å‘Š: åœ¨é€‰å®šæ—¥æœŸå†…æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„å®˜æ–¹æ¨¡å‹æ•°æ®ã€‚")
        return None

    # --- å…¨é‡æ•°æ®é€è§† (ç”¨äºå¹³å°æ€»è§ˆå’Œè¯¦ç»†æ•°æ®) ---
    all_current_data = data[data['date'] == current_date]
    all_previous_data = data[data['date'] == previous_date]
    current_pivot = create_pivot_table(all_current_data, model_order=model_order, merge_other=True)
    previous_pivot = create_pivot_table(all_previous_data, model_order=model_order, merge_other=True)
    growth_pivot = current_pivot - previous_pivot

    # --- å®˜æ–¹æ¨¡å‹æ•°æ®é€è§†ï¼ˆè¯¦ç»†å¹³å°ï¼Œä¸åˆå¹¶"å…¶ä»–"ï¼‰ ---
    # ç”¨äºæ˜¾ç¤ºè¯¦ç»†çš„å„å¹³å°æ¨¡å‹ä¸‹è½½é‡è¯¦æƒ…è¡¨æ ¼
    current_official_data = official_data[official_data['date'] == current_date]
    previous_official_data = official_data[official_data['date'] == previous_date]
    current_official_pivot = create_pivot_table(current_official_data, model_order=model_order, merge_other=False, repo_order=REPO_ORDER_DETAILED)
    previous_official_pivot = create_pivot_table(previous_official_data, model_order=model_order, merge_other=False, repo_order=REPO_ORDER_DETAILED)
    growth_official_pivot = current_official_pivot - previous_official_pivot

    # è®¡ç®—å®˜æ–¹æ¨¡å‹çš„æ€»è®¡ (ç”¨äºTop Næ’å)
    current_totals = current_official_pivot.sum(axis=1).sort_values(ascending=False)
    growth_totals = growth_official_pivot.sum(axis=1).sort_values(ascending=False)

    # Top 5 å¢é•¿æœ€é«˜çš„æ¨¡å‹
    top5_growth = growth_totals.head(5)

    # Top 3 æ€»ä¸‹è½½é‡æœ€é«˜çš„æ¨¡å‹
    top3_downloads = current_totals.head(3)

    # --- è¡ç”Ÿæ¨¡å‹æ•°æ® ---
    derivative_data = data[data['is_official'] == False].copy()
    current_derivative_data = derivative_data[derivative_data['date'] == current_date]
    previous_derivative_data = derivative_data[derivative_data['date'] == previous_date]
    # æ³¨æ„ï¼šæ­¤å¤„ model_order=Noneï¼Œä»¥åŒ…å«æ‰€æœ‰è¡ç”Ÿæ¨¡å‹
    # ğŸ”´ é‡è¦ï¼šä½¿ç”¨ group_by_publisher=True æ¥åŒºåˆ†ä¸åŒ publisher çš„åŒåæ¨¡å‹
    current_derivative_pivot = create_pivot_table(current_derivative_data, model_order=None, group_by_publisher=True)
    previous_derivative_pivot = create_pivot_table(previous_derivative_data, model_order=None, group_by_publisher=True)
    
    # ç¡®ä¿ä¸¤ä¸ªé€è§†è¡¨æœ‰ç›¸åŒçš„ç´¢å¼•å’Œåˆ—ï¼Œä»¥ä¾¿ç›¸å‡
    all_derivative_models = current_derivative_pivot.index.union(previous_derivative_pivot.index)
    current_derivative_pivot = current_derivative_pivot.reindex(index=all_derivative_models, columns=REPO_ORDER, fill_value=0)
    previous_derivative_pivot = previous_derivative_pivot.reindex(index=all_derivative_models, columns=REPO_ORDER, fill_value=0)
    
    growth_derivative_pivot = current_derivative_pivot - previous_derivative_pivot

    # å„å¹³å°ä¸‹è½½é‡æœ€é«˜å’Œå¢é•¿æœ€é«˜çš„æ¨¡å‹ (è¯¦ç»†ç‰ˆ)
    def _get_top_models(repo, current_pivot, growth_pivot, data_source):
        """è¾…åŠ©å‡½æ•°ï¼Œç”¨äºè·å–æŒ‡å®šå¹³å°å’Œæ•°æ®ç±»å‹çš„é¡¶å°–æ¨¡å‹"""
        if repo not in current_pivot.columns or current_pivot[repo].sum() == 0:
            return {
                'top_download_model': 'N/A', 'top_download_publisher': '', 'top_download_count': 0, 'top_download_growth': 0,
                'top_growth_model': 'N/A', 'top_growth_publisher': '', 'top_growth_count': 0, 'top_growth_current': 0,
            }

        # æ£€æŸ¥ pivot ç´¢å¼•æ˜¯å¦ä¸ºå¤šå±‚ç´¢å¼•ï¼ˆè¡ç”Ÿæ¨¡å‹ï¼‰
        has_multiindex = isinstance(current_pivot.index, pd.MultiIndex)

        # ä¸‹è½½é‡æœ€é«˜
        top_download_idx = current_pivot[repo].idxmax()
        top_download_count = current_pivot.loc[top_download_idx, repo]
        top_download_growth = growth_pivot.loc[top_download_idx, repo] if repo in growth_pivot.columns and top_download_idx in growth_pivot.index else 0

        if has_multiindex:
            # å¤šå±‚ç´¢å¼•ï¼š(model_name, publisher)
            top_download_model, top_download_publisher = top_download_idx
        else:
            # å•å±‚ç´¢å¼•ï¼šmodel_name
            top_download_model = top_download_idx
            # ğŸ”§ ä¿®å¤ï¼šä» data_source ä¸­ç­›é€‰å‡ºå¯¹åº” repo çš„æ•°æ®å†æŸ¥æ‰¾ publisher
            filtered_data_source = data_source[data_source['repo'] == repo]
            top_download_publisher = filtered_data_source.loc[filtered_data_source['model_name'] == top_download_model, 'publisher'].iloc[0] if not filtered_data_source.loc[filtered_data_source['model_name'] == top_download_model].empty else ''

        # å¢é•¿æœ€é«˜
        if repo not in growth_pivot.columns or growth_pivot[repo].sum() == 0:
            top_growth_model, top_growth_publisher, top_growth_count, top_growth_current = 'N/A', '', 0, 0
        else:
            top_growth_idx = growth_pivot[repo].idxmax()
            top_growth_count = growth_pivot.loc[top_growth_idx, repo]
            top_growth_current = current_pivot.loc[top_growth_idx, repo]

            if has_multiindex:
                # å¤šå±‚ç´¢å¼•ï¼š(model_name, publisher)
                top_growth_model, top_growth_publisher = top_growth_idx
            else:
                # å•å±‚ç´¢å¼•ï¼šmodel_name
                top_growth_model = top_growth_idx
                # ğŸ”§ ä¿®å¤ï¼šä» data_source ä¸­ç­›é€‰å‡ºå¯¹åº” repo çš„æ•°æ®å†æŸ¥æ‰¾ publisher
                filtered_data_source = data_source[data_source['repo'] == repo]
                top_growth_publisher = filtered_data_source.loc[filtered_data_source['model_name'] == top_growth_model, 'publisher'].iloc[0] if not filtered_data_source.loc[filtered_data_source['model_name'] == top_growth_model].empty else ''

        return {
            'top_download_model': top_download_model, 'top_download_publisher': top_download_publisher,
            'top_download_count': int(top_download_count), 'top_download_growth': int(top_download_growth),
            'top_growth_model': top_growth_model, 'top_growth_publisher': top_growth_publisher,
            'top_growth_count': int(top_growth_count), 'top_growth_current': int(top_growth_current),
        }

    # --- ä¸ºâ€œå„å¹³å°æ¦œé¦–æ¨¡å‹â€å‡†å¤‡åˆå¹¶åçš„æ•°æ® ---
    # åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„ current_official_dataï¼Œå°† 'é­”ä¹ Modelers', 'é²¸æ™º', 'Gitee' åˆå¹¶åˆ° 'å…¶ä»–'
    temp_current_official_data_merged = current_official_data.copy()
    temp_current_official_data_merged['repo'] = temp_current_official_data_merged['repo'].replace(['é­”ä¹ Modelers', 'é²¸æ™º', 'Gitee'], 'å…¶ä»–')
    current_official_pivot_merged = create_pivot_table(temp_current_official_data_merged, model_order=model_order, merge_other=True)

    # åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„ previous_official_dataï¼Œå°† 'é­”ä¹ Modelers', 'é²¸æ™º', 'Gitee' åˆå¹¶åˆ° 'å…¶ä»–'
    temp_previous_official_data_merged = previous_official_data.copy()
    temp_previous_official_data_merged['repo'] = temp_previous_official_data_merged['repo'].replace(['é­”ä¹ Modelers', 'é²¸æ™º', 'Gitee'], 'å…¶ä»–')
    previous_official_pivot_merged = create_pivot_table(temp_previous_official_data_merged, model_order=model_order, merge_other=True)

    growth_official_pivot_merged = current_official_pivot_merged - previous_official_pivot_merged

    platform_top_models = []
    
    for repo in REPO_ORDER: # éå† REPO_ORDERï¼ŒåŒ…å« 'å…¶ä»–'
        if repo in ['Hugging Face', 'AI Studio', 'ModelScope', 'GitCode']: # è¿™äº›å¹³å°ä¿æŒç‹¬ç«‹
            official_tops = _get_top_models(repo, current_official_pivot, growth_official_pivot, current_official_data)
            # åªæœ‰ Hugging Face å’Œ ModelScope æœ‰è¡ç”Ÿæ¨¡å‹
            derivative_tops = None
            if repo in ['Hugging Face', 'ModelScope']:
                derivative_tops = _get_top_models(repo, current_derivative_pivot, growth_derivative_pivot, current_derivative_data)
            platform_top_models.append({
                'platform': repo,
                'official_tops': official_tops,
                'derivative_tops': derivative_tops
            })
        elif repo == 'å…¶ä»–': # 'å…¶ä»–'å¹³å°ä½¿ç”¨åˆå¹¶åçš„æ•°æ®
            official_tops = _get_top_models(repo, current_official_pivot_merged, growth_official_pivot_merged, temp_current_official_data_merged)
            # 'å…¶ä»–'å¹³å°ç›®å‰æ²¡æœ‰åŒºåˆ†è¡ç”Ÿæ¨¡å‹
            platform_top_models.append({'platform': repo, 'official_tops': official_tops, 'derivative_tops': None})

    # å„å¹³å°æ€»ä¸‹è½½é‡å’Œå¢é•¿ (åŸºäºå…¨é‡æ•°æ®)
    # ğŸ”§ ä¿®å¤ï¼šä¸ºäº†åœ¨å¹³å°æ±‡æ€»ä¸­æ˜¾ç¤ºâ€œå…¶ä»–â€çš„æ±‡æ€»æ•°æ®ï¼Œéœ€è¦å°† Gitee, Modelers, é²¸æ™ºåˆå¹¶ä¸ºâ€œå…¶ä»–â€
    all_current_data_with_other = data[data['date'] == current_date].copy()
    all_current_data_with_other['repo'] = all_current_data_with_other['repo'].replace(['é­”ä¹ Modelers', 'é²¸æ™º', 'Gitee'], 'å…¶ä»–')
    current_platform_totals = all_current_data_with_other.groupby('repo')['download_count'].sum()

    all_previous_data_with_other = data[data['date'] == previous_date].copy()
    all_previous_data_with_other['repo'] = all_previous_data_with_other['repo'].replace(['é­”ä¹ Modelers', 'é²¸æ™º', 'Gitee'], 'å…¶ä»–')
    previous_platform_totals = all_previous_data_with_other.groupby('repo')['download_count'].sum()

    # åˆå¹¶å¹¶ç¡®ä¿æ•°å€¼ç±»å‹ï¼Œé¿å…TypeError
    platform_summary = pd.DataFrame({
        'current_total': current_platform_totals,
        'previous_total': previous_platform_totals
    }).reindex(REPO_ORDER).fillna(0).astype(int) # ğŸ”§ ä¿®å¤ï¼šreindex ä½¿ç”¨ REPO_ORDER

    platform_summary['growth_total'] = platform_summary['current_total'] - platform_summary['previous_total']
    platform_summary = platform_summary[['current_total', 'growth_total']]

    # å¢åŠ æ€»ä½“ç»Ÿè®¡
    # å®˜æ–¹æ¨¡å‹
    official_current_total = official_data[official_data['date'] == current_date]['download_count'].sum()
    official_previous_total = official_data[official_data['date'] == previous_date]['download_count'].sum()
    official_growth = official_current_total - official_previous_total

    # æ‰€æœ‰æ¨¡å‹ï¼ˆåŒ…æ‹¬è¡ç”Ÿï¼‰
    all_current_data = data[data['date'] == current_date]
    all_previous_data = data[data['date'] == previous_date]
    all_current_total = all_current_data['download_count'].sum()
    all_previous_total = all_previous_data['download_count'].sum()
    all_growth = all_current_total - all_previous_total

    # è¡ç”Ÿæ¨¡å‹
    derivative_current_total = all_current_total - official_current_total
    derivative_growth = all_growth - official_growth
    # è¡ç”Ÿæ¨¡å‹ï¼ˆæŒ‰ HFã€éå®˜æ–¹ã€publisher+model_name å»é‡çš„æ–°å‡ºç°æ•°é‡ï¼‰
    hf_curr_non_official = all_current_data[
        (all_current_data['date'] == current_date) &
        (all_current_data['repo'] == 'Hugging Face') &
        (all_current_data['is_official'] == False)
    ]
    hf_prev_non_official = all_previous_data[
        (all_previous_data['date'] == previous_date) &
        (all_previous_data['repo'] == 'Hugging Face') &
        (all_previous_data['is_official'] == False)
    ]
    curr_deriv_keys = set(zip(hf_curr_non_official['publisher'], hf_curr_non_official['model_name']))
    prev_deriv_keys = set(zip(hf_prev_non_official['publisher'], hf_prev_non_official['model_name']))
    new_derivative_keys = curr_deriv_keys - prev_deriv_keys
    derivative_new_models = len(new_derivative_keys)
    # åˆ—è¡¨æ˜ç»†ï¼ˆHFéå®˜æ–¹æ–°å¢å·®é›†ï¼‰
    derivative_new_models_list = []
    if derivative_new_models > 0:
        # æŒ‰ä¸‹è½½é‡é™åºï¼Œä¿æŒå”¯ä¸€
        hf_curr_non_official = hf_curr_non_official.sort_values('download_count', ascending=False)
        seen = set()
        for _, row in hf_curr_non_official.iterrows():
            key = (row['publisher'], row['model_name'])
            if key in new_derivative_keys and key not in seen:
                seen.add(key)
                derivative_new_models_list.append({
                    'model_name': row['model_name'],
                    'publisher': row['publisher'],
                    'download_count': int(row.get('download_count', 0) or 0),
                    'model_type': row.get('model_type'),
                    'model_category': row.get('model_category'),
                    'base_model': row.get('base_model'),
                    'repo': row.get('repo')
                })

    # ç¤¾åŒºç»´åº¦ & æ¨¡å‹ç»´åº¦
    platform_top_models_df = pd.DataFrame(platform_top_models)
    # 1. ç¤¾åŒºç»´åº¦ï¼šHFå¢é•¿æœ€é«˜ (åŸºäºå®˜æ–¹æ¨¡å‹)
    hf_row = platform_top_models_df.loc[platform_top_models_df['platform'] == 'Hugging Face']
    if not hf_row.empty:
        hf_official_tops = hf_row['official_tops'].iloc[0]
        hf_top_growth_model_name = hf_official_tops['top_growth_model']
        hf_top_model_growth = hf_official_tops['top_growth_count']
    else:
        hf_top_growth_model_name = "N/A"
        hf_top_model_growth = 0

    # 2. æ¨¡å‹ç»´åº¦ï¼šä¸‹è½½æ€»é‡å‰ä¸‰
    top3_downloads_details = current_totals.head(3)

    # 3. æ¨¡å‹ç»´åº¦ï¼šå¢é•¿æœ€å¿«å‰ä¸‰
    top3_growth_details = growth_totals.head(3)

    community_summary = {
        'hf_top_model_name': hf_top_growth_model_name,
        'hf_top_model_growth': hf_top_model_growth,
        'top3_downloads_details': top3_downloads_details.to_dict(),
        'top3_growth_details': top3_growth_details.to_dict(),
    }

    # è·å–æœ¬å‘¨æ–°å¢çš„Finetuneå’ŒAdapteræ¨¡å‹
    try:
        from .fetchers.fetchers_modeltree import get_weekly_new_finetune_adapters
        # ğŸ”§ ä¿®å¤ï¼šä¼ é€’ model_series å‚æ•°ä»¥ç²¾ç¡®ç­›é€‰æ¨¡å‹ç³»åˆ—
        new_models_info = get_weekly_new_finetune_adapters(current_date, previous_date, model_series=model_series)
    except Exception as e:
        print(f"è·å–æ–°å¢Finetune/Adapteræ¨¡å‹ä¿¡æ¯å¤±è´¥: {e}")
        new_models_info = {
            'new_finetune_models': [],
            'new_adapter_models': [],
            'new_lora_models': [],
            'total_new': 0,
            'summary': 'è·å–æ–°å¢æ¨¡å‹ä¿¡æ¯æ—¶å‡ºé”™'
        }

    # ğŸ†• è·å–æœ¬å‘¨æ–°å¢çš„æ‰€æœ‰æ¨¡å‹ï¼ˆå®Œæ•´åˆ—è¡¨ï¼‰
    # ğŸ”§ ä¿®å¤ï¼šç›´æ¥ä¼ é€’æ—¥æœŸï¼Œè®©å‡½æ•°è‡ªå·±åŠ è½½æ•°æ®ï¼ˆä¸ get_weekly_new_finetune_adapters ä¿æŒä¸€è‡´ï¼‰
    all_new_models_info = get_all_new_models(
        current_date=current_date,
        previous_date=previous_date,
        model_series=model_series
    )

    # ç»Ÿè®¡æ¨¡å‹æ•°é‡ï¼ˆæŒ‰ç±»åˆ«ã€æŒ‰æ˜¯å¦åŸå§‹ï¼‰
    derivative_current_total_models = len(
        all_current_data[
            (all_current_data['model_category'] == ('ernie-4.5' if model_series == 'ERNIE-4.5' else 'paddleocr-vl')) &
            (all_current_data['model_type'] != 'original')
        ]
    )

    summary_stats = {
        'all_current_total': all_current_total,
        'all_growth': all_growth,
        'official_current_total': official_current_total,
        'official_growth': official_growth,
        'derivative_current_total': derivative_current_total,
        'derivative_growth': derivative_growth,
        'derivative_current_total_models': derivative_current_total_models,
        'derivative_new_models': derivative_new_models,
        'derivative_new_models_list': derivative_new_models_list,
    }

    # ğŸ”´ è´Ÿå¢é•¿æ£€æµ‹ï¼šæ£€æŸ¥æ‰€æœ‰å¹³å°å’Œæ¨¡å‹çš„å¢é•¿æƒ…å†µ
    negative_growth_warnings = []

    # ğŸ”§ ä¿®å¤ï¼šåŸºäºåŸå§‹æ•°æ®æ£€æµ‹è´Ÿå¢é•¿ï¼Œè€Œä¸æ˜¯åŸºäº pivot è¡¨
    # è¿™æ ·èƒ½æ•è·æ‰€æœ‰æ¨¡å‹ï¼ˆåŒ…æ‹¬ä¸åœ¨ model_order ä¸­çš„è¡ç”Ÿæ¨¡å‹ï¼‰
    # ä¿®å¤ï¼šä½¿ç”¨ REPO_ORDER_DETAILED æ¥å•ç‹¬æ£€æµ‹ 'é­”ä¹ Modelers', 'é²¸æ™º', 'Gitee' çš„è´Ÿå¢é•¿
    for repo in REPO_ORDER_DETAILED: # ä¿®æ”¹ä¸º REPO_ORDER_DETAILED
        # è·å–è¯¥å¹³å°ä¸Šå‘¨å’Œæœ¬å‘¨çš„åŸå§‹å½“æ—¥æ•°æ®ï¼ˆä¸ä½¿ç”¨ last_value_per_modelï¼‰
        prev_platform_data = warn_previous_raw[warn_previous_raw['repo'] == repo].copy()
        curr_platform_data = warn_current_raw[warn_current_raw['repo'] == repo].copy()

        # æŒ‰æ¨¡å‹+å‘å¸ƒè€…èšåˆä¸‹è½½é‡ï¼Œé¿å…ä¸åŒå‘å¸ƒè€…çš„åŒåæ¨¡å‹è¢«åˆå¹¶
        prev_by_model = prev_platform_data.groupby(['model_name', 'publisher'])['download_count'].sum()
        curr_by_model = curr_platform_data.groupby(['model_name', 'publisher'])['download_count'].sum()

        # æ£€æŸ¥æ¯ä¸ªåœ¨ä¸Šå‘¨å­˜åœ¨çš„æ¨¡å‹
        for model_name in prev_by_model.index:
            previous_val = prev_by_model[model_name]
            current_val = curr_by_model.get(model_name, 0)  # å¦‚æœä¸å­˜åœ¨åˆ™ä¸º0
            growth_val = current_val - previous_val

            # åªæŠ¥å‘Šè´Ÿå¢é•¿
            if growth_val < 0:
                # åˆ¤æ–­æ˜¯å®˜æ–¹æ¨¡å‹è¿˜æ˜¯è¡ç”Ÿæ¨¡å‹
                model_name_only, publisher = model_name
                prev_rows = prev_platform_data[
                    (prev_platform_data['model_name'] == model_name_only) &
                    (prev_platform_data['publisher'] == publisher)
                ]
                curr_rows = curr_platform_data[
                    (curr_platform_data['model_name'] == model_name_only) &
                    (curr_platform_data['publisher'] == publisher)
                ]
                is_official_prev = (not prev_rows.empty) and prev_rows['is_official'].any()
                is_official_curr = (not curr_rows.empty) and curr_rows['is_official'].any()
                is_official = is_official_prev or is_official_curr
                model_type = 'å®˜æ–¹æ¨¡å‹' if is_official else 'è¡ç”Ÿæ¨¡å‹'

                negative_growth_warnings.append({
                    'platform': repo,
                    'model_name': model_name_only,
                    'publisher': publisher,
                    'model_type': model_type,
                    'previous': int(previous_val),
                    'current': int(current_val),
                    'growth': int(growth_val)
                })

    # å¦‚æœå‘ç°è´Ÿå¢é•¿ï¼Œæ‰“å°è­¦å‘Š
    if negative_growth_warnings:
        print("\n" + "=" * 80)
        print("âš ï¸  è­¦å‘Šï¼šæ£€æµ‹åˆ°è´Ÿå¢é•¿ï¼")
        print("=" * 80)
        for warning in negative_growth_warnings:
            print(f"å¹³å°: {warning['platform']}")
            print(f"æ¨¡å‹: {warning['model_name']} | å‘å¸ƒè€…: {warning['publisher']} | ç±»å‹: {warning['model_type']}")
            print(f"æ•°æ®: {warning['previous']:,} â†’ {warning['current']:,} (å˜åŒ–: {warning['growth']:,})")
            print(f"è¯´æ˜: è¯¥æ¨¡å‹åœ¨ {previous_date} æœ‰æ•°æ®ï¼Œä½†åœ¨ {current_date} æ•°æ®å‡å°‘æˆ–ç¼ºå¤±")
            print("-" * 80)

    return {
        'current_date': current_date,
        'summary_stats': summary_stats,
        'community_summary': community_summary,
        'previous_date': previous_date,
        'current_pivot': current_official_pivot,  # ä¿®æ”¹ï¼šä½¿ç”¨å®˜æ–¹æ¨¡å‹é€è§†è¡¨
        'previous_pivot': previous_official_pivot,  # ä¿®æ”¹ï¼šä½¿ç”¨å®˜æ–¹æ¨¡å‹é€è§†è¡¨
        'growth_pivot': growth_official_pivot,  # ä¿®æ”¹ï¼šä½¿ç”¨å®˜æ–¹æ¨¡å‹é€è§†è¡¨
        'top5_growth': top5_growth,
        'top3_downloads': top3_downloads,
        'platform_top_models': platform_top_models_df,
        'platform_summary': platform_summary,
        'new_models_info': new_models_info,  # æ–°å¢Finetune/Adapter/LoRAæ¨¡å‹ä¿¡æ¯
        'all_new_models_info': all_new_models_info,  # ğŸ†• æ‰€æœ‰æ–°å¢æ¨¡å‹å®Œæ•´åˆ—è¡¨
        'negative_growth_warnings': negative_growth_warnings  # è´Ÿå¢é•¿è­¦å‘Š
    }


def format_report_tables(report_data):
    """
    æ ¼å¼åŒ–æŠ¥è¡¨æ•°æ®ä¸ºå¯æ˜¾ç¤ºçš„è¡¨æ ¼

    Args:
        report_data: calculate_weekly_report è¿”å›çš„æ•°æ®

    Returns:
        dict: åŒ…å«æ ¼å¼åŒ–è¡¨æ ¼çš„å­—å…¸
    """
    if report_data is None:
        return None

    tables = {}

    # 1. & 2. åˆå¹¶ä¸‹è½½é‡å’Œå¢é•¿é‡
    current_pivot = report_data['current_pivot'].astype(int)
    growth_pivot = report_data['growth_pivot'].astype(int)

    interleaved_df = pd.DataFrame(index=current_pivot.index)

    # ä½¿ç”¨ REPO_ORDER_DETAILED ä¿è¯é¡ºåºï¼ˆæ˜¾ç¤ºè¯¦ç»†å¹³å°ï¼Œä¸åˆå¹¶"å…¶ä»–"ï¼‰
    for repo in REPO_ORDER_DETAILED:
        if repo in current_pivot.columns:
            interleaved_df[f'{repo} (æ€»)'] = current_pivot[repo]
            interleaved_df[f'{repo} (å‘¨å¢)'] = growth_pivot[repo]

    tables['combined_downloads_growth'] = interleaved_df

    # 3. Top 5 å¢é•¿æœ€é«˜çš„æ¨¡å‹
    top5_df = pd.DataFrame({
        'æ¨¡å‹åç§°': report_data['top5_growth'].index,
        'æœ¬å‘¨å¢é•¿': report_data['top5_growth'].values.astype(int)
    }).reset_index(drop=True)
    top5_df.index = top5_df.index + 1
    tables['top5_growth'] = top5_df

    # 4. Top 3 æ€»ä¸‹è½½é‡æœ€é«˜çš„æ¨¡å‹
    top3_df = pd.DataFrame({
        'æ¨¡å‹åç§°': report_data['top3_downloads'].index,
        'æ€»ä¸‹è½½é‡': report_data['top3_downloads'].values.astype(int)
    }).reset_index(drop=True)
    top3_df.index = top3_df.index + 1
    tables['top3_downloads'] = top3_df

    # 5. å„å¹³å°æ¦œé¦–æ¨¡å‹
    platform_top_df = report_data['platform_top_models'].copy()
    
    def format_model_info(tops, label, is_growth_col=False):
        if not tops:
            return ""
            
        model_name = tops['top_growth_model'] if is_growth_col else tops['top_download_model']
        if model_name == 'N/A':
            return ""

        publisher = tops['top_growth_publisher'] if is_growth_col else tops['top_download_publisher']
        total_downloads = tops['top_growth_current'] if is_growth_col else tops['top_download_count']
        weekly_growth = tops['top_growth_count'] if is_growth_col else tops['top_download_growth']

        return (
            f"{label}: {model_name} ({publisher})\n"
            f"  ({total_downloads:,}) (æœ¬å‘¨: +{weekly_growth:,})"
        )

    def format_combined_download(row):
        official_str = format_model_info(row['official_tops'], 'å®˜æ–¹', is_growth_col=False)
        derivative_str = format_model_info(row['derivative_tops'], 'è¡ç”Ÿ', is_growth_col=False)
        return "\n".join(filter(None, [official_str, derivative_str]))

    def format_combined_growth(row):
        official_str = format_model_info(row['official_tops'], 'å®˜æ–¹', is_growth_col=True)
        derivative_str = format_model_info(row['derivative_tops'], 'è¡ç”Ÿ', is_growth_col=True)
        return "\n".join(filter(None, [official_str, derivative_str]))

    platform_top_df['ä¸‹è½½é‡æœ€é«˜æ¨¡å‹'] = platform_top_df.apply(format_combined_download, axis=1)
    platform_top_df['å¢é•¿æœ€é«˜æ¨¡å‹'] = platform_top_df.apply(format_combined_growth, axis=1)
    
    # ç­›é€‰å¹¶é‡å‘½ååˆ—
    tables['platform_top_models'] = platform_top_df[['platform', 'ä¸‹è½½é‡æœ€é«˜æ¨¡å‹', 'å¢é•¿æœ€é«˜æ¨¡å‹']]

    # 6. å„å¹³å°æ±‡æ€»
    summary_df = report_data['platform_summary'].copy()
    summary_df.columns = ['å½“å‰æ€»ä¸‹è½½é‡', 'æœ¬å‘¨å¢é•¿']
    summary_df = summary_df.astype(int)
    tables['platform_summary'] = summary_df

    # 7. æ–°å¢Finetuneå’ŒAdapteræ¨¡å‹è¡¨æ ¼
    new_models_info = report_data.get('new_models_info', {})

    # æ ¼å¼åŒ–æ–°å¢Finetuneæ¨¡å‹
    finetune_data = new_models_info.get('new_finetune_models', [])
    if finetune_data:
        finetune_df = pd.DataFrame(finetune_data)
        finetune_df.index = finetune_df.index + 1
        finetune_df.columns = ['æ¨¡å‹åç§°', 'å‘å¸ƒè€…', 'ä¸‹è½½é‡']
        tables['new_finetune_models'] = finetune_df
    else:
        tables['new_finetune_models'] = pd.DataFrame(columns=['æ¨¡å‹åç§°', 'å‘å¸ƒè€…', 'ä¸‹è½½é‡'])

    # æ ¼å¼åŒ–æ–°å¢Adapteræ¨¡å‹
    adapter_data = new_models_info.get('new_adapter_models', [])
    if adapter_data:
        adapter_df = pd.DataFrame(adapter_data)
        adapter_df.index = adapter_df.index + 1
        adapter_df.columns = ['æ¨¡å‹åç§°', 'å‘å¸ƒè€…', 'ä¸‹è½½é‡']
        tables['new_adapter_models'] = adapter_df
    else:
        tables['new_adapter_models'] = pd.DataFrame(columns=['æ¨¡å‹åç§°', 'å‘å¸ƒè€…', 'ä¸‹è½½é‡'])

    # æ ¼å¼åŒ–æ–°å¢LoRAæ¨¡å‹
    lora_data = new_models_info.get('new_lora_models', [])
    if lora_data:
        lora_df = pd.DataFrame(lora_data)
        lora_df.index = lora_df.index + 1
        lora_df.columns = ['æ¨¡å‹åç§°', 'å‘å¸ƒè€…', 'ä¸‹è½½é‡']
        tables['new_lora_models'] = lora_df
    else:
        tables['new_lora_models'] = pd.DataFrame(columns=['æ¨¡å‹åç§°', 'å‘å¸ƒè€…', 'ä¸‹è½½é‡'])

    # æ–°å¢æ¨¡å‹æ±‡æ€»ä¿¡æ¯
    tables['new_models_summary'] = new_models_info.get('summary', 'æ— æ–°å¢æ¨¡å‹ä¿¡æ¯')

    # 8. Model Tree æ–°å¢è¡ç”Ÿæ¨¡å‹è¡¨æ ¼
    # ç§»é™¤ Model Tree æ–°å¢è¡ç”Ÿæ¨¡å‹æ¨¡å—
    tables['new_model_tree_models'] = pd.DataFrame(columns=['æ¨¡å‹åç§°', 'å‘å¸ƒè€…', 'ä¸‹è½½é‡', 'åŸºç¡€æ¨¡å‹', 'æ¨¡å‹ç±»å‹'])
    tables['model_tree_summary'] = 'ï¼ˆå·²ç§»é™¤ Model Tree æ–°å¢æ¨¡å—ï¼‰'

    # 9. è´Ÿå¢é•¿è­¦å‘Šè¡¨æ ¼
    negative_growth_warnings = report_data.get('negative_growth_warnings', [])
    if negative_growth_warnings:
        warnings_df = pd.DataFrame(negative_growth_warnings)
        warnings_df.index = warnings_df.index + 1
        warnings_df.columns = ['å¹³å°', 'æ¨¡å‹åç§°', 'å‘å¸ƒè€…', 'æ¨¡å‹ç±»å‹', 'ä¸Šå‘¨ä¸‹è½½é‡', 'æœ¬å‘¨ä¸‹è½½é‡', 'å‘¨å¢é•¿']
        tables['negative_growth_warnings'] = warnings_df
    else:
        tables['negative_growth_warnings'] = pd.DataFrame(columns=['å¹³å°', 'æ¨¡å‹åç§°', 'å‘å¸ƒè€…', 'æ¨¡å‹ç±»å‹', 'ä¸Šå‘¨ä¸‹è½½é‡', 'æœ¬å‘¨ä¸‹è½½é‡', 'å‘¨å¢é•¿'])

    # 10. ğŸ†• æ‰€æœ‰æ–°å¢æ¨¡å‹å®Œæ•´åˆ—è¡¨
    all_new_models_info = report_data.get('all_new_models_info', {})

    new_models_list = all_new_models_info.get('new_models_list', [])
    if new_models_list:
        all_new_df = pd.DataFrame(new_models_list)
        all_new_df.index = all_new_df.index + 1

        # æ ¹æ®åŒ…å«çš„åˆ—æ¥è®¾ç½®åˆ—å
        column_mapping = {
            'model_name': 'æ¨¡å‹åç§°',
            'publisher': 'å‘å¸ƒè€…',
            'repo': 'å¹³å°',
            'download_count': 'ä¸‹è½½é‡',
            'model_type': 'æ¨¡å‹ç±»å‹',
            'model_category': 'æ¨¡å‹åˆ†ç±»',
            'base_model': 'åŸºç¡€æ¨¡å‹'
        }

        # åªé‡å‘½åå­˜åœ¨çš„åˆ—
        rename_dict = {k: v for k, v in column_mapping.items() if k in all_new_df.columns}
        all_new_df = all_new_df.rename(columns=rename_dict)

        tables['all_new_models'] = all_new_df
    else:
        tables['all_new_models'] = pd.DataFrame(columns=['æ¨¡å‹åç§°', 'å‘å¸ƒè€…', 'å¹³å°', 'ä¸‹è½½é‡', 'æ¨¡å‹ç±»å‹'])

    # æ–°å¢æ¨¡å‹æ±‡æ€»ä¿¡æ¯
    tables['all_new_models_summary'] = all_new_models_info.get('summary', 'æ— æ–°å¢æ¨¡å‹')

    # 11. HF éå®˜æ–¹æ–°å¢è¡ç”Ÿæ¨¡å‹åˆ—è¡¨
    derivative_new_list = report_data['summary_stats'].get('derivative_new_models_list', [])
    if derivative_new_list:
        deriv_new_df = pd.DataFrame(derivative_new_list)
        deriv_new_df.index = deriv_new_df.index + 1
        rename_map = {
            'model_name': 'æ¨¡å‹åç§°',
            'publisher': 'å‘å¸ƒè€…',
            'download_count': 'ä¸‹è½½é‡',
            'model_type': 'æ¨¡å‹ç±»å‹',
            'model_category': 'æ¨¡å‹ç³»åˆ—',
            'base_model': 'åŸºç¡€æ¨¡å‹',
            'repo': 'å¹³å°'
        }
        deriv_new_df = deriv_new_df.rename(columns={k: v for k, v in rename_map.items() if k in deriv_new_df.columns})
        tables['derivative_new_models'] = deriv_new_df
    else:
        tables['derivative_new_models'] = pd.DataFrame(columns=['æ¨¡å‹åç§°', 'å‘å¸ƒè€…', 'ä¸‹è½½é‡', 'æ¨¡å‹ç±»å‹', 'æ¨¡å‹ç³»åˆ—', 'åŸºç¡€æ¨¡å‹', 'å¹³å°'])

    return tables


def calculate_paddleocr_vl_weekly_report(current_date=None, previous_date=None):
    """
    è®¡ç®— PaddleOCR-VL çš„å‘¨æŠ¥æ•°æ®
    """
    return calculate_weekly_report(
        current_date,
        previous_date,
        model_order=PADDLEOCR_VL_MODEL_ORDER,
        model_series='PaddleOCR-VL'
    )
