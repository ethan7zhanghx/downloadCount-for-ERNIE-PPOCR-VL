"""
Model Tree åŠŸèƒ½æ¨¡å— - è·å–å®˜æ–¹æ¨¡å‹çš„è¡ç”Ÿæ¨¡å‹
æ”¯æŒè·å– Finetune å’Œ Adapter æ¨¡å‹ï¼Œå¹¶æ™ºèƒ½åˆ†ç±»
"""
from huggingface_hub import list_models, model_info
from datetime import date
import pandas as pd
import time
import re
from typing import List, Dict, Set, Tuple
from ..db import save_to_db, get_last_model_count, update_last_model_count
from ..config import DB_PATH


def classify_model(model_name: str, publisher: str, base_model: str = None) -> str:
    """
    æ™ºèƒ½åˆ†ç±»æ¨¡å‹

    Args:
        model_name: æ¨¡å‹åç§°
        publisher: å‘å¸ƒè€…
        base_model: åŸºç¡€æ¨¡å‹IDï¼ˆå¯é€‰ï¼Œç”¨äºè¡ç”Ÿæ¨¡å‹çš„åˆ†ç±»ï¼‰

    Returns:
        str: æ¨¡å‹ç±»åˆ« ('ernie-4.5', 'paddleocr-vl', 'other-ernie', 'other')
    """
    """
    ä»…è¿”å›ä¸¤ç±»ï¼šernie-4.5 æˆ– paddleocr-vlã€‚å…¶ä½™ä¸€å¾‹å½’å…¥ ernie-4.5ï¼ˆé¿å…å‡ºç° other/other-ernieï¼‰ã€‚
    """
    def _is_paddleocr(name: str) -> bool:
        n = name.lower()
        return 'paddleocr' in n and 'vl' in n

    base_lower = base_model.lower() if base_model else ''
    name_lower = model_name.lower()

    if _is_paddleocr(base_lower) or _is_paddleocr(name_lower):
        return 'paddleocr-vl'

    # é»˜è®¤å½’å…¥ ernie-4.5
    return 'ernie-4.5'


def classify_model_type(model_name: str, tags: list, pipeline_tag: str = None, card_data: dict = None) -> str:
    """
    è¯†åˆ«æ¨¡å‹ç±»å‹ï¼Œä¼˜å…ˆä½¿ç”¨ç»“æ„åŒ–ä¿¡æ¯ï¼ˆHF æ ‡ç­¾ / æ¨¡å‹å¡ï¼‰ï¼Œæœ€åå†åç§°å…œåº•

    Args:
        model_name: æ¨¡å‹åç§°
        tags: æ¨¡å‹æ ‡ç­¾åˆ—è¡¨ï¼ˆæ¥è‡ªHuggingFace APIï¼‰
        pipeline_tag: pipelineæ ‡ç­¾
        card_data: æ¨¡å‹å¡çš„å…ƒä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰

    Returns:
        str: æ¨¡å‹ç±»å‹
        - 'quantized': é‡åŒ–æ¨¡å‹
        - 'finetune': å¾®è°ƒæ¨¡å‹
        - 'adapter': Adapteræ¨¡å‹
        - 'lora': LoRAæ¨¡å‹
        - 'merge': åˆå¹¶æ¨¡å‹
        - 'original': å®˜æ–¹åŸå§‹æ¨¡å‹
        - 'other': å…¶ä»–
    """
    card_data_dict = card_data if isinstance(card_data, dict) else None

    # 1) æ ‡ç­¾ï¼šç»“æ„åŒ–ã€ä¼˜å…ˆçº§æœ€é«˜
    tags_lower = [tag.lower() for tag in tags] if tags else []
    for tag in tags_lower:
        if tag.startswith('base_model:quantized:'):
            return 'quantized'
        if tag.startswith('base_model:adapter:'):
            return 'adapter'
        if tag.startswith('base_model:lora:'):
            return 'lora'
        if tag.startswith('base_model:merge:'):
            return 'merge'
        if tag.startswith('base_model:finetune:'):
            return 'finetune'

    # 2) æ ‡ç­¾ï¼šPEFT ä¿¡å·
    peft_indicators = ['peft', 'prefix-tuning', 'prompt-tuning', 'adapter']
    if tags_lower and any(indicator in ' '.join(tags_lower) for indicator in peft_indicators):
        if any('lora' in tag for tag in tags_lower):
            return 'lora'
        return 'adapter'

    # 3) æ¨¡å‹å¡ï¼šä»…ä½¿ç”¨å¸¸è§ã€å›ºå®šå­—æ®µ
    if card_data_dict:
        card_type = _classify_by_card_data(card_data_dict)
        if card_type != 'other':
            return card_type

    # 4) å®˜æ–¹åŸå§‹æ¨¡å‹ï¼ˆæ—  base_model æ ‡ç­¾ï¼‰
    official_patterns = ['baidu/', 'paddlepaddle/']
    model_name_lower = model_name.lower()
    if any(pattern in model_name_lower for pattern in official_patterns):
        if not any(tag.startswith('base_model:') for tag in tags_lower):
            return 'original'

    # 5) åç§°å…œåº•ï¼ˆæœ€ä¸å¯é ï¼‰
    name_based_type = _classify_by_name_fallback(model_name)
    if name_based_type != 'other':
        return name_based_type

    return 'other'


def _classify_by_card_data(card_data: dict) -> str:
    """
    åŸºäºæ¨¡å‹å¡çš„å­—æ®µè¿›è¡Œåˆ†ç±»ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
    """
    # é‡åŒ–ç›¸å…³å­—æ®µ
    quant_keys = [
        'quantization_config', 'quantization', 'quantization_bits',
        'load_in_4bit', 'load_in_8bit', 'bnb_4bit_quant_type', 'gguf'
    ]
    if any(key in card_data for key in quant_keys):
        return 'quantized'

    # PEFT / LoRA
    peft_type = str(card_data.get('peft_type', '') or '').lower()
    if 'lora' in peft_type:
        return 'lora'
    if peft_type:
        return 'adapter'

    if any(key in card_data for key in ['lora_alpha', 'lora_r', 'lora_dropout']):
        return 'lora'
    if any(key in card_data for key in ['adapters', 'adapter', 'adapter_config', 'adapter_name']):
        return 'adapter'

    # åˆå¹¶æ¨¡å‹
    if any(key in card_data for key in ['merge_method', 'merging_config', 'merge_config', 'merged_by']):
        return 'merge'

    # æ˜ç¡®çš„å¾®è°ƒé…ç½®å­—æ®µ
    finetune_keys = ['finetuning_type', 'finetuning_config', 'finetune_config']
    if any(key in card_data for key in finetune_keys):
        return 'finetune'

    return 'other'


def _classify_by_name_fallback(model_name: str) -> str:
    """
    å›é€€æ–¹æ¡ˆï¼šåŸºäºæ¨¡å‹åç§°è¿›è¡Œåˆ†ç±»ï¼ˆå½“æ²¡æœ‰æ ‡ç­¾ä¿¡æ¯æ—¶ï¼‰

    Args:
        model_name: æ¨¡å‹åç§°

    Returns:
        str: æ¨¡å‹ç±»å‹
    """
    model_name_lower = model_name.lower()

    # Quantized ç›¸å…³å…³é”®è¯ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
    quantized_keywords = [
        # æ ¼å¼æ ‡è¯†
        '-gguf', '.gguf', 'gguf', '-gptq', '-awq', '-exl2',
        # é‡åŒ–ä½æ•° - é€šç”¨æ ¼å¼
        '-4bit', '-8bit', '-6bit', '-2bit',
        'int2', 'int4', 'int8',
        # Qç³»åˆ—é‡åŒ–
        '-q1_', '-q2_', '-q3_', '-q4_', '-q5_', '-q6_', '-q8_',
        'q1_', 'q2_', 'q3_', 'q4_', 'q5_', 'q6_', 'q8_',
        # ç²¾åº¦æ ¼å¼ï¼ˆä»…ä¿ç•™ fp8ï¼Œç§»é™¤ bf16/fp16 ä»¥å…è¯¯åˆ¤ï¼‰
        'fp8',
        # W/Aé‡åŒ–æ ¼å¼
        'w4a8', 'w4a16', 'w2a8', 'w8a8', 'w4a4',
        # MLXæ ¼å¼
        'mlx-4bit', 'mlx-8bit', 'mlx-6bit',
        # å…¶ä»–æ ‡è¯†
        '-quantized', '_quantized', 'quantized'
    ]
    if any(keyword in model_name_lower for keyword in quantized_keywords):
        return 'quantized'

    # LoRA ç›¸å…³å…³é”®è¯
    lora_keywords = ['lora', 'low-rank-adaptation', 'low-rank']
    if any(keyword in model_name_lower for keyword in lora_keywords):
        return 'lora'

    # Adapter ç›¸å…³å…³é”®è¯
    adapter_keywords = ['adapter', 'adapters', 'peft', 'prefix-tuning', 'prompt-tuning']
    if any(keyword in model_name_lower for keyword in adapter_keywords):
        return 'adapter'

    # Merge ç›¸å…³å…³é”®è¯
    merge_keywords = ['-merge', '_merge', '-merged', '_merged']
    if any(keyword in model_name_lower for keyword in merge_keywords):
        return 'merge'

    # Finetune ç›¸å…³å…³é”®è¯
    finetune_keywords = [
        'finetune', 'fine-tune', 'fine-tuned', 'finetuned',
        'custom-trained', 'custom-trained-model', 'trained-on'
    ]
    if any(keyword in model_name_lower for keyword in finetune_keywords):
        return 'finetune'

    # æ£€æŸ¥æ˜¯å¦ä¸ºå®˜æ–¹åŸå§‹æ¨¡å‹
    official_patterns = ['baidu/', 'paddlepaddle/']
    if any(pattern in model_name_lower for pattern in official_patterns):
        return 'original'

    return 'other'


def get_model_tree_children(base_model_id: str, max_depth: int = 1) -> List[Dict]:
    """
    è·å–æŒ‡å®šæ¨¡å‹çš„ç›´æ¥è¡ç”Ÿæ¨¡å‹ï¼ˆé€šè¿‡ HuggingFace API çš„ base_model filterï¼‰

    Args:
        base_model_id: åŸºç¡€æ¨¡å‹IDï¼ˆå¦‚ 'baidu/ERNIE-4.5-21B-A3B-PT'ï¼‰
        max_depth: æœç´¢æ·±åº¦ï¼Œé»˜è®¤ä¸º1ï¼ˆåªè·å–ç›´æ¥è¡ç”Ÿï¼‰

    Returns:
        List[Dict]: è¡ç”Ÿæ¨¡å‹ä¿¡æ¯åˆ—è¡¨
    """
    try:
        # éªŒè¯åŸºç¡€æ¨¡å‹å­˜åœ¨
        try:
            base_info = model_info(base_model_id)
            print(f"ğŸ“Š è·å– {base_model_id} çš„model tree...")
        except Exception as e:
            print(f"âš ï¸ åŸºç¡€æ¨¡å‹ {base_model_id} ä¸å­˜åœ¨æˆ–æ— æ³•è®¿é—®: {e}")
            return []

        # ä½¿ç”¨ HuggingFace å®˜æ–¹çš„ base_model filter åŠŸèƒ½
        # è¿™æ˜¯æ­£ç¡®çš„ Model Tree æŸ¥æ‰¾æ–¹æ³•
        try:
            derivatives = list(list_models(
                filter=f"base_model:{base_model_id}",
                full=True,
                limit=1000  # å¢åŠ é™åˆ¶ä»¥è·å–æ‰€æœ‰è¡ç”Ÿæ¨¡å‹
            ))

            if not derivatives:
                print(f"  âšª æ²¡æœ‰æ‰¾åˆ°è¡ç”Ÿæ¨¡å‹")
                return []

            print(f"  âœ… æ‰¾åˆ° {len(derivatives)} ä¸ªè¡ç”Ÿæ¨¡å‹")

            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            related_models = []
            for deriv in derivatives:
                try:
                    # è·å–è¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…æ‹¬ä¸‹è½½é‡ï¼‰ - å¿…é¡»ä½¿ç”¨ expand å‚æ•°
                    deriv_info = model_info(deriv.id, expand=["downloadsAllTime"])

                    # è·å–ä¸‹è½½é‡ - ä¼˜å…ˆä½¿ç”¨ downloads_all_timeï¼Œå›é€€åˆ° downloads
                    downloads = getattr(deriv_info, 'downloads_all_time', None) or getattr(deriv_info, 'downloads', 0) or 0

                    model_data = {
                        'id': deriv.id,
                        'author': deriv.author or 'Unknown',
                        'tags': getattr(deriv, 'tags', []),  # ğŸ”§ ä¿®å¤ï¼šä» deriv è·å– tagsï¼ˆderiv_info.tags ä¸º Noneï¼‰
                        'downloads': downloads,
                        'pipeline_tag': getattr(deriv, 'pipeline_tag', None),  # ğŸ”§ ä¿®å¤ï¼šä» deriv è·å–
                        'created_at': getattr(deriv, 'created_at', None),
                        'last_modified': getattr(deriv, 'last_modified', None),
                        'likes': getattr(deriv, 'likes', 0)
                    }
                    related_models.append(model_data)

                except Exception as e:
                    print(f"    âš ï¸ è·å– {deriv.id} è¯¦æƒ…å¤±è´¥: {e}")
                    continue

            print(f"  âœ… æˆåŠŸå¤„ç† {len(related_models)} ä¸ªè¡ç”Ÿæ¨¡å‹")
            return related_models

        except Exception as e:
            print(f"  âŒ é€šè¿‡ base_model filter æŸ¥æ‰¾å¤±è´¥: {e}")
            return []

    except Exception as e:
        print(f"âŒ è·å– {base_model_id} çš„model treeå¤±è´¥: {e}")
        return []


def extract_related_models_from_card(card_data: dict, base_model_id: str) -> List[str]:
    """
    ä»æ¨¡å‹cardä¸­æå–ç›¸å…³æ¨¡å‹ID
    """
    related_models = []

    if not card_data:
        return related_models

    # é€’å½’æŸ¥æ‰¾æ‰€æœ‰æ–‡æœ¬å†…å®¹
    def extract_text(obj):
        texts = []
        if isinstance(obj, str):
            texts.append(obj)
        elif isinstance(obj, dict):
            for value in obj.values():
                texts.extend(extract_text(value))
        elif isinstance(obj, list):
            for item in obj:
                texts.extend(extract_text(item))
        return texts

    all_text = extract_text(card_data)
    combined_text = ' '.join(all_text).lower()

    # æŸ¥æ‰¾ç›¸å…³çš„æ¨¡å‹å¼•ç”¨
    base_name = base_model_id.split('/')[-1].lower()

    # æŸ¥æ‰¾æ¨¡å¼
    patterns = [
        rf'{base_model_id.lower()}',
        rf'{base_name}',
        'based on',
        'finetuned from',
        'adapter for',
        'lora for'
    ]

    # ä»æ–‡æœ¬ä¸­æå–æ¨¡å‹ID
    import re
    for text in all_text:
        # æŸ¥æ‰¾æ¨¡å‹IDæ¨¡å¼
        model_id_pattern = r'([a-zA-Z0-9_-]+/[a-zA-Z0-9_-]+)'
        matches = re.findall(model_id_pattern, text)

        for match in matches:
            if match != base_model_id and match not in related_models:
                # éªŒè¯æ˜¯å¦æ˜¯æœ‰æ•ˆçš„æ¨¡å‹
                try:
                    model_info(match)  # éªŒè¯æ¨¡å‹å­˜åœ¨
                    related_models.append(match)
                except:
                    continue

    return related_models


def is_genuine_derivative(model_info, base_model_id: str) -> bool:
    """
    éªŒè¯ä¸€ä¸ªæ¨¡å‹æ˜¯å¦çœŸçš„æ˜¯åŸºç¡€æ¨¡å‹çš„è¡ç”Ÿç‰ˆæœ¬

    Args:
        model_info: HuggingFaceæ¨¡å‹ä¿¡æ¯å¯¹è±¡
        base_model_id: åŸºç¡€æ¨¡å‹ID

    Returns:
        bool: æ˜¯å¦ä¸ºçœŸå®çš„è¡ç”Ÿæ¨¡å‹
    """
    try:
        # æ£€æŸ¥æ¨¡å‹cardå†…å®¹
        if hasattr(model_info, 'card_data') and model_info.card_data:
            card_content = str(model_info.card_data).lower()
            base_id_lower = base_model_id.lower()
            base_name = base_model_id.split('/')[-1].lower()

            # æ˜ç¡®çš„è¡ç”ŸæŒ‡æ ‡
            derivative_indicators = [
                f'based on {base_id_lower}',
                f'finetuned from {base_id_lower}',
                f'trained on {base_id_lower}',
                f'adapter for {base_id_lower}',
                f'lora adapter for {base_id_lower}',
                f'{base_name} finetune',
                f'{base_name} adapter',
                f'{base_name} lora'
            ]

            # æ£€æŸ¥æ˜¯å¦åŒ…å«è¡ç”ŸæŒ‡æ ‡
            if any(indicator in card_content for indicator in derivative_indicators):
                return True

        # æ£€æŸ¥æ¨¡å‹åç§°æ¨¡å¼
        model_name = model_info.modelId.lower()
        base_name = base_model_id.split('/')[-1].lower()

        # åç§°æ¨¡å¼æ£€æŸ¥
        derivative_patterns = [
            f'{base_name}-finetune',
            f'{base_name}-adapter',
            f'{base_name}-lora',
            f'{base_name}-fine-tuned',
            f'{base_name}-adapted',
            f'finetuned-{base_name}',
            f'adapter-{base_name}',
            f'lora-{base_name}'
        ]

        if any(pattern in model_name for pattern in derivative_patterns):
            return True

        return False

    except Exception:
        return False


def is_derivative_model(model, base_model_id: str) -> bool:
    """
    åˆ¤æ–­ä¸€ä¸ªæ¨¡å‹æ˜¯å¦æ˜¯åŸºç¡€æ¨¡å‹çš„è¡ç”Ÿæ¨¡å‹

    Args:
        model: HuggingFaceæ¨¡å‹å¯¹è±¡
        base_model_id: åŸºç¡€æ¨¡å‹ID

    Returns:
        bool: æ˜¯å¦ä¸ºè¡ç”Ÿæ¨¡å‹
    """
    model_id = model.id.lower()
    base_name = base_model_id.split('/')[-1].lower()

    # æ£€æŸ¥æ¨¡å‹åå…³ç³»
    derivative_patterns = [
        f"{base_name}-",
        f"-{base_name}",
        f"finetuned-{base_name}",
        f"{base_name}-finetune",
        f"adapter-{base_name}",
        f"{base_name}-adapter",
        f"lora-{base_name}",
        f"{base_name}-lora"
    ]

    # æ£€æŸ¥æ ‡ç­¾
    derivative_tags = ['fine-tuned', 'adapter', 'lora', 'peft']

    # åå­—åŒ¹é…æˆ–æ ‡ç­¾åŒ¹é…
    name_match = any(pattern in model_id for pattern in derivative_patterns)
    tag_match = (hasattr(model, 'tags') and
                any(tag in [t.lower() for t in model.tags] for tag in derivative_tags))

    return name_match or tag_match


def get_all_ernie_derivatives(include_paddleocr: bool = True) -> Tuple[pd.DataFrame, int]:
    """
    è·å–æ‰€æœ‰ ERNIE-4.5 å’Œ PaddleOCR-VL ç›¸å…³æ¨¡å‹ï¼Œç»“åˆï¼š
    - å…¨å±€æœç´¢ï¼ˆERNIE-4.5 / PaddleOCR-VLï¼‰
    - å®˜æ–¹è´¦å·æ¨¡å‹ï¼ˆbaidu / PaddlePaddleï¼‰
    - Model Tree è¡ç”Ÿ
    - ä»¥ base_model ä¸ºå…³é”®è¯çš„è¡¥å……æœç´¢

    ä¸è„šæœ¬ç‰ˆå¯¹é½ï¼šæ ‡è®° official ä¸º originalï¼ŒModel Tree å‘½ä¸­ä¸ search åˆå¹¶ä¸º bothï¼Œ
    ä¿ç•™ base_model_from_apiï¼Œå¹¶å¸¦å…¥æ›´å¤šå­—æ®µï¼ˆlikes/library/pipeline/æ—¶é—´æˆ³ï¼‰ã€‚
    """
    print("ğŸš€ å¼€å§‹è·å–ERNIE-4.5å’ŒPaddleOCR-VLæ¨¡å‹...")

    all_models: List[Dict] = []
    processed_ids: Set[str] = set()
    official_models: Dict[str, Dict] = {}

    search_terms = ['ERNIE-4.5', 'PaddleOCR-VL']

    # ---------- è¾…åŠ©å‡½æ•° ----------
    def normalize_tags(tags):
        return tags if isinstance(tags, list) else (tags if tags is not None else [])

    def parse_base_from_card(card_data):
        if not card_data:
            return None
        base_val = card_data.get('base_model') if isinstance(card_data, dict) else None
        if isinstance(base_val, list) and base_val:
            return base_val[0]
        if isinstance(base_val, str) and base_val:
            return base_val
        return None

    def _get_field(obj, name):
        """å…¼å®¹ dict å’Œ huggingface_hub è¿”å›å¯¹è±¡çš„å–å€¼"""
        if obj is None:
            return None
        if isinstance(obj, dict):
            return obj.get(name)
        return getattr(obj, name, None)

    def fetch_model_detail(model_id, model_obj=None):
        try:
            info = model_info(model_id, expand=["downloadsAllTime"])
        except Exception as e:
            print(f"  âš ï¸ è·å– {model_id} è¯¦æƒ…å¤±è´¥: {e}")
            return None

        card_data = None
        if hasattr(info, 'cardData') and info.cardData:
            if isinstance(info.cardData, dict):
                card_data = info.cardData
            elif hasattr(info.cardData, '__dict__'):
                card_data = info.cardData.__dict__

        tags = normalize_tags(_get_field(model_obj, 'tags') or getattr(info, 'tags', None))
        pipeline_tag = _get_field(model_obj, 'pipeline_tag') or getattr(info, 'pipeline_tag', None)
        # Hugging Face æŸäº›æ–°æ¨¡å‹çš„ author å­—æ®µå¯èƒ½ä¸ºç©ºï¼Œå›é€€åˆ° repo ownerï¼ˆID å‰ç¼€ï¼‰
        publisher = (
            getattr(info, 'author', None)
            or _get_field(model_obj, 'author')
            or (model_id.split('/')[0] if '/' in model_id else 'Unknown')
        )
        downloads = (
            getattr(info, 'downloads_all_time', None)
            or getattr(info, 'downloads', 0)
            or _get_field(model_obj, 'downloads')
            or 0
        )

        base_from_api = parse_base_from_card(card_data)

        # å¦‚æœ cardData ä¸­æ²¡æœ‰ base_modelï¼Œå°è¯•ä» tags ä¸­æå–
        if not base_from_api and tags:
            for tag in tags:
                if isinstance(tag, str) and tag.startswith('base_model:'):
                    # æå– base_modelï¼Œæ ¼å¼å¦‚: base_model:PaddlePaddle/PaddleOCR-VL
                    # æˆ– base_model:adapter:PaddlePaddle/PaddleOCR-VL
                    parts = tag.split(':', 2)  # æœ€å¤šåˆ†å‰²æˆ3éƒ¨åˆ†
                    if len(parts) >= 2:
                        # base_model:ModelID æˆ– base_model:type:ModelID
                        candidate = parts[-1]  # å–æœ€åä¸€éƒ¨åˆ†ä½œä¸º model ID
                        # éªŒè¯æ˜¯å¦æ˜¯æœ‰æ•ˆçš„ model ID æ ¼å¼ (åŒ…å« /)
                        if '/' in candidate and not candidate.startswith('license:'):
                            base_from_api = candidate
                            break

        model_category = classify_model(model_id, publisher, base_from_api)
        model_type = classify_model_type(model_id, tags, pipeline_tag, card_data)

        return {
            'model_id': model_id,
            'publisher': publisher,
            'downloads': downloads,
            'tags': tags,
            'pipeline_tag': pipeline_tag,
            'likes': getattr(info, 'likes', None),
            'library_name': getattr(info, 'library_name', None),
            'created_at': getattr(info, 'created_at', None),
            'last_modified': getattr(info, 'last_modified', None),
            'card_data': card_data,
            'model_category': model_category,
            'model_type': model_type,
            'base_model_from_api': base_from_api,
        }

    def search_models_with_keyword(keyword: str, exclude_ids: Set[str]) -> List:
        try:
            results = list(list_models(
                search=keyword,
                full=True,
                limit=600,
                sort="downloads",
                direction=-1
            ))
            filtered = [m for m in results if m.id not in exclude_ids]
            print(f"  ğŸ” æœç´¢ '{keyword}'ï¼š{len(results)} æ¡ï¼Œå»é‡å {len(filtered)} æ¡")
            return filtered
        except Exception as e:
            print(f"  âš ï¸ æœç´¢ '{keyword}' å¤±è´¥: {e}")
            return []

    allowed_categories = {'ernie-4.5', 'paddleocr-vl'}

    def add_record(detail: Dict, data_source: str, base_model: str = None, is_original: bool = False):
        if detail is None:
            return
        model_id = detail['model_id']
        model_name = model_id.split('/')[-1]
        publisher = detail['publisher']
        base_model_val = base_model or detail.get('base_model_from_api')
        model_category = detail['model_category']

        # ä»…ä¿ç•™ç›®æ ‡ç³»åˆ—
        if model_category not in allowed_categories:
            return
        if not include_paddleocr and model_category == 'paddleocr-vl':
            return

        record = {
            'date': date.today().isoformat(),
            'repo': 'Hugging Face',
            'model_name': model_name,
            'publisher': publisher,
            'download_count': detail['downloads'],
            'model_category': model_category,
            'model_type': detail['model_type'] if not is_original else 'original',
            'is_derivative': bool(base_model_val),
            'base_model': base_model_val,
            'data_source': data_source,
            'tags': detail['tags'],
            'likes': detail.get('likes'),
            'library_name': detail.get('library_name'),
            'pipeline_tag': detail.get('pipeline_tag'),
            'created_at': detail.get('created_at'),
            'last_modified': detail.get('last_modified'),
            'fetched_at': date.today().isoformat(),
            'base_model_from_api': detail.get('base_model_from_api'),
            'url': f"https://huggingface.co/{model_id}"  # æ¨¡å‹è¯¦æƒ…é¡µURL
        }
        all_models.append(record)
        processed_ids.add(model_id)

    # ---------- 1. å…¨å±€æœç´¢ ----------
    print(f"\nğŸ” å…¨å±€æœç´¢ï¼ˆ{', '.join(search_terms)}ï¼‰...")
    all_search_models = []
    for search_term in search_terms:
        all_search_models.extend(search_models_with_keyword(search_term, exclude_ids=set()))

    unique_search = {}
    for m in all_search_models:
        if m.id not in unique_search:
            unique_search[m.id] = m
    print(f"ğŸ” å»é‡åå…± {len(unique_search)} æ¡æœç´¢ç»“æœ")

    for model in unique_search.values():
        detail = fetch_model_detail(model.id, model)
        if detail is None:
            continue

        add_record(detail, data_source='search')

        if model.author in ['baidu', 'PaddlePaddle']:
            official_models[model.id] = {
                'id': model.id,
                'category': detail['model_category']
            }

    # ---------- 2. è¡¥å……å®˜æ–¹æ¨¡å‹åˆ—è¡¨ ----------
    print("\nğŸŒ³ æ‰©å……å®˜æ–¹æ¨¡å‹åˆ—è¡¨...")
    try:
        baidu_official = list(list_models(author="baidu", search="ERNIE-4.5", limit=150))
        paddle_official = list(list_models(author="PaddlePaddle", search="PaddleOCR-VL", limit=50))
        print(f"  baidu è´¦å·å®˜æ–¹æ¨¡å‹ {len(baidu_official)} ä¸ªï¼›PaddlePaddle {len(paddle_official)} ä¸ª")
        for m in baidu_official + paddle_official:
            cat = 'paddleocr-vl' if 'paddleocr-vl' in m.id.lower() else 'ernie-4.5'
            if not include_paddleocr and cat == 'paddleocr-vl':
                continue
            if cat not in allowed_categories:
                continue
            official_models.setdefault(m.id, {'id': m.id, 'category': cat})
            if m.id in processed_ids:
                # å·²åœ¨æœç´¢ç»“æœä¸­ï¼Œæ›´æ–°ä¸º official
                for rec in all_models:
                    if f"{rec['publisher']}/{rec['model_name']}" == m.id:
                        rec['data_source'] = 'original'
                        rec['base_model'] = None
                        rec['is_derivative'] = False
                        rec['model_type'] = 'original'
                        break
            else:
                detail = fetch_model_detail(m.id, m)
                if detail:
                    add_record(detail, data_source='original', base_model=None, is_original=True)
    except Exception as e:
        print(f"  âš ï¸ è·å–å®˜æ–¹æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")

    official_list = list(official_models.values())
    print(f"ğŸŒ³ å®˜æ–¹åŸºåº§æ•°é‡: {len(official_list)}")

    # ---------- 3. ä¸ºæ¯ä¸ªå®˜æ–¹æ¨¡å‹æŸ¥ Model Tree + è¡¥å……å…³é”®è¯æœç´¢ ----------
    for official in official_list:
        model_id = official['id']
        model_category = official['category']
        print(f"\nğŸŒ³ å¤„ç†åŸºåº§: {model_id} ({model_category})")

        # Model Tree
        derivatives = get_model_tree_children(model_id, max_depth=1)
        if derivatives:
            for deriv in derivatives:
                deriv_detail = fetch_model_detail(deriv['id'], deriv)
                if deriv_detail is None:
                    continue

                if deriv['id'] not in processed_ids:
                    # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ Model Tree æä¾›çš„ base_model é‡æ–°åˆ†ç±»
                    # å› ä¸º fetch_model_detail ä¸­çš„åˆ†ç±»å¯èƒ½ä½¿ç”¨äº†é”™è¯¯çš„ base_from_apiï¼ˆå¯èƒ½ä¸ºç©ºï¼‰
                    deriv_detail['model_category'] = classify_model(
                        deriv['id'],
                        deriv_detail['publisher'],
                        model_id  # ä½¿ç”¨ Model Tree çš„ base_modelï¼Œè€Œä¸æ˜¯ base_from_api
                    )
                    add_record(deriv_detail, data_source='model_tree', base_model=model_id)
                else:
                    # æ›´æ–°å·²æœ‰è®°å½•ä¸º bothï¼Œè¡¥ base_model
                    for existing in all_models:
                        if f"{existing['publisher']}/{existing['model_name']}" == deriv['id']:
                            existing['data_source'] = 'both'
                            existing['base_model'] = existing.get('base_model') or model_id
                            existing['is_derivative'] = True
                            # ğŸ”§ ä¿®å¤ï¼šä¹Ÿè¦é‡æ–°åˆ†ç±»å·²æœ‰è®°å½•
                            existing['model_category'] = classify_model(
                                deriv['id'],
                                existing['publisher'],
                                model_id
                            )
                            break

        # å…³é”®è¯è¡¥å……æœç´¢ï¼ˆæŒ‰åŸºåº§åï¼‰
        base_keyword = model_id.split('/')[-1]
        extra_results = search_models_with_keyword(base_keyword, exclude_ids=processed_ids)
        for model in extra_results:
            detail = fetch_model_detail(model.id, model)
            if detail is None:
                continue
            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨å½“å‰åŸºåº§çš„ model_id é‡æ–°åˆ†ç±»
            # å› ä¸ºè¿™æ˜¯é€šè¿‡åŸºåº§åç§°æœç´¢åˆ°çš„ï¼Œåº”è¯¥ä¸å½“å‰åŸºåº§ç›¸å…³
            detail['model_category'] = classify_model(
                model.id,
                detail['publisher'],
                model_id  # ä½¿ç”¨å½“å‰åŸºåº§çš„ model_id
            )
            if not include_paddleocr and detail['model_category'] == 'paddleocr-vl':
                continue
            # å¼ºåˆ¶è®¤ä¸ºä¸å½“å‰ base ç›¸å…³ï¼ˆå…œåº•è¡¥å……ï¼‰
            if model.id not in processed_ids:
                add_record(detail, data_source='search', base_model=model_id)
                # è‹¥å·²æœ‰ base_model_from_apiï¼Œä¿ç•™
                if detail.get('base_model_from_api') and not detail.get('base_model'):
                    for rec in all_models:
                        if rec['model_name'] == model.id.split('/')[-1] and rec['publisher'] == detail['publisher']:
                            rec['base_model'] = detail['base_model_from_api']
                            break

    # ---------- 4. è½¬ DataFrame ----------
    df = pd.DataFrame(all_models)
    if not df.empty:
        if 'tags' in df.columns:
            df['tags'] = df['tags'].apply(lambda x: str(x) if isinstance(x, list) else (x if pd.notna(x) else '[]'))
        # æŠŠæ—¶é—´å­—æ®µè½¬ä¸ºå­—ç¬¦ä¸²
        for col in ['created_at', 'last_modified', 'fetched_at']:
            if col in df.columns:
                df[col] = df[col].astype(str)

        print(f"\nğŸ“Š æ€»è®¡è·å– {len(df)} ä¸ªæ¨¡å‹ï¼Œå…¶ä¸­åŸºåº§ {len(official_list)} ä¸ª")

    return df, len(all_models)


def update_ernie_model_tree(save_to_db: bool = True) -> Tuple[pd.DataFrame, int]:
    """
    æ›´æ–°ERNIEæ¨¡å‹æ ‘æ•°æ®ï¼ˆåŒ…å«æ‰€æœ‰è¡ç”Ÿæ¨¡å‹ï¼‰

    Args:
        save_to_db: æ˜¯å¦ä¿å­˜åˆ°æ•°æ®åº“

    Returns:
        Tuple[DataFrame, int]: (æ›´æ–°çš„æ•°æ®, æ€»æ•°é‡)
    """
    print("ğŸ”„ å¼€å§‹æ›´æ–°ERNIEæ¨¡å‹æ ‘æ•°æ®...")

    # è·å–æ‰€æœ‰ERNIEç›¸å…³æ¨¡å‹
    df, total_count = get_all_ernie_derivatives(include_paddleocr=True)

    if df.empty:
        print("âš ï¸ æ²¡æœ‰è·å–åˆ°ä»»ä½•æ¨¡å‹æ•°æ®")
        return df, 0

    # å‡†å¤‡æ•°æ®åº“æ ¼å¼ï¼ˆåŒ…å«æ¨¡å‹ç±»å‹å’Œæ ‡ç­¾ä¿¡æ¯ï¼‰
    required_columns = ['date', 'repo', 'model_name', 'publisher', 'download_count']
    optional_columns = [
        'model_type',
        'model_category',
        'tags',
        'base_model',
        'data_source',
        'likes',
        'library_name',
        'pipeline_tag',
        'created_at',
        'last_modified',
        'fetched_at',
        'base_model_from_api',
    ]

    # ç¡®ä¿æ‰€æœ‰å¿…éœ€åˆ—éƒ½å­˜åœ¨
    available_columns = [col for col in required_columns if col in df.columns]
    db_df = df[available_columns].copy()

    # æ·»åŠ å¯é€‰åˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    for col in optional_columns:
        if col in df.columns:
            db_df[col] = df[col]
        else:
            db_df[col] = None  # ä¸ºç¼ºå¤±çš„åˆ—å¡«å……é»˜è®¤å€¼

    # å°†tagsåˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²å­˜å‚¨
    if 'tags' in db_df.columns:
        db_df['tags'] = db_df['tags'].apply(lambda x: str(x) if isinstance(x, list) else (x if pd.notna(x) else '[]'))

    # ä¿å­˜åˆ°æ•°æ®åº“
    if save_to_db:
        save_to_db(db_df, DB_PATH)
        print(f"ğŸ’¾ å·²ä¿å­˜ {len(db_df)} æ¡è®°å½•åˆ°æ•°æ®åº“")

    return df, total_count


def get_new_derivatives_since(last_date: str) -> pd.DataFrame:
    """
    è·å–è‡ªæŒ‡å®šæ—¥æœŸä»¥æ¥çš„æ–°å¢è¡ç”Ÿæ¨¡å‹

    Args:
        last_date: ä¸Šæ¬¡æ›´æ–°æ—¥æœŸ (YYYY-MM-DD)

    Returns:
        DataFrame: æ–°å¢çš„è¡ç”Ÿæ¨¡å‹
    """
    try:
        # ä»æ•°æ®åº“è·å–æŒ‡å®šæ—¥æœŸä»¥æ¥çš„æ•°æ®
        from ..db import load_data_from_db
        recent_data = load_data_from_db(date_filter=last_date)

        if recent_data.empty:
            return pd.DataFrame()

        # ç­›é€‰ERNIEç›¸å…³çš„è¡ç”Ÿæ¨¡å‹
        ernie_data = recent_data[
            recent_data['model_name'].str.contains('ernie|ERNIE|paddleocr|PaddleOCR', case=False, na=False)
        ].copy()

        return ernie_data

    except Exception as e:
        print(f"è·å–æ–°å¢è¡ç”Ÿæ¨¡å‹å¤±è´¥: {e}")
        return pd.DataFrame()


def get_weekly_new_finetune_adapters(current_date: str, previous_date: str, model_series: str = 'ERNIE-4.5') -> Dict:
    """
    è·å–æœ¬å‘¨æ–°å¢çš„Finetuneå’ŒAdapteræ¨¡å‹ï¼ˆç”¨äºå‘¨æŠ¥å±•ç¤ºï¼‰

    Args:
        current_date: å½“å‰æ—¥æœŸ (YYYY-MM-DD)
        previous_date: å¯¹æ¯”æ—¥æœŸ (YYYY-MM-DD)
        model_series: æ¨¡å‹ç³»åˆ— ('ERNIE-4.5' æˆ– 'PaddleOCR-VL')

    Returns:
        Dict: åŒ…å«æœ¬å‘¨æ–°å¢Finetuneå’ŒAdapteræ¨¡å‹ä¿¡æ¯çš„å­—å…¸
    """
    try:
        from ..db import load_data_from_db

        # è·å–ä¸¤ä¸ªæ—¥æœŸçš„æ•°æ®
        current_data = load_data_from_db(date_filter=current_date)
        previous_data = load_data_from_db(date_filter=previous_date)

        if current_data.empty:
            return {
                'new_finetune_models': [],
                'new_adapter_models': [],
                'new_lora_models': [],
                'total_new': 0,
                'summary': 'æœ¬å‘¨æ²¡æœ‰æ–°å¢æ¨¡å‹æ•°æ®'
            }

        # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ model_category å­—æ®µç²¾ç¡®ç­›é€‰ï¼Œè€Œä¸æ˜¯æœç´¢ model_name
        # æ ¹æ® model_series ç¡®å®šè¦ç­›é€‰çš„ model_category
        if model_series == 'ERNIE-4.5':
            target_category = 'ernie-4.5'
        elif model_series == 'PaddleOCR-VL':
            target_category = 'paddleocr-vl'
        else:
            # é»˜è®¤ä¸º ERNIE-4.5
            target_category = 'ernie-4.5'

        # ç­›é€‰Hugging Faceå¹³å°çš„æŒ‡å®šç³»åˆ—æ¨¡å‹ï¼ˆä½¿ç”¨ model_category å­—æ®µï¼‰
        hf_current = current_data[
            (current_data['repo'] == 'Hugging Face') &
            (current_data['model_category'] == target_category)
        ].copy()

        if previous_data.empty:
            # å¦‚æœæ²¡æœ‰å¯¹æ¯”æ•°æ®ï¼Œå‡è®¾æ‰€æœ‰éƒ½æ˜¯æ–°å¢çš„
            hf_previous = pd.DataFrame()
        else:
            hf_previous = previous_data[
                (previous_data['repo'] == 'Hugging Face') &
                (previous_data['model_category'] == target_category)
            ].copy()

        # æ‰¾å‡ºæ–°å¢çš„æ¨¡å‹ï¼ˆåœ¨å½“å‰æ•°æ®ä¸­ä½†ä¸åœ¨å¯¹æ¯”æ•°æ®ä¸­ï¼‰
        if hf_previous.empty:
            new_models = hf_current.copy()
        else:
            previous_model_names = set(hf_previous['model_name'].tolist())
            new_models = hf_current[~hf_current['model_name'].isin(previous_model_names)].copy()

        if new_models.empty:
            return {
                'new_finetune_models': [],
                'new_adapter_models': [],
                'new_lora_models': [],
                'total_new': 0,
                'summary': 'æœ¬å‘¨æ²¡æœ‰æ–°å¢Finetuneæˆ–Adapteræ¨¡å‹'
            }

        # ğŸ”§ ä¿®å¤ï¼šç›´æ¥ä½¿ç”¨æ•°æ®åº“ä¸­å·²ç»å­˜å‚¨çš„ model_type å­—æ®µï¼Œè€Œä¸æ˜¯é‡æ–°åˆ†ç±»
        # æ•°æ®åœ¨å…¥åº“æ—¶å·²ç»é€šè¿‡ classify_model_type() æ­£ç¡®åˆ†ç±»äº†
        # å¦‚æœ model_type åˆ—ä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼Œæ‰è¿›è¡Œåˆ†ç±»ï¼ˆå…¼å®¹æ—§æ•°æ®ï¼‰
        if 'model_type' not in new_models.columns or new_models['model_type'].isna().all():
            print("âš ï¸ è­¦å‘Šï¼šmodel_type å­—æ®µä¸å­˜åœ¨æˆ–å…¨éƒ¨ä¸ºç©ºï¼Œå°è¯•é‡æ–°åˆ†ç±»")
            new_models['model_type'] = new_models.apply(
                lambda row: classify_model_type(
                    row['model_name'],
                    eval(row['tags']) if pd.notna(row.get('tags')) and row.get('tags') else [],
                    None
                ),
                axis=1
            )

        # æŒ‰ç±»å‹åˆ†ç±»
        new_finetune = new_models[new_models['model_type'] == 'finetune'].copy()
        new_adapter = new_models[new_models['model_type'] == 'adapter'].copy()
        new_lora = new_models[new_models['model_type'] == 'lora'].copy()

        # æ ¼å¼åŒ–è¾“å‡º
        def format_models(df):
            if df.empty:
                return []
            return df[['model_name', 'publisher', 'download_count']].to_dict('records')

        result = {
            'new_finetune_models': format_models(new_finetune),
            'new_adapter_models': format_models(new_adapter),
            'new_lora_models': format_models(new_lora),
            'total_new': len(new_models),
            'summary': f'æœ¬å‘¨å…±å‘ç° {len(new_models)} ä¸ªæ–°å¢æ¨¡å‹ï¼Œå…¶ä¸­ Finetune {len(new_finetune)} ä¸ªï¼ŒAdapter {len(new_adapter)} ä¸ªï¼ŒLoRA {len(new_lora)} ä¸ª'
        }

        return result

    except Exception as e:
        print(f"è·å–æœ¬å‘¨æ–°å¢Finetune/Adapteræ¨¡å‹å¤±è´¥: {e}")
        return {
            'new_finetune_models': [],
            'new_adapter_models': [],
            'new_lora_models': [],
            'total_new': 0,
            'summary': f'è·å–æ•°æ®æ—¶å‡ºé”™: {e}'
        }


def get_weekly_new_model_tree_derivatives(current_date: str, previous_date: str, model_series: str = 'ERNIE-4.5') -> Dict:
    """
    è·å–æœ¬å‘¨æ–°å¢çš„ Model Tree è¡ç”Ÿæ¨¡å‹ï¼ˆä¸“é—¨ç»Ÿè®¡ï¼‰

    æ³¨æ„ï¼šåªç»Ÿè®¡é€šè¿‡ Model Tree æ‰¾åˆ°çš„è¡ç”Ÿæ¨¡å‹ï¼ˆbase_model å­—æ®µä¸ä¸ºç©ºï¼‰

    Args:
        current_date: å½“å‰æ—¥æœŸ (YYYY-MM-DD)
        previous_date: å¯¹æ¯”æ—¥æœŸ (YYYY-MM-DD)
        model_series: æ¨¡å‹ç³»åˆ— ('ERNIE-4.5' æˆ– 'PaddleOCR-VL')

    Returns:
        Dict: åŒ…å«æœ¬å‘¨æ–°å¢Model Treeè¡ç”Ÿæ¨¡å‹ä¿¡æ¯çš„å­—å…¸
    """
    try:
        from ..db import load_data_from_db

        # è·å–ä¸¤ä¸ªæ—¥æœŸçš„æ•°æ®
        current_data = load_data_from_db(date_filter=current_date)
        previous_data = load_data_from_db(date_filter=previous_date)

        if current_data.empty:
            return {
                'new_model_tree_models': [],
                'total_new': 0,
                'summary': 'æœ¬å‘¨æ²¡æœ‰æ–°å¢æ¨¡å‹æ•°æ®'
            }

        # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ model_category å­—æ®µç²¾ç¡®ç­›é€‰ï¼Œè€Œä¸æ˜¯æœç´¢ model_name
        # æ ¹æ® model_series ç¡®å®šè¦ç­›é€‰çš„ model_category
        if model_series == 'ERNIE-4.5':
            target_category = 'ernie-4.5'
        elif model_series == 'PaddleOCR-VL':
            target_category = 'paddleocr-vl'
        else:
            # é»˜è®¤ä¸º ERNIE-4.5
            target_category = 'ernie-4.5'

        # åªç­›é€‰ Hugging Face å¹³å°çš„æŒ‡å®šç³»åˆ—æ¨¡å‹ï¼Œä¸” base_model ä¸ä¸ºç©ºï¼ˆModel Tree è¡ç”Ÿæ¨¡å‹ï¼‰
        hf_current = current_data[
            (current_data['repo'] == 'Hugging Face') &
            (current_data['model_category'] == target_category) &
            (current_data['base_model'].notna()) &  # åªè¦ Model Tree æ‰¾åˆ°çš„
            (current_data['base_model'] != '') &    # base_model ä¸ä¸ºç©º
            (current_data['base_model'] != 'None')  # æ’é™¤å­—ç¬¦ä¸² 'None'
        ].copy()

        if previous_data.empty:
            # å¦‚æœæ²¡æœ‰å¯¹æ¯”æ•°æ®ï¼Œå‡è®¾æ‰€æœ‰éƒ½æ˜¯æ–°å¢çš„
            hf_previous = pd.DataFrame()
        else:
            hf_previous = previous_data[
                (previous_data['repo'] == 'Hugging Face') &
                (previous_data['model_category'] == target_category) &
                (previous_data['base_model'].notna()) &
                (previous_data['base_model'] != '') &
                (previous_data['base_model'] != 'None')
            ].copy()

        # æ‰¾å‡ºæ–°å¢çš„æ¨¡å‹ï¼ˆåœ¨å½“å‰æ•°æ®ä¸­ä½†ä¸åœ¨å¯¹æ¯”æ•°æ®ä¸­ï¼‰
        if hf_previous.empty:
            new_models = hf_current.copy()
        else:
            previous_model_names = set(hf_previous['model_name'].tolist())
            new_models = hf_current[~hf_current['model_name'].isin(previous_model_names)].copy()

        if new_models.empty:
            return {
                'new_model_tree_models': [],
                'total_new': 0,
                'summary': 'æœ¬å‘¨æ²¡æœ‰æ–°å¢ Model Tree è¡ç”Ÿæ¨¡å‹'
            }

        # ğŸ”§ ä¿®å¤ï¼šç›´æ¥ä½¿ç”¨æ•°æ®åº“ä¸­å·²ç»å­˜å‚¨çš„ model_type å­—æ®µï¼Œè€Œä¸æ˜¯é‡æ–°åˆ†ç±»
        # æ•°æ®åœ¨å…¥åº“æ—¶å·²ç»é€šè¿‡ classify_model_type() æ­£ç¡®åˆ†ç±»äº†
        # å¦‚æœ model_type åˆ—ä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼Œæ‰è¿›è¡Œåˆ†ç±»ï¼ˆå…¼å®¹æ—§æ•°æ®ï¼‰
        if 'model_type' not in new_models.columns or new_models['model_type'].isna().all():
            print("âš ï¸ è­¦å‘Šï¼šmodel_type å­—æ®µä¸å­˜åœ¨æˆ–å…¨éƒ¨ä¸ºç©ºï¼Œå°è¯•é‡æ–°åˆ†ç±»")
            new_models['model_type'] = new_models.apply(
                lambda row: classify_model_type(
                    row['model_name'],
                    eval(row['tags']) if pd.notna(row.get('tags')) and row.get('tags') else [],
                    None
                ),
                axis=1
            )

        # æ ¼å¼åŒ–è¾“å‡ºï¼ˆå¢åŠ  base_model å’Œ model_type ä¿¡æ¯ï¼‰
        def format_models(df):
            if df.empty:
                return []
            # å¢åŠ  base_model å’Œ model_type åˆ—ï¼Œæ–¹ä¾¿åœ¨å‘¨æŠ¥ä¸­æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            if 'base_model' in df.columns and 'model_type' in df.columns:
                return df[['model_name', 'publisher', 'download_count', 'base_model', 'model_type']].to_dict('records')
            else:
                return df[['model_name', 'publisher', 'download_count']].to_dict('records')

        result = {
            'new_model_tree_models': format_models(new_models),
            'total_new': len(new_models),
            'summary': f'æœ¬å‘¨ Model Tree æ–°å¢ {len(new_models)} ä¸ªè¡ç”Ÿæ¨¡å‹'
        }

        return result

    except Exception as e:
        print(f"è·å–æœ¬å‘¨æ–°å¢ Model Tree è¡ç”Ÿæ¨¡å‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {
            'new_model_tree_models': [],
            'total_new': 0,
            'summary': f'è·å–æ•°æ®æ—¶å‡ºé”™: {e}'
        }


# =============================================================================
# ModelScope Model Tree åŠŸèƒ½æ¨¡å—
# =============================================================================

def get_modelscope_model_tree_children(base_model_id: str, driver=None, progress_callback=None) -> List[Dict]:
    """
    è·å– ModelScope æ¨¡å‹çš„è¡ç”Ÿæ¨¡å‹ï¼ˆé€šè¿‡è§£æé¡µé¢ HTMLï¼‰

    Args:
        base_model_id: åŸºç¡€æ¨¡å‹IDï¼ˆå¦‚ 'PaddlePaddle/PaddleOCR-VL'ï¼‰
        driver: Selenium WebDriver å®ä¾‹ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™åˆ›å»ºæ–°çš„ï¼‰
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°

    Returns:
        List[Dict]: è¡ç”Ÿæ¨¡å‹ä¿¡æ¯åˆ—è¡¨
    """
    from ..utils import create_chrome_driver
    from ..config import SELENIUM_TIMEOUT
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import TimeoutException, NoSuchElementException
    from modelscope.hub.api import HubApi
    import time
    import re

    print(f"\nğŸ“Š è·å– {base_model_id} çš„ ModelScope Model Tree...")

    should_close_driver = False
    if driver is None:
        driver = create_chrome_driver()
        should_close_driver = True

    try:
        # æ„å»ºæ¨¡å‹é¡µé¢URL
        model_url = f"https://modelscope.cn/models/{base_model_id}"
        print(f"  è®¿é—®: {model_url}")
        driver.get(model_url)

        # ç­‰å¾…é¡µé¢åŠ è½½
        try:
            WebDriverWait(driver, SELENIUM_TIMEOUT).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            time.sleep(2)  # é¢å¤–ç­‰å¾…ç¡®ä¿åŠ¨æ€å†…å®¹åŠ è½½
        except TimeoutException:
            print(f"  âš ï¸ é¡µé¢åŠ è½½è¶…æ—¶")
            return []

        # æŸ¥æ‰¾æ‰€æœ‰æ¨¡å‹è¡€ç¼˜ï¼ˆgenealogyï¼‰ç›¸å…³çš„è¡ç”Ÿç±»å‹å…ƒç´ 
        # ç›´æ¥æŸ¥æ‰¾ span.antd5-tree-node-content-wrapperï¼ˆè¿™æ˜¯çœŸæ­£å¯ç‚¹å‡»çš„å…ƒç´ ï¼‰
        try:
            # æŸ¥æ‰¾æ‰€æœ‰å¯ç‚¹å‡»çš„ tree node wrapper å…ƒç´ 
            node_wrappers = driver.find_elements(
                By.CSS_SELECTOR,
                "span.antd5-tree-node-content-wrapper"
            )

            if not node_wrappers:
                print(f"  âšªï¸ æ²¡æœ‰æ‰¾åˆ°è¡ç”Ÿç±»å‹")
                return []

            print(f"  âœ… æ‰¾åˆ° {len(node_wrappers)} ä¸ª node wrapper å…ƒç´ ")

            # è¿‡æ»¤å‡ºçœŸæ­£çš„è¡ç”Ÿç±»å‹ï¼ˆæ’é™¤"å½“å‰æ¨¡å‹"ï¼‰
            derivative_types = []
            for wrapper in node_wrappers:
                try:
                    # åœ¨æ¯ä¸ª wrapper å†…éƒ¨æŸ¥æ‰¾ span.antd5-tree-titleï¼Œç„¶åå†æ‰¾ div.acss-1lekzkb
                    try:
                        tree_title = wrapper.find_element(By.CSS_SELECTOR, "span.antd5-tree-title")
                        content_div = tree_title.find_element(By.CSS_SELECTOR, "div.acss-1lekzkb")
                    except:
                        continue

                    # è·å–å…ƒç´ æ–‡æœ¬ï¼Œæ£€æŸ¥æ˜¯å¦ä¸º"å½“å‰æ¨¡å‹"
                    element_text = content_div.text.strip()

                    if not element_text:
                        continue

                    # æ£€æŸ¥æ˜¯å¦åŒ…å«"å½“å‰æ¨¡å‹"æ ‡è®°
                    if "å½“å‰æ¨¡å‹" in element_text:
                        continue

                    # æå–ä¸­è‹±æ–‡åç§°
                    # æ ¹æ®HTMLç»“æ„ï¼Œåº”è¯¥æ˜¯"å¾®è°ƒ Finetunes"æˆ–ç±»ä¼¼æ ¼å¼
                    text_parts = element_text.split('\n')
                    if len(text_parts) >= 2:
                        name_zh = text_parts[0].strip()
                        name_en = text_parts[1].strip()

                        # æå–æ¨¡å‹æ•°é‡ï¼ˆé€šå¸¸åœ¨æœ€åä¸€ä¸ªéƒ¨åˆ†ï¼‰
                        count_match = re.search(r'å…±(\d+)ä¸ªæ¨¡å‹', element_text)
                        count = int(count_match.group(1)) if count_match else 0

                        if count > 0:
                            # ğŸ”§ å…³é”®ä¿®å¤ï¼šéœ€è¦ç‚¹å‡»çš„æ˜¯å†…éƒ¨çš„ div.acss-hd4erfï¼ˆåŒ…å«ä¸­æ–‡æ ‡é¢˜çš„divï¼‰
                            # è€Œä¸æ˜¯å¤–å±‚çš„ wrapper
                            try:
                                clickable_element = content_div.find_element(By.CSS_SELECTOR, "div.acss-hd4erf")
                            except:
                                # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå›é€€åˆ°ä½¿ç”¨wrapper
                                clickable_element = wrapper

                            derivative_types.append({
                                'element': clickable_element,  # ä½¿ç”¨å†…éƒ¨çš„å¯ç‚¹å‡»div
                                'name_zh': name_zh,
                                'name_en': name_en,
                                'count': count
                            })
                            print(f"    ğŸ“‚ {name_zh} / {name_en}: {count}ä¸ªæ¨¡å‹")

                except Exception as e:
                    print(f"    âš ï¸ è§£æè¡ç”Ÿç±»å‹å…ƒç´ æ—¶å‡ºé”™: {e}")
                    continue

            if not derivative_types:
                print(f"  âšªï¸ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è¡ç”Ÿç±»å‹")
                return []

            # åˆå§‹åŒ– ModelScope API
            api = HubApi()
            all_derivatives = []

            # ğŸ”§ æ–°ç­–ç•¥ï¼šå…ˆæ‰“å¼€ä¾§è¾¹æ ï¼ˆåªç‚¹å‡»ç¬¬ä¸€ä¸ªè¡ç”Ÿç±»å‹ï¼‰
            # ç„¶ååœ¨ä¾§è¾¹æ å†…éƒ¨é€šè¿‡ç‚¹å‡»æ ‡ç­¾åˆ‡æ¢ä¸åŒç±»å‹
            print(f"\n  ğŸ“‚ æ‰“å¼€ä¾§è¾¹æ ...")

            if not derivative_types:
                return []

            # ä½¿ç”¨ç¬¬ä¸€ä¸ªè¡ç”Ÿç±»å‹æ‰“å¼€ä¾§è¾¹æ 
            first_type = derivative_types[0]
            first_element = first_type['element']

            try:
                # æ»šåŠ¨åˆ°å…ƒç´ å¯è§
                driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", first_element)
                time.sleep(0.5)

                # ç‚¹å‡»ç¬¬ä¸€ä¸ªè¡ç”Ÿç±»å‹æ‰“å¼€ä¾§è¾¹æ 
                first_element.click()
                print(f"    âœ… å·²ç‚¹å‡»ç¬¬ä¸€ä¸ªè¡ç”Ÿç±»å‹æ‰“å¼€ä¾§è¾¹æ ")

                # ç­‰å¾…ä¾§è¾¹æ åŠ è½½
                print(f"    â³ ç­‰å¾…ä¾§è¾¹æ åŠ è½½...")
                before_click_links = len(driver.find_elements(By.CSS_SELECTOR, "a[href*='/models/']"))

                max_wait = 10
                waited = 0
                while waited < max_wait:
                    time.sleep(1)
                    waited += 1
                    current_links = len(driver.find_elements(By.CSS_SELECTOR, "a[href*='/models/']"))
                    if current_links > before_click_links:
                        print(f"    âœ… ä¾§è¾¹æ å·²åŠ è½½ï¼ˆç­‰å¾…äº† {waited} ç§’ï¼‰")
                        break
                else:
                    print(f"    âš ï¸ ç­‰å¾… {max_wait} ç§’åä¾§è¾¹æ ä»æœªåŠ è½½")
                    return []

            except Exception as e:
                print(f"    âŒ æ‰“å¼€ä¾§è¾¹æ å¤±è´¥: {e}")
                return []

            # ğŸ”§ å…³é”®æ”¹è¿›ï¼šåœ¨ä¾§è¾¹æ å†…éƒ¨é€šè¿‡ç‚¹å‡»æ ‡ç­¾åˆ‡æ¢ä¸åŒç±»å‹
            # æŸ¥æ‰¾ä¾§è¾¹æ å†…çš„æ ‡ç­¾å…ƒç´ 
            try:
                # ç­‰å¾…ä¾§è¾¹æ å®Œå…¨åŠ è½½
                time.sleep(2)

                # æŸ¥æ‰¾æ‰€æœ‰è¡ç”Ÿç±»å‹æ ‡ç­¾
                tab_elements = driver.find_elements(By.CSS_SELECTOR, "div.acss-xqwyei")

                if not tab_elements:
                    print(f"    âš ï¸ ä¾§è¾¹æ ä¸­æ²¡æœ‰æ‰¾åˆ°æ ‡ç­¾å…ƒç´ ")
                    # ä½¿ç”¨åŸæ¥çš„é€»è¾‘ï¼ˆé€ä¸ªç‚¹å‡»å¤–éƒ¨å…ƒç´ ï¼‰
                    print(f"    ğŸ“‹ å›é€€åˆ°åŸæ¥çš„ç‚¹å‡»æ–¹å¼...")
                else:
                    print(f"    âœ… æ‰¾åˆ° {len(tab_elements)} ä¸ªä¾§è¾¹æ æ ‡ç­¾")

                    # ä¸ºæ¯ä¸ªæ ‡ç­¾å»ºç«‹æ˜ å°„ï¼šæ ‡ç­¾æ–‡æœ¬ -> è¡ç”Ÿç±»å‹ä¿¡æ¯
                    tab_mapping = []
                    for tab in tab_elements:
                        try:
                            tab_text = tab.text.strip()
                            # æå–ä¸­æ–‡åç§°ï¼ˆç¬¬ä¸€è¡Œï¼‰
                            name_zh = tab_text.split('\n')[0] if '\n' in tab_text else tab_text

                            # åœ¨ derivative_types ä¸­æ‰¾åˆ°å¯¹åº”çš„ç±»å‹ä¿¡æ¯
                            matching_type = None
                            for dt in derivative_types:
                                if dt['name_zh'] == name_zh:
                                    matching_type = dt
                                    break

                            if matching_type:
                                tab_mapping.append({
                                    'tab': tab,
                                    'name_zh': matching_type['name_zh'],
                                    'name_en': matching_type['name_en'],
                                    'count': matching_type['count']
                                })
                        except:
                            continue

                    print(f"    ğŸ“‹ å»ºç«‹äº† {len(tab_mapping)} ä¸ªæ ‡ç­¾æ˜ å°„")

                    # é€ä¸ªç‚¹å‡»æ ‡ç­¾å¹¶è·å–æ¨¡å‹
                    for idx, tab_info in enumerate(tab_mapping):
                        try:
                            name_zh = tab_info['name_zh']
                            name_en = tab_info['name_en']
                            tab = tab_info['tab']

                            print(f"\n  [{idx + 1}/{len(tab_mapping)}] åˆ‡æ¢åˆ°: {name_zh} / {name_en}")

                            # ç‚¹å‡»æ ‡ç­¾
                            driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", tab)
                            time.sleep(0.5)

                            # ä½¿ç”¨JavaScriptç‚¹å‡»ï¼ˆæ›´å¯é ï¼‰
                            driver.execute_script("arguments[0].click();", tab)
                            print(f"    âœ… å·²åˆ‡æ¢æ ‡ç­¾")

                            # ç­‰å¾…å†…å®¹åŠ è½½
                            time.sleep(2)

                            # æŸ¥æ‰¾å½“å‰æ ‡ç­¾ä¸‹çš„æ¨¡å‹å¡ç‰‡
                            all_links = driver.find_elements(By.CSS_SELECTOR, "a[href*='/models/']")
                            model_cards = []
                            for link in all_links:
                                href = link.get_attribute('href')
                                if href and '/models/' in href:
                                    if not any(x in href for x in ['/summary', '/files', '/feedback', '/file/view']):
                                        model_cards.append(link)

                            if not model_cards:
                                print(f"    âšªï¸ å½“å‰æ ‡ç­¾ä¸‹æ²¡æœ‰æ‰¾åˆ°æ¨¡å‹å¡ç‰‡")
                                continue

                            print(f"    âœ… æ‰¾åˆ° {len(model_cards)} ä¸ªæ¨¡å‹å¡ç‰‡")

                            # æå–æ¨¡å‹ä¿¡æ¯
                            for card in model_cards:
                                try:
                                    href = card.get_attribute('href')
                                    if not href or '/models/' not in href:
                                        continue

                                    model_id = href.split('/models/')[-1]
                                    if '?' in model_id:
                                        model_id = model_id.split('?')[0]

                                    print(f"      ğŸ” æ£€æŸ¥æ¨¡å‹: {model_id}")

                                    # è·³è¿‡åŸºç¡€æ¨¡å‹æœ¬èº«
                                    if model_id == base_model_id:
                                        print(f"        â­ï¸ è·³è¿‡ï¼ˆè¿™æ˜¯åŸºç¡€æ¨¡å‹æœ¬èº«ï¼‰")
                                        continue

                                    print(f"      ğŸ“¦ {model_id}")

                                    # ä½¿ç”¨APIè·å–æ¨¡å‹è¯¦ç»†ä¿¡æ¯
                                    try:
                                        info = api.get_model(model_id, revision="master")
                                        downloads = info.get("Downloads", 0)

                                        from datetime import datetime
                                        created_at = None
                                        last_modified = None

                                        if "CreatedTime" in info and info["CreatedTime"]:
                                            try:
                                                created_at = datetime.fromtimestamp(info["CreatedTime"]).strftime('%Y-%m-%d')
                                            except:
                                                pass

                                        if "LastUpdatedTime" in info and info["LastUpdatedTime"]:
                                            try:
                                                last_modified = datetime.fromtimestamp(info["LastUpdatedTime"]).strftime('%Y-%m-%d')
                                            except:
                                                pass

                                        publisher = model_id.split('/')[0] if '/' in model_id else 'Unknown'

                                        derivative_info = {
                                            'id': model_id,
                                            'author': publisher,
                                            'downloads': downloads,
                                            'pipeline_tag': None,
                                            'tags': [],
                                            'created_at': created_at,
                                            'last_modified': last_modified,
                                            'likes': info.get('Likes', 0),
                                            'model_type': name_en.lower(),
                                            'base_model': base_model_id,
                                            'name_zh': name_zh,
                                            'name_en': name_en
                                        }

                                        all_derivatives.append(derivative_info)

                                    except Exception as e:
                                        print(f"        âš ï¸ APIè·å–å¤±è´¥: {e}")
                                        publisher = model_id.split('/')[0] if '/' in model_id else 'Unknown'
                                        derivative_info = {
                                            'id': model_id,
                                            'author': publisher,
                                            'downloads': 0,
                                            'pipeline_tag': None,
                                            'tags': [],
                                            'created_at': None,
                                            'last_modified': None,
                                            'likes': 0,
                                            'model_type': name_en.lower(),
                                            'base_model': base_model_id,
                                            'name_zh': name_zh,
                                            'name_en': name_en
                                        }
                                        all_derivatives.append(derivative_info)

                                except Exception as e:
                                    print(f"      âš ï¸ å¤„ç†æ¨¡å‹æ—¶å‡ºé”™: {e}")
                                    continue

                        except Exception as e:
                            print(f"    âš ï¸ å¤„ç†æ ‡ç­¾æ—¶å‡ºé”™: {e}")
                            continue

                    print(f"\n  âœ… æ€»å…±è·å– {len(all_derivatives)} ä¸ªè¡ç”Ÿæ¨¡å‹")
                    return all_derivatives

            except Exception as e:
                print(f"    âŒ ä¾§è¾¹æ æ ‡ç­¾åˆ‡æ¢å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                # ç»§ç»­æ‰§è¡Œï¼Œå°è¯•ä½¿ç”¨åŸæ¥çš„é€»è¾‘
                pass

            # å¦‚æœä¾§è¾¹æ æ ‡ç­¾åˆ‡æ¢å¤±è´¥ï¼Œä½¿ç”¨åŸæ¥çš„é€ä¸ªç‚¹å‡»æ–¹å¼
            print(f"\n  ğŸ“‹ ä½¿ç”¨åŸæ¥çš„é€ä¸ªç‚¹å‡»æ–¹å¼...")
            for idx, deriv_type in enumerate(derivative_types):
                try:
                    name_zh = deriv_type['name_zh']
                    name_en = deriv_type['name_en']
                    count = deriv_type['count']
                    element = deriv_type['element']

                    print(f"\n  [{idx + 1}/{len(derivative_types)}] å¤„ç†è¡ç”Ÿç±»å‹: {name_zh} / {name_en}")

                    # ç‚¹å‡»è¡ç”Ÿç±»å‹å…ƒç´ ï¼Œæ‰“å¼€ä¾§è¾¹æ 
                    try:
                        # æ»šåŠ¨åˆ°å…ƒç´ å¯è§
                        driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", element)
                        time.sleep(0.5)

                        # ç‚¹å‡»å…ƒç´ 
                        element.click()
                        print(f"    âœ… å·²ç‚¹å‡»è¡ç”Ÿç±»å‹")

                        # ğŸ”§ å…³é”®ä¿®å¤ï¼šç­‰å¾…ä¾§è¾¹æ çœŸçš„å‡ºç°ï¼Œè€Œä¸æ˜¯ç®€å•ç­‰å¾…å›ºå®šæ—¶é—´
                        # ç­‰å¾…é“¾æ¥æ•°é‡å¢åŠ ï¼ˆè¯´æ˜ä¾§è¾¹æ å·²ç»åŠ è½½äº†æ–°å†…å®¹ï¼‰
                        print(f"    â³ ç­‰å¾…ä¾§è¾¹æ åŠ è½½...")

                        # å…ˆè·å–å½“å‰é“¾æ¥æ•°é‡
                        before_click_links = len(driver.find_elements(By.CSS_SELECTOR, "a[href*='/models/']"))

                        # ç­‰å¾…æœ€å¤š10ç§’ï¼Œç›´åˆ°é“¾æ¥æ•°é‡å¢åŠ 
                        max_wait = 10
                        waited = 0
                        while waited < max_wait:
                            time.sleep(1)
                            waited += 1
                            current_links = len(driver.find_elements(By.CSS_SELECTOR, "a[href*='/models/']"))
                            if current_links > before_click_links:
                                print(f"    âœ… ä¾§è¾¹æ å·²åŠ è½½ï¼ˆç­‰å¾…äº† {waited} ç§’ï¼‰")
                                break
                        else:
                            print(f"    âš ï¸ ç­‰å¾… {max_wait} ç§’åä¾§è¾¹æ ä»æœªåŠ è½½æ–°å†…å®¹")

                    except Exception as e:
                        print(f"    âš ï¸ ç‚¹å‡»è¡ç”Ÿç±»å‹å¤±è´¥: {e}")
                        continue

                    # æŸ¥æ‰¾ä¾§è¾¹æ ä¸­çš„æ¨¡å‹å¡ç‰‡
                    # æ ¹æ®HTMLç»“æ„ï¼Œæ¨¡å‹å¡ç‰‡åœ¨ä¾§è¾¹æ ä¸­ï¼ŒåŒ…å«æ¨¡å‹åç§°
                    try:
                        # å°è¯•å¤šç§é€‰æ‹©å™¨æŸ¥æ‰¾æ¨¡å‹å¡ç‰‡
                        model_cards = []

                        # æ–¹æ³•1: ç›´æ¥æŸ¥æ‰¾æ¨¡å‹é“¾æ¥ï¼ˆæ’é™¤åŸºç¡€æ¨¡å‹æœ¬èº«çš„å­é¡µé¢ï¼‰
                        all_links = driver.find_elements(By.CSS_SELECTOR, "a[href*='/models/']")
                        for link in all_links:
                            href = link.get_attribute('href')
                            if href and '/models/' in href:
                                # æ’é™¤åŸºç¡€æ¨¡å‹æœ¬èº«çš„å­é¡µé¢ï¼ˆå¦‚ summaryã€filesã€feedbackï¼‰
                                # åªä¿ç•™çœŸæ­£çš„æ¨¡å‹é“¾æ¥ï¼ˆæ ¼å¼ï¼š/models/username/modelnameï¼‰
                                if not any(x in href for x in ['/summary', '/files', '/feedback', '/file/view']):
                                    model_cards.append(link)

                        if not model_cards:
                            print(f"    âšªï¸ ä¾§è¾¹æ ä¸­æ²¡æœ‰æ‰¾åˆ°æ¨¡å‹å¡ç‰‡")
                            # å°è¯•å…³é—­ä¾§è¾¹æ ï¼ˆæŒ‰ESCé”®æˆ–ç‚¹å‡»èƒŒæ™¯ï¼‰
                            try:
                                from selenium.webdriver.common.keys import Keys
                                driver.find_element(By.TAG_NAME, "body").send_keys(Keys.ESCAPE)
                                time.sleep(0.5)
                            except:
                                pass
                            continue

                        print(f"    âœ… æ‰¾åˆ° {len(model_cards)} ä¸ªæ¨¡å‹å¡ç‰‡")

                        # æå–æ¨¡å‹ä¿¡æ¯
                        for card in model_cards:
                            try:
                                # è·å–æ¨¡å‹IDï¼ˆä»hrefå±æ€§ï¼‰
                                href = card.get_attribute('href')
                                if not href or '/models/' not in href:
                                    print(f"      âš ï¸ è·³è¿‡æ— æ•ˆé“¾æ¥: href={href}")
                                    continue

                                model_id = href.split('/models/')[-1]
                                if '?' in model_id:
                                    model_id = model_id.split('?')[0]

                                print(f"      ğŸ” æ£€æŸ¥æ¨¡å‹: {model_id}")

                                # è·³è¿‡åŸºç¡€æ¨¡å‹æœ¬èº«
                                if model_id == base_model_id:
                                    print(f"        â­ï¸ è·³è¿‡ï¼ˆè¿™æ˜¯åŸºç¡€æ¨¡å‹æœ¬èº«ï¼‰")
                                    continue

                                print(f"      ğŸ“¦ {model_id}")

                                # ä½¿ç”¨APIè·å–æ¨¡å‹è¯¦ç»†ä¿¡æ¯
                                try:
                                    info = api.get_model(model_id, revision="master")

                                    # æå–ä¸‹è½½é‡
                                    downloads = info.get("Downloads", 0)

                                    # æå–æ—¶é—´å­—æ®µ
                                    from datetime import datetime
                                    created_at = None
                                    last_modified = None

                                    if "CreatedTime" in info and info["CreatedTime"]:
                                        try:
                                            created_at = datetime.fromtimestamp(info["CreatedTime"]).strftime('%Y-%m-%d')
                                        except:
                                            pass

                                    if "LastUpdatedTime" in info and info["LastUpdatedTime"]:
                                        try:
                                            last_modified = datetime.fromtimestamp(info["LastUpdatedTime"]).strftime('%Y-%m-%d')
                                        except:
                                            pass

                                    # æå–å‘å¸ƒè€…
                                    publisher = model_id.split('/')[0] if '/' in model_id else 'Unknown'

                                    # åˆ›å»ºè¡ç”Ÿæ¨¡å‹è®°å½•
                                    derivative_info = {
                                        'id': model_id,
                                        'author': publisher,
                                        'downloads': downloads,
                                        'pipeline_tag': None,
                                        'tags': [],
                                        'created_at': created_at,
                                        'last_modified': last_modified,
                                        'likes': info.get('Likes', 0),
                                        'model_type': name_en.lower(),  # finetune, quantized, etc.
                                        'base_model': base_model_id,
                                        'name_zh': name_zh,
                                        'name_en': name_en
                                    }

                                    all_derivatives.append(derivative_info)

                                except Exception as e:
                                    print(f"        âš ï¸ APIè·å–å¤±è´¥: {e}")
                                    # å³ä½¿APIå¤±è´¥ï¼Œä¹Ÿå¯ä»¥ä¿å­˜åŸºæœ¬ä¿¡æ¯
                                    publisher = model_id.split('/')[0] if '/' in model_id else 'Unknown'
                                    derivative_info = {
                                        'id': model_id,
                                        'author': publisher,
                                        'downloads': 0,
                                        'pipeline_tag': None,
                                        'tags': [],
                                        'created_at': None,
                                        'last_modified': None,
                                        'likes': 0,
                                        'model_type': name_en.lower(),
                                        'base_model': base_model_id,
                                        'name_zh': name_zh,
                                        'name_en': name_en
                                    }
                                    all_derivatives.append(derivative_info)

                            except Exception as e:
                                print(f"      âš ï¸ å¤„ç†æ¨¡å‹å¡ç‰‡æ—¶å‡ºé”™: {e}")
                                continue

                        # å…³é—­ä¾§è¾¹æ ï¼ˆæŒ‰ESCé”®æˆ–ç‚¹å‡»èƒŒæ™¯ï¼‰
                        try:
                            from selenium.webdriver.common.keys import Keys
                            driver.find_element(By.TAG_NAME, "body").send_keys(Keys.ESCAPE)
                            time.sleep(0.5)
                        except:
                            pass

                        if progress_callback:
                            progress_callback(idx + 1, total=len(derivative_types))

                    except Exception as e:
                        print(f"    âš ï¸ å¤„ç†ä¾§è¾¹æ æ—¶å‡ºé”™: {e}")
                        continue

                except Exception as e:
                    print(f"  âš ï¸ å¤„ç†è¡ç”Ÿç±»å‹æ—¶å‡ºé”™: {e}")
                    continue

            print(f"\n  âœ… æ€»å…±è·å– {len(all_derivatives)} ä¸ªè¡ç”Ÿæ¨¡å‹")
            return all_derivatives

        except NoSuchElementException:
            print(f"  âšªï¸ æœªæ‰¾åˆ°æ¨¡å‹è¡€ç¼˜å…ƒç´ ")
            return []

    except Exception as e:
        print(f"  âŒ è·å– {base_model_id} çš„ ModelScope Model Tree å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return []

    finally:
        if should_close_driver and driver:
            driver.quit()


def get_all_modelscope_derivatives(
    base_models: List[str] = None,
    auto_discover: bool = True,
    progress_callback=None
) -> Tuple[pd.DataFrame, int]:
    """
    è·å– ModelScope ä¸Šæ‰€æœ‰æŒ‡å®šåŸºç¡€æ¨¡å‹çš„è¡ç”Ÿæ¨¡å‹

    Args:
        base_models: åŸºç¡€æ¨¡å‹IDåˆ—è¡¨ï¼ˆå¦‚æœä¸ºNoneä¸”auto_discover=Trueï¼Œåˆ™è‡ªåŠ¨ä»æ•°æ®åº“å‘ç°ï¼‰
        auto_discover: æ˜¯å¦è‡ªåŠ¨ä»æ•°æ®åº“ä¸­å‘ç°æ‰€æœ‰ModelScopeå®˜æ–¹æ¨¡å‹
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°

    Returns:
        Tuple[DataFrame, int]: (è¡ç”Ÿæ¨¡å‹æ•°æ®, æ€»æ•°é‡)
    """
    from ..utils import create_chrome_driver
    import sqlite3

    # å¦‚æœæ²¡æœ‰æä¾›åŸºç¡€æ¨¡å‹åˆ—è¡¨ï¼Œè‡ªåŠ¨ä»æ•°æ®åº“å‘ç°
    if base_models is None and auto_discover:
        print(f"\nğŸ” è‡ªåŠ¨å‘ç° ModelScope å®˜æ–¹æ¨¡å‹...")
        try:
            conn = sqlite3.connect(DB_PATH)

            # æŸ¥è¯¢æ‰€æœ‰ModelScopeå¹³å°çš„å®˜æ–¹æ¨¡å‹
            query = """
                SELECT DISTINCT publisher, model_name
                FROM model_downloads
                WHERE repo = 'ModelScope'
                AND (
                    publisher IN ('ç™¾åº¦', 'baidu', 'Paddle', 'PaddlePaddle', 'yiyan', 'ä¸€è¨€')
                    OR publisher LIKE '%ç™¾åº¦%'
                    OR publisher LIKE '%baidu%'
                    OR publisher LIKE '%Paddle%'
                )
                ORDER BY publisher, model_name
            """

            df = pd.read_sql_query(query, conn)
            conn.close()

            if df.empty:
                print(f"  âš ï¸ æ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ° ModelScope å®˜æ–¹æ¨¡å‹")
                base_models = []
            else:
                # æ„å»ºæ¨¡å‹IDåˆ—è¡¨
                base_models = [f"{row['publisher']}/{row['model_name']}" for _, row in df.iterrows()]
                print(f"  âœ… å‘ç° {len(base_models)} ä¸ªå®˜æ–¹æ¨¡å‹")

                # æ˜¾ç¤ºå‰10ä¸ªæ¨¡å‹
                for i, model_id in enumerate(base_models[:10]):
                    print(f"    {i+1}. {model_id}")
                if len(base_models) > 10:
                    print(f"    ... è¿˜æœ‰ {len(base_models) - 10} ä¸ªæ¨¡å‹")

        except Exception as e:
            print(f"  âŒ è‡ªåŠ¨å‘ç°å¤±è´¥: {e}")
            base_models = []

    # å¦‚æœä»ç„¶æ²¡æœ‰åŸºç¡€æ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤åˆ—è¡¨
    if not base_models:
        base_models = [
            'PaddlePaddle/PaddleOCR-VL',
        ]
        print(f"\nğŸ“‹ ä½¿ç”¨é»˜è®¤åŸºç¡€æ¨¡å‹åˆ—è¡¨")

    print(f"\nğŸš€ å¼€å§‹è·å– ModelScope è¡ç”Ÿæ¨¡å‹...")
    print(f"ğŸ“‹ åŸºç¡€æ¨¡å‹åˆ—è¡¨: {len(base_models)} ä¸ª")
    print(f"   {', '.join(base_models[:5])}")
    if len(base_models) > 5:
        print(f"   ... è¿˜æœ‰ {len(base_models) - 5} ä¸ªæ¨¡å‹")

    all_models = []
    processed_ids = set()

    driver = create_chrome_driver()

    try:
        for idx, base_model in enumerate(base_models, start=1):
            print(f"\n{'=' * 80}")
            print(f"[{idx}/{len(base_models)}] å¤„ç†åŸºç¡€æ¨¡å‹: {base_model}")
            print(f"{'=' * 80}")

            # è°ƒç”¨è¿›åº¦å›è°ƒ
            if progress_callback:
                progress_callback(idx)

            try:
                # è·å–è¯¥åŸºç¡€æ¨¡å‹çš„è¡ç”Ÿæ¨¡å‹
                derivatives = get_modelscope_model_tree_children(base_model, driver=driver)

                if derivatives:
                    print(f"  âœ… è·å–åˆ° {len(derivatives)} ä¸ªè¡ç”Ÿæ¨¡å‹")

                    for deriv in derivatives:
                        model_id = deriv['id']

                        # è·³è¿‡é‡å¤çš„æ¨¡å‹
                        if model_id in processed_ids:
                            print(f"      â­ï¸ è·³è¿‡é‡å¤æ¨¡å‹: {model_id}")
                            continue

                        processed_ids.add(model_id)

                        # åˆ›å»ºè®°å½•
                        record = {
                            'date': date.today().isoformat(),
                            'repo': 'ModelScope',
                            'model_name': model_id.split('/')[-1] if '/' in model_id else model_id,
                            'publisher': deriv['author'],
                            'download_count': deriv['downloads'],
                            'model_category': classify_model(
                                deriv['id'],
                                deriv['author'],
                                deriv['base_model']
                            ),
                            'model_type': deriv.get('model_type', 'other'),
                            'base_model': deriv['base_model'],
                            'data_source': 'model_tree',
                            'tags': str(deriv.get('tags', [])),
                            'likes': deriv.get('likes'),
                            'library_name': None,
                            'pipeline_tag': deriv.get('pipeline_tag'),
                            'created_at': deriv.get('created_at'),
                            'last_modified': deriv.get('last_modified'),
                            'fetched_at': date.today().isoformat(),
                            'base_model_from_api': deriv['base_model'],
                            'search_keyword': deriv['base_model']
                        }

                        all_models.append(record)
                        print(f"    âœ“ {deriv['name_zh']}: {model_id}")
                else:
                    print(f"  âšªï¸ æ²¡æœ‰æ‰¾åˆ°è¡ç”Ÿæ¨¡å‹")

            except Exception as e:
                print(f"  âŒ å¤„ç† {base_model} æ—¶å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
                continue

    finally:
        driver.quit()

    # è½¬æ¢ä¸º DataFrame
    if all_models:
        df = pd.DataFrame(all_models)
        print(f"\n{'=' * 80}")
        print(f"âœ… æˆåŠŸè·å– {len(df)} ä¸ªè¡ç”Ÿæ¨¡å‹")
        print(f"{'=' * 80}")
        return df, len(all_models)
    else:
        print(f"\n{'=' * 80}")
        print(f"âš ï¸ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è¡ç”Ÿæ¨¡å‹")
        print(f"{'=' * 80}")
        return pd.DataFrame(), 0


def update_modelscope_model_tree(
    save_to_db: bool = True,
    base_models: List[str] = None,
    auto_discover: bool = True,
    progress_callback=None
) -> Tuple[pd.DataFrame, int]:
    """
    æ›´æ–° ModelScope Model Tree æ•°æ®ï¼ˆåŒ…å«å»é‡å¤„ç†ï¼‰

    Args:
        save_to_db: æ˜¯å¦ä¿å­˜åˆ°æ•°æ®åº“
        base_models: åŸºç¡€æ¨¡å‹IDåˆ—è¡¨ï¼ˆå¦‚æœä¸ºNoneä¸”auto_discover=Trueï¼Œåˆ™è‡ªåŠ¨ä»æ•°æ®åº“å‘ç°ï¼‰
        auto_discover: æ˜¯å¦è‡ªåŠ¨ä»æ•°æ®åº“ä¸­å‘ç°æ‰€æœ‰ModelScopeå®˜æ–¹æ¨¡å‹
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°

    Returns:
        Tuple[DataFrame, int]: (æ›´æ–°çš„æ•°æ®, æ€»æ•°é‡)
    """
    print("\nğŸ”„ å¼€å§‹æ›´æ–° ModelScope Model Tree æ•°æ®...")

    # è·å–è¡ç”Ÿæ¨¡å‹ï¼ˆè‡ªåŠ¨å‘ç°æ‰€æœ‰å®˜æ–¹æ¨¡å‹ï¼‰
    df, total_count = get_all_modelscope_derivatives(
        base_models=base_models,
        auto_discover=auto_discover,
        progress_callback=progress_callback
    )

    if df.empty:
        print("âš ï¸ æ²¡æœ‰è·å–åˆ°ä»»ä½•è¡ç”Ÿæ¨¡å‹æ•°æ®")
        return df, 0

    # å»é‡å¤„ç†ï¼šæ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦å·²å­˜åœ¨ç›¸åŒçš„æ¨¡å‹
    if save_to_db:
        try:
            import sqlite3
            from ..db import load_data_from_db, save_to_db as save_to_db_func

            # è·å–ç°æœ‰ ModelScope æ•°æ®
            conn = sqlite3.connect(DB_PATH)
            existing_query = """
                SELECT DISTINCT publisher, model_name
                FROM model_downloads
                WHERE repo = 'ModelScope'
            """
            existing_df = pd.read_sql_query(existing_query, conn)
            conn.close()

            if not existing_df.empty:
                # åˆ›å»ºå·²å­˜åœ¨æ¨¡å‹çš„é›†åˆ
                existing_models = set(
                    f"{row['publisher']}/{row['model_name']}"
                    for _, row in existing_df.iterrows()
                )

                # è¿‡æ»¤æ‰å·²å­˜åœ¨çš„æ¨¡å‹
                df['model_key'] = df['publisher'] + '/' + df['model_name']
                new_df = df[~df['model_key'].isin(existing_models)].copy()
                new_df = new_df.drop(columns=['model_key'])

                print(f"ğŸ“Š å»é‡å‰: {len(df)} æ¡ï¼Œå»é‡å: {len(new_df)} æ¡")
                print(f"ğŸ—‘ï¸  è¿‡æ»¤æ‰ {len(df) - len(new_df)} æ¡å·²å­˜åœ¨çš„è®°å½•")

                if new_df.empty:
                    print("âš ï¸ æ²¡æœ‰æ–°çš„æ¨¡å‹éœ€è¦ä¿å­˜")
                    return df, 0

                df = new_df

            # ä¿å­˜åˆ°æ•°æ®åº“
            save_to_db_func(df, DB_PATH)
            print(f"ğŸ’¾ å·²ä¿å­˜ {len(df)} æ¡æ–°è®°å½•åˆ°æ•°æ®åº“")

        except Exception as e:
            print(f"âŒ ä¿å­˜æ•°æ®æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()

    return df, total_count


if __name__ == "__main__":
    # æµ‹è¯•åŠŸèƒ½
    print("=== æµ‹è¯• Model Tree åŠŸèƒ½ ===")
    print("1. Hugging Face Model Tree")
    print("2. AI Studio Model Tree")
    print("3. ModelScope Model Tree (NEW)")
    print("4. å…¨éƒ¨æµ‹è¯•")
    print()

    choice = input("è¯·é€‰æ‹©æµ‹è¯•æ¨¡å¼ (1/2/3/4=å…¨éƒ¨, é»˜è®¤=4): ").strip()

    # æµ‹è¯•åˆ†ç±»åŠŸèƒ½
    test_cases = [
        ("ernie-4.5-8b", "baidu"),
        ("ernie-4.5-8b-finetuned", "user123"),
        ("paddleocr-vl", "PaddlePaddle"),
        ("ernie-3.0", "baidu"),
        ("some-other-model", "user")
    ]

    print("\nğŸ§ª æµ‹è¯•æ¨¡å‹åˆ†ç±»:")
    for model_name, publisher in test_cases:
        category = classify_model(model_name, publisher)
        print(f"  {model_name} -> {category}")

    # æµ‹è¯•Hugging Face Model Tree
    if choice in ['1', '4', '']:
        print("\nğŸŒ³ æµ‹è¯• Hugging Face Model Tree:")
        df, count = get_all_ernie_derivatives(include_paddleocr=True)
        print(f"æ€»å…±è·å–åˆ° {count} ä¸ªæ¨¡å‹")

        if not df.empty:
            print("\nå‰5ä¸ªæ¨¡å‹:")
            print(df[['model_name', 'publisher', 'download_count', 'model_category']].head())

    # æµ‹è¯•AI Studio Model Tree
    if choice in ['2', '4', '']:
        print("\nğŸŒ³ æµ‹è¯• AI Studio Model Tree (æµ‹è¯•æ¨¡å¼):")
        df, count = update_aistudio_model_tree(save_to_db=False, test_mode=True)
        print(f"æ€»å…±è·å–åˆ° {count} ä¸ªè¡ç”Ÿæ¨¡å‹")

        if not df.empty:
            print("\nå‰5ä¸ªè¡ç”Ÿæ¨¡å‹:")
            print(df[['model_name', 'publisher', 'download_count', 'model_type', 'base_model']].head())

    # æµ‹è¯• ModelScope Model Tree
    if choice in ['3', '4', '']:
        print("\nğŸŒ³ æµ‹è¯• ModelScope Model Tree:")
        df, count = update_modelscope_model_tree(
            save_to_db=False,
            base_models=['PaddlePaddle/PaddleOCR-VL']
        )
        print(f"æ€»å…±è·å–åˆ° {count} ä¸ªè¡ç”Ÿæ¨¡å‹")

        if not df.empty:
            print("\nå‰5ä¸ªè¡ç”Ÿæ¨¡å‹:")
            print(df[['model_name', 'publisher', 'download_count', 'model_type', 'base_model']].head())


# =============================================================================
# AI Studio Model Tree åŠŸèƒ½æ¨¡å—
# =============================================================================

def get_aistudio_official_models():
    """
    ä»æ•°æ®åº“è·å–æ‰€æœ‰AI Studioå®˜æ–¹æ¨¡å‹

    Returns:
        DataFrame: å®˜æ–¹æ¨¡å‹æ•°æ®ï¼ŒåŒ…å« model_name, publisher, url ç­‰å­—æ®µ
    """
    try:
        from ..db import load_data_from_db
        import sqlite3
        import pandas as pd

        conn = sqlite3.connect(DB_PATH)

        # è·å–AI Studioå¹³å°çš„æ•°æ®
        query = """
            SELECT DISTINCT model_name, publisher, url
            FROM model_downloads
            WHERE repo = 'AI Studio'
            AND (
                publisher IN ('ç™¾åº¦', 'baidu', 'Paddle', 'PaddlePaddle', 'yiyan', 'ä¸€è¨€')
                OR publisher LIKE '%ç™¾åº¦%'
                OR publisher LIKE '%baidu%'
                OR publisher LIKE '%Paddle%'
            )
            AND url IS NOT NULL
            AND url != ''
        """

        df = pd.read_sql_query(query, conn)
        conn.close()

        print(f"ğŸ“Š æ‰¾åˆ° {len(df)} ä¸ªAI Studioå®˜æ–¹æ¨¡å‹")
        return df

    except Exception as e:
        print(f"âŒ è·å–AI Studioå®˜æ–¹æ¨¡å‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def fetch_aistudio_model_tree(
    progress_callback=None,
    include_official_publishers=None,
    test_mode=False,
    save_to_db=False
):
    """
    è·å–AI Studioå®˜æ–¹æ¨¡å‹çš„Model Treeï¼ˆè¡ç”Ÿæ¨¡å‹ï¼‰

    Args:
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
        include_official_publishers: å®˜æ–¹å‘å¸ƒè€…åˆ—è¡¨ï¼ˆé»˜è®¤ä½¿ç”¨æ ‡å‡†åˆ—è¡¨ï¼‰
        test_mode: æµ‹è¯•æ¨¡å¼ï¼Œåªå¤„ç†ç¬¬ä¸€ä¸ªæ¨¡å‹
        save_to_db: æ˜¯å¦ä¿å­˜åˆ°æ•°æ®åº“

    Returns:
        tuple: (DataFrame, total_count) è¡ç”Ÿæ¨¡å‹æ•°æ®å’Œæ•°é‡
    """
    from ..utils import create_chrome_driver
    from ..config import SELENIUM_TIMEOUT, DB_PATH
    from ..fetchers.selenium import AIStudioFetcher
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import TimeoutException, NoSuchElementException
    import time
    import re
    import sqlite3

    print("\n" + "=" * 80)
    print("ğŸŒ³ å¼€å§‹è·å– AI Studio Model Tree")
    print("=" * 80)

    # è·å–å·²å­˜åœ¨çš„æ¨¡å‹é›†åˆï¼ˆç”¨äºè·³è¿‡URLè·å–ï¼‰
    existing_models_with_url = set()
    try:
        conn = sqlite3.connect(DB_PATH)
        existing_query = """
            SELECT DISTINCT publisher, model_name
            FROM model_downloads
            WHERE repo = 'AI Studio' AND url IS NOT NULL AND url != ''
        """
        existing_df = pd.read_sql_query(existing_query, conn)
        conn.close()

        if not existing_df.empty:
            existing_models_with_url = set(
                f"{row['publisher']}/{row['model_name']}"
                for _, row in existing_df.iterrows()
            )
            print(f"ğŸ“š æ•°æ®åº“ä¸­å·²æœ‰ {len(existing_models_with_url)} ä¸ªæ¨¡å‹å¸¦URL")
            print(f"âš¡ è¿™äº›æ¨¡å‹åœ¨åˆ—è¡¨é¡µå°†è·³è¿‡URLè·å–")
    except Exception as e:
        print(f"âš ï¸  æ— æ³•åŠ è½½å·²å­˜åœ¨æ¨¡å‹åˆ—è¡¨: {e}")
        print(f"ğŸ”„ å°†ä¸ºæ‰€æœ‰æ¨¡å‹è·å–URL")

    # è·å–å®˜æ–¹æ¨¡å‹åˆ—è¡¨
    official_models_df = get_aistudio_official_models()
    if official_models_df is None or official_models_df.empty:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°AI Studioå®˜æ–¹æ¨¡å‹")
        return pd.DataFrame(), 0

    # æµ‹è¯•æ¨¡å¼ï¼šåªå¤„ç†ç¬¬ä¸€ä¸ªæ¨¡å‹
    if test_mode:
        official_models_df = official_models_df.head(1)
        print(f"ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šåªå¤„ç†ç¬¬ä¸€ä¸ªæ¨¡å‹")

    # åˆ›å»ºAIStudioFetcherå®ä¾‹ä»¥å¤ç”¨_get_detailed_infoæ–¹æ³•
    fetcher = AIStudioFetcher(test_mode=test_mode, enable_detailed_log=False)

    driver = None
    all_derivative_models = []
    processed_count = 0
    total_count = len(official_models_df)
    skipped_url_count = 0  # ç»Ÿè®¡è·³è¿‡URLè·å–çš„æ¨¡å‹æ•°

    try:
        driver = create_chrome_driver()

        for idx, row in official_models_df.iterrows():
            base_model_name = row['model_name']
            base_url = row['url']

            print(f"\n{'=' * 80}")
            print(f"[{idx + 1}/{total_count}] å¤„ç†æ¨¡å‹: {base_model_name}")
            print(f"{'=' * 80}")

            # æ­¥éª¤1ï¼šè·å–è¡ç”Ÿç±»å‹åˆ—è¡¨
            print(f"è®¿é—®: {base_url}")
            driver.get(base_url)

            try:
                WebDriverWait(driver, SELENIUM_TIMEOUT).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
                time.sleep(2)

                # å…³é—­å¹¿å‘Šæ¨ªå¹…ï¼ˆæ¯ä¸ªæ¨¡å‹é¡µé¢å…³é—­ä¸€æ¬¡ï¼‰
                try:
                    close_button_selectors = [
                        "#main > div.a-s-6th-footer-banner-wrapper > a > span",
                        "div.a-s-6th-footer-banner-wrapper > a > span",
                        ".a-s-6th-footer-banner-wrapper a span",
                    ]

                    for selector in close_button_selectors:
                        try:
                            close_buttons = driver.find_elements(By.CSS_SELECTOR, selector)
                            if close_buttons:
                                close_buttons[0].click()
                                print(f"  âœ… å·²å…³é—­æ¨ªå¹…å¹¿å‘Š")
                                time.sleep(0.5)
                                break
                        except:
                            continue

                    # å¦‚æœæ‰¾ä¸åˆ°å…³é—­æŒ‰é’®ï¼Œä½¿ç”¨JavaScriptç§»é™¤
                    try:
                        driver.execute_script("""
                            var bannerWrapper = document.querySelector('div.a-s-6th-footer-banner-wrapper');
                            if (bannerWrapper) {
                                bannerWrapper.style.display = 'none';
                            }
                        """)
                    except:
                        pass

                except Exception as e:
                    # å…³é—­æ¨ªå¹…å¤±è´¥ä¸å½±å“ç»§ç»­æ‰§è¡Œ
                    pass

            except TimeoutException:
                print(f"âš ï¸  é¡µé¢åŠ è½½è¶…æ—¶ï¼Œè·³è¿‡")
                continue

            # æŸ¥æ‰¾æ¨¡å‹è¡€ç¼˜æ ‘å…ƒç´ 
            try:
                tree_items = driver.find_elements(
                    By.CSS_SELECTOR,
                    "div.model-lineage-tree-item-wrap.child-model"
                )

                if not tree_items:
                    print(f"  âšªï¸  æ²¡æœ‰æ‰¾åˆ°è¡ç”Ÿç±»å‹")
                    continue

                print(f"  âœ… æ‰¾åˆ° {len(tree_items)} ä¸ªè¡ç”Ÿç±»å‹")

                # æ­¥éª¤2ï¼šå…ˆæ”¶é›†æ‰€æœ‰è¡ç”Ÿç±»å‹çš„ä¿¡æ¯ï¼ˆé¿å…stale element referenceï¼‰
                tree_type_list = []
                for tree_item in tree_items:
                    try:
                        # æ£€æŸ¥æ˜¯å¦ä¸º"å½“å‰æ¨¡å‹"æ ‡è®°ï¼ˆè¯´æ˜å½“å‰æ¨¡å‹æœ¬èº«æ˜¯è¡ç”Ÿç‰ˆæœ¬ï¼Œä¸éœ€è¦çˆ¬å–ï¼‰
                        try:
                            opt_current_elements = tree_item.find_elements(By.CSS_SELECTOR, "div.opt-current")
                            if opt_current_elements:
                                # è¿™æ˜¯ä¸€ä¸ª"å½“å‰æ¨¡å‹"æ ‡è®°ï¼Œè·³è¿‡
                                try:
                                    skip_name_zh = tree_item.find_element(By.CSS_SELECTOR, "div.name-zh").text.strip()
                                    skip_name_en = tree_item.find_element(By.CSS_SELECTOR, "div.name-en").text.strip()
                                    print(f"  â­ï¸  è·³è¿‡ '{skip_name_zh} / {skip_name_en}'ï¼ˆå½“å‰æ¨¡å‹æœ¬èº«æ˜¯è¡ç”Ÿç‰ˆæœ¬ï¼‰")
                                except:
                                    print(f"  â­ï¸  è·³è¿‡ä¸€ä¸ªè¡ç”Ÿç±»å‹ï¼ˆå½“å‰æ¨¡å‹æœ¬èº«æ˜¯è¡ç”Ÿç‰ˆæœ¬ï¼‰")
                                continue
                        except:
                            pass

                        # æå–ç±»å‹ä¿¡æ¯
                        name_zh = tree_item.find_element(
                            By.CSS_SELECTOR, "div.name-zh"
                        ).text.strip()

                        name_en = tree_item.find_element(
                            By.CSS_SELECTOR, "div.name-en"
                        ).text.strip()

                        # æå–æ¨¡å‹æ•°é‡
                        count_text = tree_item.find_element(
                            By.CSS_SELECTOR, "div.opt-link"
                        ).text.strip()

                        count_match = re.search(r'(\d+)', count_text)
                        count = int(count_match.group(1)) if count_match else 0

                        # è·å–é“¾æ¥
                        link_element = tree_item.find_element(
                            By.CSS_SELECTOR, "a.model-lineage-tree-item"
                        )
                        link = link_element.get_attribute('href')

                        tree_type_list.append({
                            'name_zh': name_zh,
                            'name_en': name_en,
                            'count': count,
                            'link': link
                        })
                    except Exception as e:
                        print(f"  âš ï¸  æå–è¡ç”Ÿç±»å‹ä¿¡æ¯æ—¶å‡ºé”™: {e}")
                        continue

                # æ­¥éª¤3ï¼šå¯¹æ¯ä¸ªè¡ç”Ÿç±»å‹è·å–æ¨¡å‹åˆ—è¡¨
                for idx, tree_type in enumerate(tree_type_list):
                    try:
                        name_zh = tree_type['name_zh']
                        name_en = tree_type['name_en']
                        count = tree_type['count']
                        link = tree_type['link']

                        print(f"\n  ğŸ“‚ è¡ç”Ÿç±»å‹: {name_zh} / {name_en} ({count}ä¸ªæ¨¡å‹)")

                        if link.startswith('/'):
                            full_url = f"https://aistudio.baidu.com{link}"
                        else:
                            full_url = link

                        # è®¿é—®è¡ç”Ÿæ¨¡å‹åˆ—è¡¨é¡µ
                        driver.get(full_url)

                        try:
                            WebDriverWait(driver, SELENIUM_TIMEOUT).until(
                                EC.presence_of_element_located(
                                    (By.CSS_SELECTOR, "div.ai-model-list-wapper")
                                )
                            )
                            time.sleep(2)
                        except TimeoutException:
                            print(f"    âš ï¸  è¡ç”Ÿæ¨¡å‹åˆ—è¡¨é¡µåŠ è½½è¶…æ—¶")
                            continue

                        # æå–æ‰€æœ‰æ¨¡å‹å¡ç‰‡
                        cards = driver.find_elements(
                            By.CSS_SELECTOR,
                            "div.ai-model-list-wapper > div"
                        )

                        print(f"    âœ… æ‰¾åˆ° {len(cards)} ä¸ªæ¨¡å‹")

                        for card_idx, card in enumerate(cards):
                            try:
                                # è·å–æ¨¡å‹åç§°
                                full_model_name = card.find_element(
                                    By.CSS_SELECTOR,
                                    "div.ai-model-list-wapper-card-right-desc"
                                ).text.strip()

                                # è·å–å‘å¸ƒè€…
                                publisher = card.find_element(
                                    By.CSS_SELECTOR,
                                    "span.ai-model-list-wapper-card-right-detail-one-publisher"
                                ).text.strip()

                                # è·å–ä¸‹è½½é‡å’Œæ—¶é—´å­—æ®µ
                                detail_items = card.find_elements(
                                    By.CSS_SELECTOR,
                                    "div.ai-model-list-wapper-card-right-detail-one-item-tip"
                                )

                                usage_count = detail_items[0].find_element(
                                    By.CSS_SELECTOR,
                                    "span.ai-model-list-wapper-card-right-detail-one-like"
                                ).text.strip()

                                # ğŸ”§ æ–°å¢ï¼šè·å–æ›´æ–°æ—¶é—´ï¼ˆç¬¬3ä¸ªtipï¼‰
                                last_modified = None
                                if len(detail_items) >= 3:
                                    try:
                                        last_modified = detail_items[2].find_element(
                                            By.CSS_SELECTOR,
                                            "span.ai-model-list-wapper-card-right-detail-one-like"
                                        ).text.strip()
                                        print(f"      ğŸ“… æ›´æ–°æ—¶é—´: {last_modified}")
                                    except Exception as e:
                                        print(f"      âš ï¸ è·å–æ›´æ–°æ—¶é—´å¤±è´¥: {e}")

                                # å¤„ç†æ¨¡å‹åç§°
                                if full_model_name.startswith("PaddlePaddle/"):
                                    model_name = full_model_name[len("PaddlePaddle/"):]
                                else:
                                    model_name = full_model_name

                                # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²æœ‰URLï¼ˆåœ¨searché˜¶æ®µå·²è·å–è¿‡ï¼‰
                                model_key = f"{publisher}/{model_name}"
                                should_fetch_url = model_key not in existing_models_with_url

                                if not should_fetch_url:
                                    print(f"      â­ï¸  è·³è¿‡URLè·å–ï¼ˆå·²æœ‰URLï¼‰: {model_key}")
                                    skipped_url_count += 1
                                    model_url = None
                                else:
                                    # å¤ç”¨AIStudioFetcherçš„_get_detailed_infoæ–¹æ³•è·å–URL
                                    print(f"      ğŸ” è·å–URL: {model_key}")
                                    detailed_count, model_url = fetcher._get_detailed_info(
                                        driver, card, card_idx, list_usage_count=usage_count
                                    )
                                    if detailed_count:
                                        usage_count = detailed_count

                                # åˆ›å»ºè®°å½•
                                record = {
                                    'date': date.today().isoformat(),
                                    'repo': 'AI Studio',
                                    'model_name': model_name,
                                    'publisher': publisher,
                                    'download_count': usage_count,
                                    'model_category': classify_model(
                                        model_name,
                                        publisher,
                                        base_model_name
                                    ),
                                    'model_type': name_en.lower(),  # adapter, finetune, etc.
                                    'base_model': base_model_name,
                                    'data_source': 'model_tree',
                                    'search_keyword': base_model_name,
                                    'url': model_url,  # ä»searchæˆ–model treeè·å–çš„URL
                                    'last_modified': last_modified  # ğŸ”§ æ–°å¢ï¼šæ›´æ–°æ—¶é—´
                                }

                                all_derivative_models.append(record)

                            except Exception as e:
                                print(f"      âš ï¸  å¤„ç†æ¨¡å‹æ—¶å‡ºé”™: {e}")
                                continue

                        # è¿”å›åŸºç¡€æ¨¡å‹è¯¦æƒ…é¡µ
                        driver.back()
                        time.sleep(1)

                    except Exception as e:
                        print(f"  âš ï¸  å¤„ç†è¡ç”Ÿç±»å‹æ—¶å‡ºé”™: {e}")
                        continue

                processed_count += 1
                if progress_callback:
                    progress_callback(processed_count)

            except NoSuchElementException:
                print(f"  âšªï¸  æœªæ‰¾åˆ°æ¨¡å‹è¡€ç¼˜æ ‘å…ƒç´ ")
                continue

        # è½¬æ¢ä¸ºDataFrame
        if all_derivative_models:
            df = pd.DataFrame(all_derivative_models)
            print(f"\n{'=' * 80}")
            print(f"âœ… æˆåŠŸè·å– {len(df)} ä¸ªè¡ç”Ÿæ¨¡å‹")
            if skipped_url_count > 0:
                print(f"âš¡ è·³è¿‡äº† {skipped_url_count} ä¸ªå·²æœ‰URLçš„æ¨¡å‹")
            print(f"{'=' * 80}")

            # ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if save_to_db and not df.empty:
                try:
                    from ..db import save_to_db as save_to_db_func
                    save_to_db_func(df, DB_PATH)
                    print(f"ğŸ’¾ å·²ä¿å­˜ {len(df)} æ¡è®°å½•åˆ°æ•°æ®åº“")
                except Exception as e:
                    print(f"âš ï¸ ä¿å­˜åˆ°æ•°æ®åº“å¤±è´¥: {e}")

            return df, len(df)
        else:
            print(f"\n{'=' * 80}")
            print(f"âš ï¸  æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è¡ç”Ÿæ¨¡å‹")
            if skipped_url_count > 0:
                print(f"âš¡ è·³è¿‡äº† {skipped_url_count} ä¸ªå·²æœ‰URLçš„æ¨¡å‹")
            print(f"{'=' * 80}")
            return pd.DataFrame(), 0

    except Exception as e:
        print(f"\nâŒ è·å–AI Studio Model Treeå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return pd.DataFrame(), 0

    finally:
        if driver:
            driver.quit()


def update_aistudio_model_tree(save_to_db=True, test_mode=False):
    """
    æ›´æ–°AI Studio Model Treeæ•°æ®ï¼ˆåŒ…å«å»é‡å¤„ç†ï¼‰

    Args:
        save_to_db: æ˜¯å¦ä¿å­˜åˆ°æ•°æ®åº“
        test_mode: æµ‹è¯•æ¨¡å¼ï¼Œåªå¤„ç†ç¬¬ä¸€ä¸ªæ¨¡å‹

    Returns:
        tuple: (DataFrame, total_count) è¡ç”Ÿæ¨¡å‹æ•°æ®å’Œæ•°é‡
    """
    print("\nğŸ”„ å¼€å§‹æ›´æ–°AI Studio Model Treeæ•°æ®...")

    # è·å–è¡ç”Ÿæ¨¡å‹
    df, total_count = fetch_aistudio_model_tree(test_mode=test_mode)

    if df.empty:
        print("âš ï¸ æ²¡æœ‰è·å–åˆ°ä»»ä½•è¡ç”Ÿæ¨¡å‹æ•°æ®")
        return df, 0

    # å»é‡å¤„ç†ï¼šæ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦å·²å­˜åœ¨ç›¸åŒçš„æ¨¡å‹ï¼ˆæ ¹æ®publisherå’Œmodel_nameï¼‰
    if save_to_db:
        try:
            from ..db import load_data_from_db, save_to_db as save_to_db_func
            import sqlite3

            # è·å–ç°æœ‰AI Studioæ•°æ®
            conn = sqlite3.connect(DB_PATH)
            existing_query = """
                SELECT DISTINCT publisher, model_name
                FROM model_downloads
                WHERE repo = 'AI Studio'
            """
            existing_df = pd.read_sql_query(existing_query, conn)
            conn.close()

            if not existing_df.empty:
                # åˆ›å»ºå·²å­˜åœ¨æ¨¡å‹çš„é›†åˆ
                existing_models = set(
                    f"{row['publisher']}/{row['model_name']}"
                    for _, row in existing_df.iterrows()
                )

                # è¿‡æ»¤æ‰å·²å­˜åœ¨çš„æ¨¡å‹
                df['model_key'] = df['publisher'] + '/' + df['model_name']
                new_df = df[~df['model_key'].isin(existing_models)].copy()
                new_df = new_df.drop(columns=['model_key'])

                print(f"ğŸ“Š å»é‡å‰: {len(df)} æ¡ï¼Œå»é‡å: {len(new_df)} æ¡")
                print(f"ğŸ—‘ï¸  è¿‡æ»¤æ‰ {len(df) - len(new_df)} æ¡å·²å­˜åœ¨çš„è®°å½•")

                if new_df.empty:
                    print("âš ï¸ æ²¡æœ‰æ–°çš„æ¨¡å‹éœ€è¦ä¿å­˜")
                    return df, 0

                df = new_df

            # ä¿å­˜åˆ°æ•°æ®åº“
            save_to_db_func(df, DB_PATH)
            print(f"ğŸ’¾ å·²ä¿å­˜ {len(df)} æ¡æ–°è®°å½•åˆ°æ•°æ®åº“")

        except Exception as e:
            print(f"âŒ ä¿å­˜æ•°æ®æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()

    return df, total_count
