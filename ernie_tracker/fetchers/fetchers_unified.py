"""
ç»Ÿä¸€çš„æ•°æ®è·å–æ¨¡å— - ä¸€æ¬¡æœç´¢è·å–æ‰€æœ‰PaddlePaddleæ¨¡å‹
"""
import requests
from bs4 import BeautifulSoup
import time
import streamlit as st
import pandas as pd
import numpy as np
import sqlite3
import re
from datetime import date
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.action_chains import ActionChains
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from tqdm.notebook import tqdm

from ..config import SEARCH_QUERY, DB_PATH


# hugging face
from huggingface_hub import list_models, model_info

def fetch_hugging_face_data_unified(progress_callback=None, progress_total=None, use_model_tree: bool = True):
    """
    ç»Ÿä¸€è·å–Hugging Faceä¸Šçš„PaddlePaddleæ¨¡å‹ï¼ˆåŒ…å«model treeå’Œæœç´¢ï¼‰

    Args:
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
        progress_total: æ€»æ•°å‚è€ƒ
        use_model_tree: æ˜¯å¦ä½¿ç”¨model treeåŠŸèƒ½è·å–è¡ç”Ÿæ¨¡å‹

    Returns:
        tuple: (DataFrame, æ€»æ•°é‡)
    """
    print(f"ğŸ¤– å¼€å§‹è·å–Hugging Faceæ¨¡å‹ (Model Tree: {use_model_tree})")

    all_models = []
    processed_ids = set()

    if use_model_tree:
        # Model Tree æ¨¡å¼ï¼šè·å–ERNIE-4.5å’ŒPaddleOCR-VLçš„å®Œæ•´æ¨¡å‹
        print("ğŸŒ³ è·å–ERNIE-4.5å’ŒPaddleOCR-VLå®Œæ•´æ¨¡å‹æ ‘...")
        try:
            from .fetchers_modeltree import get_all_ernie_derivatives

            model_tree_df, tree_count = get_all_ernie_derivatives(include_paddleocr=True)

            if not model_tree_df.empty:
                # è¿‡æ»¤Hugging Faceå¹³å°æ•°æ®
                hf_tree_df = model_tree_df[model_tree_df['repo'] == 'Hugging Face'].copy()

                # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼ï¼ˆä¿ç•™æ‰€æœ‰é‡è¦å­—æ®µï¼‰
                if not hf_tree_df.empty:
                    # ç¡®ä¿åŒ…å«æ‰€æœ‰å¿…è¦çš„åˆ—
                    required_cols = ['date', 'repo', 'model_name', 'publisher', 'download_count']
                    optional_cols = ['model_type', 'model_category', 'tags', 'base_model', 'data_source']

                    # é€‰æ‹©å­˜åœ¨çš„åˆ—
                    cols_to_keep = [col for col in required_cols + optional_cols if col in hf_tree_df.columns]
                    tree_results = hf_tree_df[cols_to_keep].to_dict('records')

                    all_models.extend(tree_results)
                    print(f"âœ… Model Treeè·å–: {len(tree_results)} ä¸ªERNIE/PaddleOCRç›¸å…³æ¨¡å‹")
                else:
                    print("âš ï¸ Model Treeæœªæ‰¾åˆ°Hugging Faceæ•°æ®")
            else:
                print("âš ï¸ Model Treeæœªè·å–åˆ°ä»»ä½•æ•°æ®")

        except Exception as e:
            print(f"âš ï¸ Model Treeè·å–å¤±è´¥: {e}")
    else:
        # ä¼ ç»Ÿæ¨¡å¼ï¼šåªæœç´¢ERNIE-4.5å’ŒPaddleOCR-VLï¼Œä¸æŸ¥æ‰¾model tree
        print("ğŸ” æœç´¢ERNIE-4.5å’ŒPaddleOCR-VLæ¨¡å‹...")
        try:
            search_terms = ['ERNIE-4.5', 'PaddleOCR-VL']
            search_results = []

            for search_term in search_terms:
                try:
                    term_models = list(list_models(search=search_term, full=True, limit=500))
                    print(f"  ğŸ” æœç´¢ '{search_term}' æ‰¾åˆ° {len(term_models)} ä¸ªæ¨¡å‹")

                    for i, m in enumerate(term_models):
                        try:
                            info = model_info(m.id, expand=["downloadsAllTime"])

                            # ç›´æ¥è·å–ä¸‹è½½é‡å¹¶æ·»åŠ è°ƒè¯•ä¿¡æ¯
                            downloads = getattr(info, 'downloads_all_time', 0)

                            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
                            if i < 3:  # åªæ‰“å°å‰3ä¸ªæ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯
                                print(f"  è°ƒè¯• {m.id}:")
                                print(f"    - downloads_all_time: {downloads}")
                                print(f"    - downloads (fallback): {getattr(info, 'downloads', 'N/A')}")
                                print(f"    - infoå¯¹è±¡å±æ€§: {[attr for attr in dir(info) if 'download' in attr.lower()]}")

                            model_data = {
                                "date": date.today().isoformat(),
                                "repo": "Hugging Face",
                                "model_name": m.id.split("/", 1)[1] if "/" in m.id else m.id,
                                "publisher": m.id.split("/")[0],
                                "download_count": downloads,
                                # ä¼ ç»Ÿæœç´¢æ¨¡å¼ä¸åŒ…å« model tree ä¿¡æ¯
                                "model_type": None,
                                "model_category": None,
                                "tags": None,
                                "base_model": None,
                                "data_source": 'search',  # æ ‡è®°ä¸ºä¼ ç»Ÿæœç´¢æ¨¡å¼
                                "search_keyword": search_term  # è®°å½•æœç´¢å…³é”®è¯
                            }
                            search_results.append(model_data)

                            if progress_callback:
                                progress_callback(len(search_results), discovered_total=None)

                        except Exception as e:
                            print(f"è·å–æ¨¡å‹ {m.id} è¯¦æƒ…å¤±è´¥: {e}")

                except Exception as e:
                    print(f"æœç´¢ '{search_term}' å¤±è´¥: {e}")

            # æ·»åŠ æœç´¢ç»“æœåˆ°æ€»åˆ—è¡¨
            all_models.extend(search_results)
            print(f"âœ… ä¼ ç»Ÿæœç´¢è·å–: {len(search_results)} ä¸ªæ¨¡å‹")

        except Exception as e:
            print(f"âŒ ä¼ ç»Ÿæœç´¢å¤±è´¥: {e}")

    # 3. åˆå¹¶å’Œæœ€ç»ˆå¤„ç†
    if all_models:
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(all_models)

        # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
        df['download_count'] = pd.to_numeric(df['download_count'], errors='coerce').fillna(0).astype(int)

        # å»é‡ï¼ˆåŸºäºpublisherå’Œmodel_nameçš„ç»„åˆï¼‰
        initial_count = len(df)
        df = df.drop_duplicates(subset=['publisher', 'model_name'], keep='first')
        final_count = len(df)

        print(f"ğŸ“Š Hugging Faceæ€»è®¡è·å– {final_count} ä¸ªæ¨¡å‹ï¼ˆå»é‡å‰: {initial_count} ä¸ªï¼‰")

        # æŒ‰ä¸‹è½½é‡æ’åº
        df = df.sort_values('download_count', ascending=False).reset_index(drop=True)

        return df, final_count
    else:
        print("âš ï¸ ä¸¤ç§æ¨¡å¼å‡æœªè·å–åˆ°æ•°æ®")
        empty_df = pd.DataFrame(columns=["date", "repo", "model_name", "publisher", "download_count"])
        return empty_df, 0


# ModelScope
from modelscope.hub.api import HubApi

def get_modelscope_ids_unified():
    """è·å–ModelScopeä¸Šçš„æ‰€æœ‰ERNIE-4.5å’ŒPaddleOCR-VLæ¨¡å‹ID

    Returns:
        dict: {model_id: search_keyword} è®°å½•æ¯ä¸ªæ¨¡å‹é€šè¿‡å“ªä¸ªå…³é”®è¯æœç´¢åˆ°çš„
    """
    driver = create_chrome_driver(headless=False)
    wait = WebDriverWait(driver, 20)
    model_id_to_keyword = {}  # è®°å½•æ¯ä¸ªæ¨¡å‹IDå¯¹åº”çš„æœç´¢å…³é”®è¯

    # æœç´¢ ERNIE-4.5 å’Œ PaddleOCR-VL
    search_terms = ["ERNIE-4.5", "PaddleOCR-VL"]

    for search_term in search_terms:
        print(f"[ModelScope] æœç´¢ {search_term}...")
        page = 1

        while True:
            url = f"https://modelscope.cn/search?page={page}&search={search_term}&type=model"
            print(f"[ModelScope] çˆ¬å–é¡µé¢: {url}")
            driver.get(url)

            try:
                wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "#normal_tab_model .antd5-row a")))
            except TimeoutException:
                print(f"[ModelScope] {search_term} é¡µé¢åŠ è½½å¤±è´¥ï¼Œå·²åˆ°æœ€åä¸€é¡µ")
                break

            cards = driver.find_elements(By.CSS_SELECTOR, "#normal_tab_model .antd5-row a")
            if not cards:
                break

            for link in cards:
                href = link.get_attribute("href")
                if "/models/" in href:
                    model_id = href.split("/models/")[-1]
                    # å¦‚æœæ¨¡å‹IDå·²å­˜åœ¨ï¼Œä¿æŒç¬¬ä¸€ä¸ªæœç´¢è¯ï¼ˆERNIE-4.5ä¼˜å…ˆï¼‰
                    if model_id not in model_id_to_keyword:
                        model_id_to_keyword[model_id] = search_term

            page += 1

    driver.quit()
    return model_id_to_keyword

def fetch_modelscope_data_unified(progress_callback=None, progress_total=None):
    """ç»Ÿä¸€è·å–ModelScopeä¸Šçš„PaddlePaddleæ¨¡å‹"""
    from .fetchers_modeltree import classify_model
    today = date.today().isoformat()
    model_id_to_keyword = get_modelscope_ids_unified()  # è¿”å›å­—å…¸
    total_count = len(model_id_to_keyword)

    api = HubApi()
    records = []

    for i, (model_id, search_keyword) in enumerate(model_id_to_keyword.items(), start=1):
        try:
            info = api.get_model(model_id, revision="master")
            downloads = info.get("Downloads", 0)

            # ğŸ”§ æ–°å¢ï¼šè·å–æ—¶é—´å­—æ®µ
            from datetime import datetime
            created_at = None
            last_modified = None

            if "CreatedTime" in info and info["CreatedTime"]:
                try:
                    created_at = datetime.fromtimestamp(info["CreatedTime"]).strftime('%Y-%m-%d')
                except:
                    created_at = None

            if "LastUpdatedTime" in info and info["LastUpdatedTime"]:
                try:
                    last_modified = datetime.fromtimestamp(info["LastUpdatedTime"]).strftime('%Y-%m-%d')
                except:
                    last_modified = None

            # ğŸ”§ æ–°å¢ï¼šæå–æ¨¡å‹åˆ†ç±»ä¿¡æ¯
            # 1. BaseModel (base_model)
            base_model = None
            if "BaseModel" in info and info["BaseModel"]:
                if isinstance(info["BaseModel"], list) and len(info["BaseModel"]) > 0:
                    base_model = info["BaseModel"][0]
                elif isinstance(info["BaseModel"], str):
                    base_model = info["BaseModel"]

            # 2. BaseModelRelation (model_type)
            model_type = None
            if "BaseModelRelation" in info and info["BaseModelRelation"]:
                model_type = info["BaseModelRelation"].lower()
                # æ˜ å°„åˆ°æ ‡å‡†ç±»å‹åç§°
                type_mapping = {
                    'finetune': 'finetune',
                    'quantized': 'quantized',
                    'adapter': 'adapter',
                    'lora': 'lora',
                    'merge': 'merge'
                }
                if model_type not in type_mapping:
                    model_type = 'other' if model_type else None
            else:
                # å¦‚æœæ²¡æœ‰ BaseModelRelationï¼Œä½†ä¹Ÿæ²¡æœ‰ base_modelï¼Œåˆ™å¯èƒ½æ˜¯ original
                if not base_model:
                    model_type = 'original'

            # 3. model_category - ä½¿ç”¨ classify_model å‡½æ•°æ ¹æ®åç§°ã€å‘å¸ƒè€…å’Œ base_model æ¨æ–­
            publisher = model_id.split("/")[0] if "/" in model_id else 'Unknown'
            model_name = model_id.split("/", 1)[1] if "/" in model_id else model_id
            model_category = classify_model(model_name, publisher, base_model)

            records.append({
                "date": today,
                "repo": "ModelScope",
                "model_name": model_id.split("/", 1)[1] if "/" in model_id else model_id,
                "publisher": model_id.split("/")[0],
                "download_count": downloads,
                "search_keyword": search_keyword,
                "created_at": created_at,
                "last_modified": last_modified,
                "model_category": model_category,
                "model_type": model_type,
                "base_model": base_model,
                "base_model_from_api": base_model
            })
        except Exception as e:
            print(f"è·å– {model_id} å¤±è´¥: {e}")

        if progress_callback:
            progress_callback(i, discovered_total=total_count)

    df = pd.DataFrame(
        records,
        columns=["date", "repo", "model_name", "publisher", "download_count", "search_keyword",
                 "created_at", "last_modified", "model_category", "model_type", "base_model",
                 "base_model_from_api"]
    )
    df['download_count'] = pd.to_numeric(df['download_count'], errors='coerce').fillna(0).astype(int)
    return df, total_count


# AI Studio (ä½¿ç”¨ä¿®å¤åçš„seleniumç‰ˆæœ¬)
from ..utils import create_chrome_driver, is_simplified_count, extract_numbers

def fetch_aistudio_data_unified(progress_callback=None, progress_total=None):
    """ç»Ÿä¸€è·å–AI Studioä¸Šçš„PaddlePaddleæ¨¡å‹"""
    from .selenium import AIStudioFetcher

    fetcher = AIStudioFetcher()
    fetcher.name = "AI Studio"
    return fetcher.fetch(progress_callback=progress_callback, progress_total=progress_total)


# GitCode (åŒ…å«ERNIE-4.5å’ŒPaddleOCR-VL)
def fetch_gitcode_data_unified(progress_callback=None, progress_total=None):
    """ç»Ÿä¸€è·å–GitCodeä¸Šçš„PaddlePaddleæ¨¡å‹ï¼ˆåŒ…å«ERNIE-4.5å’ŒPaddleOCR-VLï¼‰"""
    today = date.today().isoformat()
    driver = create_chrome_driver(headless=False)
    wait = WebDriverWait(driver, 40)

    results = []

    # ERNIE-4.5 æ¨¡å‹åˆ—è¡¨
    ernie_model_links = [
        "https://ai.gitcode.com/paddlepaddle/ERNIE-4.5-VL-424B-A47B-Paddle",
        "https://ai.gitcode.com/paddlepaddle/ERNIE-4.5-VL-28B-A3B-PT",
        "https://ai.gitcode.com/paddlepaddle/ERNIE-4.5-VL-424B-A47B-PT",
        "https://ai.gitcode.com/paddlepaddle/ERNIE-4.5-21B-A3B-Base-Paddle",
        "https://ai.gitcode.com/paddlepaddle/ERNIE-4.5-21B-A3B-Paddle",
        "https://ai.gitcode.com/paddlepaddle/ERNIE-4.5-300B-A47B-PT",
        "https://ai.gitcode.com/paddlepaddle/ERNIE-4.5-21B-A3B-Base-PT",
        "https://ai.gitcode.com/paddlepaddle/ERNIE-4.5-21B-A3B-PT",
        "https://ai.gitcode.com/paddlepaddle/ERNIE-4.5-300B-A47B-FP8-Paddle",
        "https://ai.gitcode.com/paddlepaddle/ERNIE-4.5-300B-A47B-Paddle",
        "https://ai.gitcode.com/paddlepaddle/ERNIE-4.5-0.3B-PT",
        "https://ai.gitcode.com/paddlepaddle/ERNIE-4.5-0.3B-Base-PT",
        "https://ai.gitcode.com/paddlepaddle/ERNIE-4.5-300B-A47B-Base-Paddle",
        "https://ai.gitcode.com/paddlepaddle/ERNIE-4.5-0.3B-Base-Paddle",
        "https://ai.gitcode.com/paddlepaddle/ERNIE-4.5-0.3B-Paddle",
        "https://ai.gitcode.com/paddlepaddle/ERNIE-4.5-VL-424B-A47B-Base-PT",
        "https://ai.gitcode.com/paddlepaddle/ERNIE-4.5-VL-424B-A47B-Base-Paddle",
        "https://ai.gitcode.com/paddlepaddle/ERNIE-4.5-VL-28B-A3B-Base-PT",
        "https://ai.gitcode.com/paddlepaddle/ERNIE-4.5-VL-28B-A3B-Base-Paddle",
        "https://ai.gitcode.com/paddlepaddle/ERNIE-4.5-VL-28B-A3B-Paddle",
        "https://ai.gitcode.com/paddlepaddle/ERNIE-4.5-300B-A47B-Base-PT",
        "https://ai.gitcode.com/paddlepaddle/ERNIE-4.5-300B-A47B-W4A8C8-TP4-Paddle"
    ]

    # PaddleOCR-VL æ¨¡å‹
    paddleocr_model_links = [
        "https://ai.gitcode.com/paddlepaddle/PaddleOCR-VL"
    ]

    # åˆå¹¶æ‰€æœ‰æ¨¡å‹é“¾æ¥
    all_model_links = ernie_model_links + paddleocr_model_links
    total_count = len(all_model_links)

    for i, model_link in enumerate(all_model_links, start=1):
        try:
            driver.get(model_link)
            print(f"è®¿é—®é“¾æ¥: {model_link}")
            # ç­‰å¾… URL å˜åŒ–æˆ–é¡µé¢åŠ è½½å®Œæˆ
            try:
                wait.until(EC.url_changes(model_link))
            except TimeoutException:
                print(f"URL æœªå˜åŒ–ï¼Œå¯èƒ½æ²¡æœ‰é‡å®šå‘æˆ–é¡µé¢åŠ è½½ç¼“æ…¢: {driver.current_url}")
            wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
            time.sleep(1) # é¢å¤–ç­‰å¾…ï¼Œç¡®ä¿URLæ›´æ–°
            print(f"å½“å‰é¡µé¢URL (åŠ è½½å): {driver.current_url}")

            model_name_selector = "#repo-banner-box > div > div.repo-info.h-full.ai-hub > div > div:nth-child(1) > div > div > div.info-item.project-name > div.project-text > div > p > a > span"
            model_name = wait.until(
                EC.visibility_of_element_located((By.CSS_SELECTOR, model_name_selector))
            ).text.strip()

            # æ£€æŸ¥æ˜¯å¦é‡å®šå‘åˆ° /model-inference é¡µé¢
            if "/model-inference" in driver.current_url:
                print(f"æ£€æµ‹åˆ°é‡å®šå‘åˆ° /model-inference é¡µé¢: {driver.current_url}")
                try:
                    # ç‚¹å‡»â€œæ¨¡å‹ä»‹ç»â€æ ‡ç­¾å›åˆ°åŸå§‹é¡µé¢
                    model_intro_selector = "#repo-header-tab > div.nav-tabs-item.flex-1.w-\[100\%\].overflow-hidden > div > div.repo-header-tab-ul > a:nth-child(1) > div"
                    print(f"å°è¯•ç‚¹å‡»å…ƒç´ : {model_intro_selector}")
                    model_intro_element = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, model_intro_selector)))
                    model_intro_element.click()
                    print("å·²ç‚¹å‡»â€œæ¨¡å‹ä»‹ç»â€æ ‡ç­¾ï¼Œç­‰å¾…é¡µé¢åŠ è½½...")
                    # ç­‰å¾… URL å˜åŒ–å›åŸå§‹é“¾æ¥ï¼Œå¹¶ç­‰å¾…ä¸‹è½½é‡å…ƒç´ é‡æ–°å‡ºç°
                    wait.until(EC.url_contains(model_link.split('?')[0]))
                    wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id="app"]/div/div[2]/div[2]/div/div/div/div/div/div[2]/div[1]/div[1]/div/div[2]')))
                    time.sleep(3) # é¢å¤–ç­‰å¾…ï¼Œç¡®ä¿é¡µé¢ç¨³å®šå’Œå…ƒç´ å¯è§
                    print(f"ç‚¹å‡»åå½“å‰é¡µé¢URL: {driver.current_url}")
                except Exception as click_e:
                    print(f"ç‚¹å‡»â€œæ¨¡å‹ä»‹ç»â€æ ‡ç­¾æˆ–ç­‰å¾…é¡µé¢åŠ è½½å¤±è´¥: {click_e}")
                    downloads = "0" # å¦‚æœç‚¹å‡»å¤±è´¥ï¼Œåˆ™æ— æ³•è·å–ä¸‹è½½é‡
            
            downloads = "0" # é»˜è®¤å€¼
            if downloads == "0": # å¦‚æœä¹‹å‰ç‚¹å‡»å¤±è´¥å¯¼è‡´ downloads ä¸º "0"ï¼Œåˆ™ä¸å†å°è¯•è·å–
                max_retries = 3
                for attempt in range(max_retries):
                    try:
                        time.sleep(2) # Add a small delay to allow the page to stabilize
                        # å°è¯•åŸå§‹ XPath è·å–ä¸‹è½½é‡
                        downloads_xpath = '//*[@id="app"]/div/div[2]/div[2]/div/div/div/div/div/div[2]/div[1]/div[1]/div/div[2]'
                        print(f"å°è¯•è·å–ä¸‹è½½é‡å…ƒç´ : {downloads_xpath}")
                        downloads_element = wait.until(EC.presence_of_element_located((By.XPATH, downloads_xpath)))
                        last_val = ""
                        for _ in range(5):
                            # Re-locate the element in each iteration to avoid StaleElementReferenceException
                            downloads_element = wait.until(EC.presence_of_element_located((By.XPATH, downloads_xpath)))
                            val = downloads_element.text.strip().replace(',', '')
                            if val and val != last_val:
                                last_val = val
                                time.sleep(1)
                            else:
                                break
                        downloads = last_val
                        print(f"è·å–åˆ°ä¸‹è½½é‡: {downloads}")
                        break # If successful, break out of the retry loop
                    except (TimeoutException, NoSuchElementException, StaleElementReferenceException) as e:
                        print(f"åœ¨ {driver.current_url} é¡µé¢è·å–ä¸‹è½½é‡å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                        downloads = "0" # æœ€ç»ˆå¤±è´¥ï¼Œè®¾ä¸º0
                        if attempt == max_retries - 1: # If it's the last attempt, set downloads to "0"
                            downloads = "0"
                        else:
                            time.sleep(2) # Wait before retrying

            results.append({
                "date": today,
                "repo": "GitCode",
                "model_name": model_name,
                "publisher": "é£æ¡¨PaddlePaddle",
                "download_count": downloads
            })

        except Exception as e:
            print(f"è·å– {model_link} å¤±è´¥: {e}")

        if progress_callback:
            progress_callback(i, discovered_total=total_count)

    driver.quit()

    df = pd.DataFrame(
        results,
        columns=["date", "repo", "model_name", "publisher", "download_count"]
    )
    df['download_count'] = pd.to_numeric(df['download_count'], errors='coerce').fillna(0).astype(int)
    return df, total_count


# é²¸æ™º (CAICT)
def fetch_caict_data_unified(progress_callback=None, progress_total=None):
    """ç»Ÿä¸€è·å–é²¸æ™ºä¸Šçš„PaddlePaddleæ¨¡å‹"""
    from ..config import CAICT_MODEL_LINKS

    today = date.today().isoformat()
    driver = create_chrome_driver(headless=False)
    wait = WebDriverWait(driver, 40)

    model_links = CAICT_MODEL_LINKS
    results = []
    total_models = len(model_links)

    for idx, model_link in enumerate(model_links, start=1):
        print(f"[é²¸æ™º] æ­£åœ¨å¤„ç† {idx}/{total_models}ï¼š{model_link}")
        driver.get(model_link)

        try:
            model_name_selector = "#community-app > div > div:nth-child(2) > div.w-full.bg-\[\#FCFCFD\].pt-9.pb-\[60px\].xl\:px-10.md\:px-0.md\:pb-6.md\:h-auto > div > div.flex.flex-col.gap-\[16px\].flex-wrap.mb-\[8px\].text-lg.text-\[\#606266\].font-semibold.md\:px-5 > div > a"
            model_name = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, model_name_selector))).text.strip()

            downloads_selector = "#pane-summary > div > div.w-\[40\%\].sm\:w-\[100\%\].border-l.border-\[\#EBEEF5\].md\:border-l-0.md\:border-b.md\:w-full.md\:pl-0 > div > div.text-\[\#303133\].text-base.font-semibold.leading-6.mt-1.md\:pl-0"
            downloads_element = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, downloads_selector)))

            # å¢åŠ ç­‰å¾…å’Œé‡è¯•æœºåˆ¶ï¼Œç¡®ä¿ä¸‹è½½é‡åˆ·æ–°
            downloads = "0"
            for _ in range(5):  # æœ€å¤šç­‰å¾…5ç§’
                downloads = downloads_element.text.strip().replace(',', '')
                if downloads and downloads != "0":
                    break
                time.sleep(1)

            results.append({
                "date": today,
                "repo": "é²¸æ™º",
                "model_name": model_name,
                "publisher": "PaddlePaddle", # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿ publisher å§‹ç»ˆä¸º "PaddlePaddle" (ç»Ÿä¸€å¤§å°å†™)
                "download_count": downloads
            })

        except Exception as e:
            print(f"!! é”™è¯¯ï¼šå¤„ç† {model_link} æ—¶å¤±è´¥ï¼ŒåŸå› ï¼š{e}")
            continue
        if progress_callback:
            progress_callback(idx, discovered_total=total_models)

    driver.quit()
    df = pd.DataFrame(results)
    df['download_count'] = pd.to_numeric(df['download_count'], errors='coerce').fillna(0).astype(int)
    return df, total_models


# é­”ä¹ Modelers (ä½¿ç”¨ä¿®å¤åçš„seleniumç‰ˆæœ¬)
def fetch_modelers_data_unified(progress_callback=None, progress_total=None):
    """ç»Ÿä¸€è·å–é­”ä¹Modelersä¸Šçš„PaddlePaddleæ¨¡å‹"""
    from .selenium import ModelersFetcher

    fetcher = ModelersFetcher()
    fetcher.name = "é­”ä¹ Modelers"
    return fetcher.fetch(progress_callback=progress_callback, progress_total=progress_total)


# Gitee (ä½¿ç”¨ä¿®å¤åçš„seleniumç‰ˆæœ¬)
def fetch_gitee_data_unified(progress_callback=None, progress_total=None):
    """ç»Ÿä¸€è·å–Giteeä¸Šçš„PaddlePaddleæ¨¡å‹"""
    from .selenium import GiteeFetcher

    fetcher = GiteeFetcher()
    fetcher.name = "Gitee"
    return fetcher.fetch(progress_callback=progress_callback, progress_total=progress_total)


# ç»Ÿä¸€çš„å¹³å°æŠ“å–å™¨å­—å…¸
UNIFIED_PLATFORM_FETCHERS = {
    "Hugging Face": fetch_hugging_face_data_unified,
    "ModelScope": fetch_modelscope_data_unified,
    "AI Studio": fetch_aistudio_data_unified,
    "GitCode": fetch_gitcode_data_unified,
    "é²¸æ™º": fetch_caict_data_unified,
    "é­”ä¹ Modelers": fetch_modelers_data_unified,
    "Gitee": fetch_gitee_data_unified,
}


def fetch_all_paddlepaddle_data(platforms=None, progress_callback=None, progress_total=None, enable_model_tree=True):
    """
    ä¸€æ¬¡æ€§è·å–æ‰€æœ‰å¹³å°çš„PaddlePaddleæ¨¡å‹æ•°æ®ï¼ˆåŒ…å«ERNIE-4.5å’ŒPaddleOCR-VLï¼‰

    Args:
        platforms: è¦æŠ“å–çš„å¹³å°åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºæ‰€æœ‰å¹³å°
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
        progress_total: æ€»æ•°å‚è€ƒ
        enable_model_tree: æ˜¯å¦å¯ç”¨AI Studio Model TreeåŠŸèƒ½

    Returns:
        tuple: (DataFrame, æ€»æ•°é‡)
    """
    if platforms is None:
        platforms = list(UNIFIED_PLATFORM_FETCHERS.keys())

    all_dfs = []
    total_count = 0
    aistudio_included = "AI Studio" in platforms

    for platform in platforms:
        fetcher = UNIFIED_PLATFORM_FETCHERS.get(platform)
        if fetcher:
            try:
                df, count = fetcher(progress_callback=progress_callback, progress_total=progress_total)
                all_dfs.append(df)
                total_count += count
                print(f"âœ… {platform} å®Œæˆï¼šè·å– {count} ä¸ªæ¨¡å‹")
            except Exception as e:
                print(f"âŒ {platform} å¤±è´¥ï¼š{e}")
        else:
            print(f"âš ï¸ æ‰¾ä¸åˆ° {platform} çš„æŠ“å–å™¨")

    # æ³¨æ„ï¼šAI Studio Model Tree å·²ç§»è‡³ app.py ä¸­è°ƒç”¨ï¼Œé¿å…é‡å¤æ‰§è¡Œ
    # åŸå› ï¼šapp.py ä¸­éœ€è¦æ›´ç²¾ç»†çš„ UI è¿›åº¦æ§åˆ¶ï¼Œä¸”èƒ½é¿å…ä¸¤å±‚è°ƒç”¨å¯¼è‡´çš„é‡å¤æ‰§è¡Œ
    # å‚è€ƒï¼šapp.py çš„å¹¶è¡Œå’Œä¸²è¡Œæ‰§è¡Œæ¨¡å¼ä¸­çš„ Model Tree è°ƒç”¨

    if all_dfs:
        final_df = pd.concat(all_dfs, ignore_index=True)
        return final_df, total_count
    else:
        return pd.DataFrame(), 0
