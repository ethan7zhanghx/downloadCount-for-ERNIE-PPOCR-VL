"""
æ•°æ®åº“ç®¡ç†æ¨¡å— - æä¾›å¤‡ä»½ã€åˆ é™¤ã€æŸ¥çœ‹ç­‰ç®¡ç†åŠŸèƒ½
"""
import sqlite3
import pandas as pd
import shutil
import os
from datetime import datetime, date
from .config import DB_PATH, DATA_TABLE, STATS_TABLE


def backup_database(backup_dir="backups"):
    """
    å¤‡ä»½æ•°æ®åº“

    Args:
        backup_dir: å¤‡ä»½ç›®å½•

    Returns:
        tuple: (success, backup_path or error_message)
    """
    try:
        # åˆ›å»ºå¤‡ä»½ç›®å½•
        os.makedirs(backup_dir, exist_ok=True)

        # ç”Ÿæˆå¤‡ä»½æ–‡ä»¶åï¼ˆåŒ…å«æ—¶é—´æˆ³ï¼‰
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_filename = f"ernie_downloads_backup_{timestamp}.db"
        backup_path = os.path.join(backup_dir, backup_filename)

        # å¤åˆ¶æ•°æ®åº“æ–‡ä»¶
        shutil.copy2(DB_PATH, backup_path)

        return True, backup_path

    except Exception as e:
        return False, str(e)


def restore_database(backup_path):
    """
    ä»å¤‡ä»½æ¢å¤æ•°æ®åº“

    Args:
        backup_path: å¤‡ä»½æ–‡ä»¶è·¯å¾„

    Returns:
        tuple: (success, message)
    """
    try:
        if not os.path.exists(backup_path):
            return False, f"å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: {backup_path}"

        # å…ˆå¤‡ä»½å½“å‰æ•°æ®åº“
        success, current_backup = backup_database()
        if not success:
            return False, f"æ— æ³•å¤‡ä»½å½“å‰æ•°æ®åº“: {current_backup}"

        # æ¢å¤å¤‡ä»½
        shutil.copy2(backup_path, DB_PATH)

        return True, f"æ•°æ®åº“å·²æ¢å¤ï¼Œå½“å‰æ•°æ®åº“å·²å¤‡ä»½åˆ°: {current_backup}"

    except Exception as e:
        return False, str(e)


def delete_data_by_date(target_date):
    """
    åˆ é™¤æŒ‡å®šæ—¥æœŸçš„æ•°æ®

    Args:
        target_date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸º 'YYYY-MM-DD'

    Returns:
        tuple: (success, message, deleted_count)
    """
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()

        # å…ˆæŸ¥è¯¢è¦åˆ é™¤çš„è®°å½•æ•°
        cursor.execute(f"SELECT COUNT(*) FROM {DATA_TABLE} WHERE date = ?", (target_date,))
        count = cursor.fetchone()[0]

        if count == 0:
            conn.close()
            return True, f"æ—¥æœŸ {target_date} æ²¡æœ‰æ•°æ®", 0

        # åˆ é™¤æ•°æ®
        cursor.execute(f"DELETE FROM {DATA_TABLE} WHERE date = ?", (target_date,))
        conn.commit()
        conn.close()

        return True, f"æˆåŠŸåˆ é™¤ {count} æ¡è®°å½•", count

    except Exception as e:
        return False, str(e), 0


def delete_data_by_platform(platform, target_date=None):
    """
    åˆ é™¤æŒ‡å®šå¹³å°çš„æ•°æ®

    Args:
        platform: å¹³å°åç§°
        target_date: å¯é€‰çš„æ—¥æœŸè¿‡æ»¤

    Returns:
        tuple: (success, message, deleted_count)
    """
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()

        # æ„å»ºæŸ¥è¯¢
        if target_date:
            query = f"SELECT COUNT(*) FROM {DATA_TABLE} WHERE repo = ? AND date = ?"
            params = (platform, target_date)
        else:
            query = f"SELECT COUNT(*) FROM {DATA_TABLE} WHERE repo = ?"
            params = (platform,)

        cursor.execute(query, params)
        count = cursor.fetchone()[0]

        if count == 0:
            conn.close()
            return True, f"å¹³å° {platform} æ²¡æœ‰æ•°æ®", 0

        # åˆ é™¤æ•°æ®
        if target_date:
            cursor.execute(f"DELETE FROM {DATA_TABLE} WHERE repo = ? AND date = ?", params)
        else:
            cursor.execute(f"DELETE FROM {DATA_TABLE} WHERE repo = ?", params)

        conn.commit()
        conn.close()

        return True, f"æˆåŠŸåˆ é™¤ {count} æ¡è®°å½•", count

    except Exception as e:
        return False, str(e), 0


def get_database_stats():
    """
    è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯

    Returns:
        dict: ç»Ÿè®¡ä¿¡æ¯
    """
    try:
        conn = sqlite3.connect(DB_PATH)

        # æ€»è®°å½•æ•°
        total_records = pd.read_sql(f"SELECT COUNT(*) as count FROM {DATA_TABLE}", conn).iloc[0]['count']

        # æŒ‰æ—¥æœŸç»Ÿè®¡
        date_stats = pd.read_sql(
            f"SELECT date, COUNT(*) as count FROM {DATA_TABLE} GROUP BY date ORDER BY date DESC",
            conn
        )

        # æŒ‰å¹³å°ç»Ÿè®¡
        platform_stats = pd.read_sql(
            f"SELECT repo, COUNT(*) as count FROM {DATA_TABLE} GROUP BY repo ORDER BY count DESC",
            conn
        )

        # æ•°æ®åº“æ–‡ä»¶å¤§å°
        db_size = os.path.getsize(DB_PATH) if os.path.exists(DB_PATH) else 0
        db_size_mb = db_size / (1024 * 1024)

        # æ—¥æœŸèŒƒå›´
        date_range = pd.read_sql(
            f"SELECT MIN(date) as min_date, MAX(date) as max_date FROM {DATA_TABLE}",
            conn
        )

        conn.close()

        return {
            'total_records': int(total_records),
            'date_stats': date_stats,
            'platform_stats': platform_stats,
            'db_size_mb': round(db_size_mb, 2),
            'min_date': date_range.iloc[0]['min_date'],
            'max_date': date_range.iloc[0]['max_date']
        }

    except Exception as e:
        return {
            'error': str(e),
            'total_records': 0,
            'date_stats': pd.DataFrame(),
            'platform_stats': pd.DataFrame(),
            'db_size_mb': 0,
            'min_date': None,
            'max_date': None
        }


def get_available_backups(backup_dir="backups"):
    """
    è·å–æ‰€æœ‰å¯ç”¨çš„å¤‡ä»½æ–‡ä»¶

    Args:
        backup_dir: å¤‡ä»½ç›®å½•

    Returns:
        list: å¤‡ä»½æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨
    """
    try:
        if not os.path.exists(backup_dir):
            return []

        backups = []
        for filename in os.listdir(backup_dir):
            if filename.endswith('.db') and filename.startswith('ernie_downloads_backup_'):
                filepath = os.path.join(backup_dir, filename)
                file_size = os.path.getsize(filepath) / (1024 * 1024)  # MB
                file_time = datetime.fromtimestamp(os.path.getmtime(filepath))

                backups.append({
                    'filename': filename,
                    'filepath': filepath,
                    'size_mb': round(file_size, 2),
                    'created_time': file_time.strftime("%Y-%m-%d %H:%M:%S")
                })

        # æŒ‰åˆ›å»ºæ—¶é—´å€’åºæ’åˆ—
        backups.sort(key=lambda x: x['created_time'], reverse=True)

        return backups

    except Exception as e:
        print(f"è·å–å¤‡ä»½åˆ—è¡¨å¤±è´¥: {e}")
        return []


def delete_backup(backup_path):
    """
    åˆ é™¤å¤‡ä»½æ–‡ä»¶

    Args:
        backup_path: å¤‡ä»½æ–‡ä»¶è·¯å¾„

    Returns:
        tuple: (success, message)
    """
    try:
        if os.path.exists(backup_path):
            os.remove(backup_path)
            return True, "å¤‡ä»½æ–‡ä»¶å·²åˆ é™¤"
        else:
            return False, "å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨"

    except Exception as e:
        return False, str(e)


def vacuum_database():
    """
    æ¸…ç†æ•°æ®åº“ï¼Œå›æ”¶ç©ºé—´

    Returns:
        tuple: (success, message)
    """
    try:
        conn = sqlite3.connect(DB_PATH)

        # è®°å½•æ¸…ç†å‰çš„å¤§å°
        before_size = os.path.getsize(DB_PATH) / (1024 * 1024)

        # æ‰§è¡Œ VACUUM
        conn.execute("VACUUM")
        conn.close()

        # è®°å½•æ¸…ç†åçš„å¤§å°
        after_size = os.path.getsize(DB_PATH) / (1024 * 1024)
        saved = before_size - after_size

        return True, f"æ•°æ®åº“å·²ä¼˜åŒ–ï¼ŒèŠ‚çœäº† {saved:.2f} MB ç©ºé—´"

    except Exception as e:
        return False, str(e)


def export_database_to_excel(output_path, date_filter=None):
    """
    å¯¼å‡ºæ•°æ®åº“åˆ° Excelï¼ˆè‡ªåŠ¨å»é‡ï¼Œå–æœ€å¤§ä¸‹è½½é‡ï¼‰

    ç­–ç•¥ï¼š
    - å¯¹äºç›¸åŒçš„ (date, repo, publisher, model_name)ï¼Œå– download_count æœ€å¤§çš„è®°å½•
    - ä¸ load_data_from_db() ä½¿ç”¨ç›¸åŒçš„å»é‡é€»è¾‘

    Args:
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        date_filter: å¯é€‰çš„æ—¥æœŸè¿‡æ»¤

    Returns:
        tuple: (success, message)
    """
    try:
        conn = sqlite3.connect(DB_PATH)

        # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ä¸ load_data_from_db() ç›¸åŒçš„å»é‡é€»è¾‘
        # æŒ‰ (date, repo, publisher, model_name) åˆ†ç»„ï¼Œå–æœ€å¤§ä¸‹è½½é‡
        if date_filter:
            query = f"""
                SELECT t1.*
                FROM {DATA_TABLE} t1
                INNER JOIN (
                    SELECT
                        date, repo, publisher, model_name,
                        MAX(CAST(download_count AS REAL)) as max_download
                    FROM {DATA_TABLE}
                    WHERE date = ?
                    GROUP BY date, repo, publisher, model_name
                ) t2
                ON t1.date = t2.date
                   AND t1.repo = t2.repo
                   AND t1.publisher = t2.publisher
                   AND t1.model_name = t2.model_name
                   AND CAST(t1.download_count AS REAL) = t2.max_download
                ORDER BY repo, model_name
            """
            df = pd.read_sql(query, conn, params=(date_filter,))
        else:
            query = f"""
                SELECT t1.*
                FROM {DATA_TABLE} t1
                INNER JOIN (
                    SELECT
                        date, repo, publisher, model_name,
                        MAX(CAST(download_count AS REAL)) as max_download
                    FROM {DATA_TABLE}
                    GROUP BY date, repo, publisher, model_name
                ) t2
                ON t1.date = t2.date
                   AND t1.repo = t2.repo
                   AND t1.publisher = t2.publisher
                   AND t1.model_name = t2.model_name
                   AND CAST(t1.download_count AS REAL) = t2.max_download
                ORDER BY date DESC, repo, model_name
            """
            df = pd.read_sql(query, conn)

        conn.close()

        if df.empty:
            return False, "æ²¡æœ‰æ•°æ®å¯å¯¼å‡º"

        # é‡æ–°æ’åˆ—åˆ—é¡ºåºï¼ŒæŠŠé‡è¦çš„model treeä¿¡æ¯æ”¾åœ¨å‰é¢
        # ç¡®ä¿è¿™äº›åˆ—å­˜åœ¨
        base_columns = ['date', 'repo', 'model_name', 'publisher', 'download_count']

        # Model Tree ç›¸å…³åˆ—
        model_tree_columns = []
        if 'base_model' in df.columns:
            model_tree_columns.append('base_model')
        if 'model_type' in df.columns:
            model_tree_columns.append('model_type')
        if 'model_category' in df.columns:
            model_tree_columns.append('model_category')

        # å…¶ä»–åˆ—
        other_columns = [col for col in df.columns if col not in base_columns + model_tree_columns]

        # é‡æ–°æ’åˆ—åˆ—é¡ºåºï¼šåŸºç¡€åˆ— -> Model Treeåˆ— -> å…¶ä»–åˆ—
        ordered_columns = base_columns + model_tree_columns + other_columns
        ordered_columns = [col for col in ordered_columns if col in df.columns]

        df = df[ordered_columns]

        # æ·»åŠ ä¸€ä¸ªæ›´ç›´è§‚çš„"æ¥æº"åˆ—ï¼ˆä¸­æ–‡æè¿°ï¼‰ï¼Œæ–¹ä¾¿è¯†åˆ«
        if 'data_source' in df.columns:
            # æ•°æ®åº“å·²æœ‰ data_source å­—æ®µï¼Œæ·»åŠ ä¸­æ–‡æè¿°åˆ—
            def get_source_cn(row):
                source = row.get('data_source')
                if pd.notna(source):
                    if source == 'search':
                        return 'æœç´¢å‘ç°'
                    elif source == 'model_tree':
                        return f'Model Tree (è¡ç”Ÿè‡ª {row.get("base_model", "æœªçŸ¥")})'
                    elif source == 'both':
                        return f'æœç´¢+Model Tree (è¡ç”Ÿè‡ª {row.get("base_model", "æœªçŸ¥")})'
                    else:
                        return source
                else:
                    # å†å²æ•°æ®å…¼å®¹ï¼šæ ¹æ® base_model æ¨æ–­
                    if pd.notna(row.get('base_model')) and row.get('base_model'):
                        return f'Model Tree (è¡ç”Ÿè‡ª {row["base_model"]})'
                    else:
                        return 'ç›´æ¥æœç´¢ (å†å²æ•°æ®)'

            df['data_source_cn'] = df.apply(get_source_cn, axis=1)
            # æŠŠä¸­æ–‡åˆ—æ”¾åœ¨ data_source åˆ—åé¢
            if 'data_source' in df.columns:
                source_col_idx = df.columns.get_loc('data_source')
                cols = df.columns.tolist()
                cols.insert(source_col_idx + 1, cols.pop(cols.index('data_source_cn')))
                df = df[cols]
        elif 'base_model' in df.columns:
            # å…¼å®¹è€ç‰ˆæœ¬ï¼šæ²¡æœ‰ data_source å­—æ®µï¼Œæ ¹æ® base_model åˆ›å»º
            def get_source_legacy(row):
                if pd.notna(row.get('base_model')) and row.get('base_model'):
                    return f"Model Tree (è¡ç”Ÿè‡ª {row['base_model']})"
                else:
                    return "ç›´æ¥æœç´¢"

            df.insert(5, 'data_source_cn', df.apply(get_source_legacy, axis=1))

        # ç¿»è¯‘ model_type å’Œ model_category ä¸ºæ›´æ˜“è¯»çš„ä¸­æ–‡
        if 'model_type' in df.columns:
            type_mapping = {
                'original': 'åŸå§‹æ¨¡å‹',
                'finetune': 'Finetuneå¾®è°ƒ',
                'adapter': 'Adapteré€‚é…å™¨',
                'lora': 'LoRA',
                'other': 'å…¶ä»–'
            }
            df['model_type_cn'] = df['model_type'].map(type_mapping).fillna(df['model_type'])
            # æŠŠä¸­æ–‡åˆ—æ”¾åœ¨è‹±æ–‡åˆ—åé¢
            type_col_idx = df.columns.get_loc('model_type')
            cols = df.columns.tolist()
            cols.insert(type_col_idx + 1, cols.pop(cols.index('model_type_cn')))
            df = df[cols]

        if 'model_category' in df.columns:
            category_mapping = {
                'ernie-4.5': 'ERNIE-4.5ç³»åˆ—',
                'paddleocr-vl': 'PaddleOCR-VLç³»åˆ—',
                'other-ernie': 'å…¶ä»–ERNIEç³»åˆ—',
                'other': 'å…¶ä»–'
            }
            df['model_category_cn'] = df['model_category'].map(category_mapping).fillna(df['model_category'])
            # æŠŠä¸­æ–‡åˆ—æ”¾åœ¨è‹±æ–‡åˆ—åé¢
            category_col_idx = df.columns.get_loc('model_category')
            cols = df.columns.tolist()
            cols.insert(category_col_idx + 1, cols.pop(cols.index('model_category_cn')))
            df = df[cols]

        # å¯¼å‡ºåˆ° Excel
        df.to_excel(output_path, index=False, engine='openpyxl')

        return True, f"æˆåŠŸå¯¼å‡º {len(df)} æ¡è®°å½•åˆ° {output_path}"

    except Exception as e:
        return False, str(e)


def get_duplicate_records():
    """
    æŸ¥æ‰¾é‡å¤è®°å½•

    Returns:
        DataFrame: é‡å¤è®°å½•
    """
    try:
        conn = sqlite3.connect(DB_PATH)

        # æŸ¥æ‰¾é‡å¤çš„è®°å½•ï¼ˆç›¸åŒçš„æ—¥æœŸã€å¹³å°ã€å‘å¸ƒè€…ã€æ¨¡å‹åç§°ï¼‰
        query = f"""
        SELECT date, repo, publisher, model_name, COUNT(*) as count
        FROM {DATA_TABLE}
        GROUP BY date, repo, publisher, model_name
        HAVING count > 1
        ORDER BY count DESC, date DESC
        """

        duplicates = pd.read_sql(query, conn)
        conn.close()

        return duplicates

    except Exception as e:
        print(f"æŸ¥æ‰¾é‡å¤è®°å½•å¤±è´¥: {e}")
        return pd.DataFrame()


def remove_duplicate_records():
    """
    åˆ é™¤é‡å¤è®°å½•ï¼Œä¿ç•™æœ€æ–°çš„ä¸€æ¡

    Returns:
        tuple: (success, message, deleted_count)
    """
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()

        # åˆ é™¤é‡å¤è®°å½•ï¼Œä¿ç•™ rowid æœ€å¤§çš„ï¼ˆæœ€æ–°çš„ï¼‰
        cursor.execute(f"""
        DELETE FROM {DATA_TABLE}
        WHERE rowid NOT IN (
            SELECT MAX(rowid)
            FROM {DATA_TABLE}
            GROUP BY date, repo, publisher, model_name
        )
        """)

        deleted_count = cursor.rowcount
        conn.commit()
        conn.close()

        return True, f"æˆåŠŸåˆ é™¤ {deleted_count} æ¡é‡å¤è®°å½•", deleted_count

    except Exception as e:
        return False, str(e), 0


def insert_single_record(date, repo, model_name, publisher, download_count,
                        base_model=None, model_type=None, model_category=None):
    """
    æ’å…¥å•æ¡è®°å½•åˆ°æ•°æ®åº“

    Args:
        date: æ—¥æœŸ (YYYY-MM-DD)
        repo: å¹³å°åç§°
        model_name: æ¨¡å‹åç§°
        publisher: å‘å¸ƒè€…
        download_count: ä¸‹è½½é‡
        base_model: åŸºç¡€æ¨¡å‹ï¼ˆå¯é€‰ï¼Œç”¨äºè¡ç”Ÿæ¨¡å‹ï¼‰
        model_type: æ¨¡å‹ç±»å‹ï¼ˆå¯é€‰ï¼šoriginal, finetune, adapter, lora, otherï¼‰
        model_category: æ¨¡å‹åˆ†ç±»ï¼ˆå¯é€‰ï¼šernie-4.5, paddleocr-vl, other-ernie, otherï¼‰

    Returns:
        tuple: (success, message)
    """
    try:
        # éªŒè¯å¿…å¡«å­—æ®µ
        if not all([date, repo, model_name, publisher]):
            return False, "æ—¥æœŸã€å¹³å°ã€æ¨¡å‹åç§°ã€å‘å¸ƒè€…ä¸èƒ½ä¸ºç©º"

        # éªŒè¯ä¸‹è½½é‡æ˜¯å¦ä¸ºæ•°å­—
        try:
            download_count = int(download_count)
            if download_count < 0:
                return False, "ä¸‹è½½é‡ä¸èƒ½ä¸ºè´Ÿæ•°"
        except (ValueError, TypeError):
            return False, "ä¸‹è½½é‡å¿…é¡»æ˜¯æ•°å­—"

        # éªŒè¯æ—¥æœŸæ ¼å¼
        try:
            datetime.strptime(date, '%Y-%m-%d')
        except ValueError:
            return False, "æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œåº”ä¸º YYYY-MM-DD"

        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒè®°å½•
        cursor.execute(f"""
            SELECT COUNT(*) FROM {DATA_TABLE}
            WHERE date = ? AND repo = ? AND publisher = ? AND model_name = ?
        """, (date, repo, publisher, model_name))

        if cursor.fetchone()[0] > 0:
            conn.close()
            return False, f"è¯¥è®°å½•å·²å­˜åœ¨ï¼ˆæ—¥æœŸ: {date}, å¹³å°: {repo}, æ¨¡å‹: {model_name}, å‘å¸ƒè€…: {publisher}ï¼‰"

        # æ’å…¥è®°å½•
        cursor.execute(f"""
            INSERT INTO {DATA_TABLE}
            (date, repo, model_name, publisher, download_count, base_model, model_type, model_category)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        """, (date, repo, model_name, publisher, download_count, base_model, model_type, model_category))

        conn.commit()
        conn.close()

        return True, f"æˆåŠŸæ’å…¥è®°å½•ï¼š{model_name} ({publisher}) - {download_count:,} æ¬¡ä¸‹è½½"

    except Exception as e:
        return False, f"æ’å…¥å¤±è´¥: {str(e)}"


def import_from_excel(file_path, skip_duplicates=True):
    """
    ä» Excel æ–‡ä»¶æ‰¹é‡å¯¼å…¥æ•°æ®

    Args:
        file_path: Excel æ–‡ä»¶è·¯å¾„æˆ–æ–‡ä»¶å¯¹è±¡
        skip_duplicates: æ˜¯å¦è·³è¿‡é‡å¤è®°å½•ï¼ˆTrueï¼‰æˆ–è¦†ç›–ï¼ˆFalseï¼‰

    Returns:
        tuple: (success, message, stats_dict)
        stats_dict åŒ…å«: total, inserted, skipped, errors
    """
    try:
        # è¯»å– Excel æ–‡ä»¶
        if isinstance(file_path, str):
            df = pd.read_excel(file_path, engine='openpyxl')
        else:
            # æ”¯æŒ BytesIO å¯¹è±¡ï¼ˆç”¨äº Streamlit ä¸Šä¼ ï¼‰
            df = pd.read_excel(file_path, engine='openpyxl')

        if df.empty:
            return False, "Excel æ–‡ä»¶ä¸ºç©º", {'total': 0, 'inserted': 0, 'skipped': 0, 'errors': 0}

        # éªŒè¯å¿…éœ€çš„åˆ—
        required_columns = ['date', 'repo', 'model_name', 'publisher', 'download_count']
        missing_columns = [col for col in required_columns if col not in df.columns]

        if missing_columns:
            return False, f"Excel æ–‡ä»¶ç¼ºå°‘å¿…éœ€çš„åˆ—: {', '.join(missing_columns)}", \
                   {'total': len(df), 'inserted': 0, 'skipped': 0, 'errors': 0}

        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()

        stats = {
            'total': len(df),
            'inserted': 0,
            'skipped': 0,
            'errors': 0
        }

        error_details = []

        for idx, row in df.iterrows():
            try:
                # æå–æ•°æ®
                date = str(row['date']) if pd.notna(row['date']) else None
                repo = str(row['repo']) if pd.notna(row['repo']) else None
                model_name = str(row['model_name']) if pd.notna(row['model_name']) else None
                publisher = str(row['publisher']) if pd.notna(row['publisher']) else None
                download_count = int(row['download_count']) if pd.notna(row['download_count']) else 0

                # å¯é€‰å­—æ®µ
                base_model = str(row['base_model']) if 'base_model' in row and pd.notna(row['base_model']) else None
                model_type = str(row['model_type']) if 'model_type' in row and pd.notna(row['model_type']) else None
                model_category = str(row['model_category']) if 'model_category' in row and pd.notna(row['model_category']) else None

                # éªŒè¯å¿…å¡«å­—æ®µ
                if not all([date, repo, model_name, publisher]):
                    stats['errors'] += 1
                    error_details.append(f"ç¬¬ {idx + 2} è¡Œ: å¿…å¡«å­—æ®µä¸èƒ½ä¸ºç©º")
                    continue

                # æ ¼å¼åŒ–æ—¥æœŸ
                if isinstance(date, str):
                    # å°è¯•è§£ææ—¥æœŸ
                    try:
                        # å¦‚æœæ˜¯æ—¶é—´æˆ³æ ¼å¼
                        if date.isdigit():
                            date = datetime.fromtimestamp(int(date) / 1000).strftime('%Y-%m-%d')
                        else:
                            # å°è¯•è§£æå¸¸è§æ—¥æœŸæ ¼å¼
                            parsed_date = pd.to_datetime(date)
                            date = parsed_date.strftime('%Y-%m-%d')
                    except:
                        pass

                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                cursor.execute(f"""
                    SELECT COUNT(*) FROM {DATA_TABLE}
                    WHERE date = ? AND repo = ? AND publisher = ? AND model_name = ?
                """, (date, repo, publisher, model_name))

                exists = cursor.fetchone()[0] > 0

                if exists:
                    if skip_duplicates:
                        stats['skipped'] += 1
                        continue
                    else:
                        # è¦†ç›–æ¨¡å¼ï¼šå…ˆåˆ é™¤æ—§è®°å½•
                        cursor.execute(f"""
                            DELETE FROM {DATA_TABLE}
                            WHERE date = ? AND repo = ? AND publisher = ? AND model_name = ?
                        """, (date, repo, publisher, model_name))

                # æ’å…¥è®°å½•
                cursor.execute(f"""
                    INSERT INTO {DATA_TABLE}
                    (date, repo, model_name, publisher, download_count, base_model, model_type, model_category)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                """, (date, repo, model_name, publisher, download_count, base_model, model_type, model_category))

                stats['inserted'] += 1

            except Exception as e:
                stats['errors'] += 1
                error_details.append(f"ç¬¬ {idx + 2} è¡Œ: {str(e)}")

        conn.commit()
        conn.close()

        # æ„å»ºç»“æœæ¶ˆæ¯
        message = f"å¯¼å…¥å®Œæˆï¼\n"
        message += f"- æ€»è®°å½•æ•°: {stats['total']}\n"
        message += f"- æˆåŠŸæ’å…¥: {stats['inserted']}\n"
        message += f"- è·³è¿‡é‡å¤: {stats['skipped']}\n"
        message += f"- é”™è¯¯è®°å½•: {stats['errors']}"

        if error_details and len(error_details) <= 10:
            message += f"\n\né”™è¯¯è¯¦æƒ…:\n" + "\n".join(error_details)
        elif error_details:
            message += f"\n\né”™è¯¯è¯¦æƒ…ï¼ˆå‰10æ¡ï¼‰:\n" + "\n".join(error_details[:10])
            message += f"\n... è¿˜æœ‰ {len(error_details) - 10} æ¡é”™è¯¯"

        success = stats['inserted'] > 0 or (stats['skipped'] > 0 and stats['errors'] == 0)

        return success, message, stats

    except Exception as e:
        return False, f"å¯¼å…¥å¤±è´¥: {str(e)}", {'total': 0, 'inserted': 0, 'skipped': 0, 'errors': 0}


def search_records(date_filter=None, repo_filter=None, model_name_filter=None,
                   publisher_filter=None, limit=100):
    """
    æœç´¢æ•°æ®åº“è®°å½•

    Args:
        date_filter: æ—¥æœŸè¿‡æ»¤ï¼ˆYYYY-MM-DDï¼‰
        repo_filter: å¹³å°è¿‡æ»¤
        model_name_filter: æ¨¡å‹åç§°è¿‡æ»¤ï¼ˆæ”¯æŒæ¨¡ç³Šæœç´¢ï¼‰
        publisher_filter: å‘å¸ƒè€…è¿‡æ»¤ï¼ˆæ”¯æŒæ¨¡ç³Šæœç´¢ï¼‰
        limit: è¿”å›è®°å½•æ•°ä¸Šé™

    Returns:
        DataFrame: æœç´¢ç»“æœï¼ŒåŒ…å« rowid
    """
    try:
        conn = sqlite3.connect(DB_PATH)

        # æ„å»ºæŸ¥è¯¢
        query = f"SELECT rowid, * FROM {DATA_TABLE}"
        conditions = []
        params = []

        if date_filter:
            conditions.append("date = ?")
            params.append(date_filter)

        if repo_filter:
            conditions.append("repo = ?")
            params.append(repo_filter)

        if model_name_filter:
            conditions.append("model_name LIKE ?")
            params.append(f"%{model_name_filter}%")

        if publisher_filter:
            conditions.append("publisher LIKE ?")
            params.append(f"%{publisher_filter}%")

        if conditions:
            query += " WHERE " + " AND ".join(conditions)

        query += f" ORDER BY date DESC, repo, model_name LIMIT {limit}"

        df = pd.read_sql_query(query, conn, params=params)
        conn.close()

        return df

    except Exception as e:
        print(f"æœç´¢è®°å½•å¤±è´¥: {e}")
        return pd.DataFrame()


def get_record_by_rowid(rowid):
    """
    æ ¹æ® rowid è·å–å•æ¡è®°å½•

    Args:
        rowid: è®°å½•çš„ rowid

    Returns:
        dict: è®°å½•æ•°æ®ï¼Œå¦‚æœæœªæ‰¾åˆ°è¿”å› None
    """
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()

        cursor.execute(f"SELECT rowid, * FROM {DATA_TABLE} WHERE rowid = ?", (rowid,))
        row = cursor.fetchone()

        if row:
            # åŠ¨æ€è·å–åˆ—å
            cursor.execute(f"PRAGMA table_info({DATA_TABLE})")
            table_columns = [col[1] for col in cursor.fetchall()]

            # æ·»åŠ  rowid åˆ°åˆ—ååˆ—è¡¨çš„å¼€å¤´
            columns = ['rowid'] + table_columns

            conn.close()
            return dict(zip(columns, row))

        conn.close()
        return None

    except Exception as e:
        print(f"è·å–è®°å½•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def update_record(rowid, date=None, repo=None, model_name=None, publisher=None,
                 download_count=None, base_model=None, model_type=None,
                 model_category=None, tags=None):
    """
    æ›´æ–°æ•°æ®åº“è®°å½•

    Args:
        rowid: è¦æ›´æ–°çš„è®°å½•çš„ rowid
        date: æ—¥æœŸï¼ˆå¯é€‰ï¼‰
        repo: å¹³å°ï¼ˆå¯é€‰ï¼‰
        model_name: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
        publisher: å‘å¸ƒè€…ï¼ˆå¯é€‰ï¼‰
        download_count: ä¸‹è½½é‡ï¼ˆå¯é€‰ï¼‰
        base_model: åŸºç¡€æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
        model_type: æ¨¡å‹ç±»å‹ï¼ˆå¯é€‰ï¼‰
        model_category: æ¨¡å‹åˆ†ç±»ï¼ˆå¯é€‰ï¼‰
        tags: æ ‡ç­¾ï¼ˆå¯é€‰ï¼‰

    Returns:
        tuple: (success, message)
    """
    try:
        # è·å–ç°æœ‰è®°å½•
        existing = get_record_by_rowid(rowid)
        if not existing:
            return False, f"æœªæ‰¾åˆ° rowid={rowid} çš„è®°å½•"

        # æ„å»ºæ›´æ–°å­—æ®µ
        updates = []
        params = []

        if date is not None:
            # éªŒè¯æ—¥æœŸæ ¼å¼
            try:
                datetime.strptime(date, '%Y-%m-%d')
                updates.append("date = ?")
                params.append(date)
            except ValueError:
                return False, "æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œåº”ä¸º YYYY-MM-DD"

        if repo is not None:
            updates.append("repo = ?")
            params.append(repo)

        if model_name is not None:
            updates.append("model_name = ?")
            params.append(model_name)

        if publisher is not None:
            updates.append("publisher = ?")
            params.append(publisher)

        if download_count is not None:
            # éªŒè¯ä¸‹è½½é‡
            try:
                download_count = int(download_count)
                if download_count < 0:
                    return False, "ä¸‹è½½é‡ä¸èƒ½ä¸ºè´Ÿæ•°"
                updates.append("download_count = ?")
                params.append(download_count)
            except (ValueError, TypeError):
                return False, "ä¸‹è½½é‡å¿…é¡»æ˜¯æ•°å­—"

        if base_model is not None:
            updates.append("base_model = ?")
            params.append(base_model if base_model else None)

        if model_type is not None:
            updates.append("model_type = ?")
            params.append(model_type if model_type else None)

        if model_category is not None:
            updates.append("model_category = ?")
            params.append(model_category if model_category else None)

        if tags is not None:
            updates.append("tags = ?")
            params.append(tags if tags else None)

        if not updates:
            return False, "æ²¡æœ‰éœ€è¦æ›´æ–°çš„å­—æ®µ"

        # æ‰§è¡Œæ›´æ–°
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()

        query = f"UPDATE {DATA_TABLE} SET {', '.join(updates)} WHERE rowid = ?"
        params.append(rowid)

        cursor.execute(query, params)
        conn.commit()
        conn.close()

        return True, f"æˆåŠŸæ›´æ–°è®°å½• (rowid={rowid})"

    except Exception as e:
        return False, f"æ›´æ–°å¤±è´¥: {str(e)}"


def delete_record_by_rowid(rowid):
    """
    æ ¹æ® rowid åˆ é™¤è®°å½•

    Args:
        rowid: è¦åˆ é™¤çš„è®°å½•çš„ rowid

    Returns:
        tuple: (success, message)
    """
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()

        # æ£€æŸ¥è®°å½•æ˜¯å¦å­˜åœ¨
        cursor.execute(f"SELECT COUNT(*) FROM {DATA_TABLE} WHERE rowid = ?", (rowid,))
        if cursor.fetchone()[0] == 0:
            conn.close()
            return False, f"æœªæ‰¾åˆ° rowid={rowid} çš„è®°å½•"

        # åˆ é™¤è®°å½•
        cursor.execute(f"DELETE FROM {DATA_TABLE} WHERE rowid = ?", (rowid,))
        conn.commit()
        conn.close()

        return True, f"æˆåŠŸåˆ é™¤è®°å½• (rowid={rowid})"

    except Exception as e:
        return False, f"åˆ é™¤å¤±è´¥: {str(e)}"
