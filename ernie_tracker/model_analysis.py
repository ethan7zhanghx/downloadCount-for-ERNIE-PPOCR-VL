"""
æ¨¡å‹åˆ†ææ¨¡å— - æ¨æ–­ base_model å’Œç»Ÿè®¡è¡ç”Ÿç”Ÿæ€
"""
import re
import pandas as pd
from typing import Dict, List, Optional, Tuple
from collections import defaultdict


# å®˜æ–¹æ¨¡å‹åˆ†ç»„é…ç½®
OFFICIAL_MODEL_GROUPS = {
    'ERNIE-4.5-0.3B': {
        'models': [
            'baidu/ERNIE-4.5-0.3B-PT',
            'baidu/ERNIE-4.5-0.3B-Base-PT',
            'baidu/ERNIE-4.5-0.3B-Paddle',
            'baidu/ERNIE-4.5-0.3B-Base-Paddle',
        ],
        'pattern': r'ERNIE-4\.5-0\.3B(?!.*Thinking)',
    },
    'ERNIE-4.5-21B-A3B': {
        'models': [
            'baidu/ERNIE-4.5-21B-A3B-PT',
            'baidu/ERNIE-4.5-21B-A3B-Base-PT',
            'baidu/ERNIE-4.5-21B-A3B-Paddle',
            'baidu/ERNIE-4.5-21B-A3B-Base-Paddle',
        ],
        'pattern': r'ERNIE-4\.5-21B-A3B(?!.*Thinking)',
    },
    'ERNIE-4.5-21B-A3B-Thinking': {
        'models': [
            'baidu/ERNIE-4.5-21B-A3B-Thinking',
        ],
        'pattern': r'ERNIE-4\.5-21B-A3B-Thinking',
    },
    'ERNIE-4.5-VL-28B-A3B': {
        'models': [
            'baidu/ERNIE-4.5-VL-28B-A3B-PT',
            'baidu/ERNIE-4.5-VL-28B-A3B-Base-PT',
            'baidu/ERNIE-4.5-VL-28B-A3B-Paddle',
            'baidu/ERNIE-4.5-VL-28B-A3B-Base-Paddle',
        ],
        'pattern': r'ERNIE-4\.5-VL-28B-A3B(?!.*Thinking)',
    },
    'ERNIE-4.5-VL-28B-A3B-Thinking': {
        'models': [
            'baidu/ERNIE-4.5-VL-28B-A3B-Thinking',
        ],
        'pattern': r'ERNIE-4\.5-VL-28B-A3B-Thinking',
    },
    'ERNIE-4.5-300B-A47B': {
        'models': [
            'baidu/ERNIE-4.5-300B-A47B-PT',
            'baidu/ERNIE-4.5-300B-A47B-Base-PT',
            'baidu/ERNIE-4.5-300B-A47B-Paddle',
            'baidu/ERNIE-4.5-300B-A47B-Base-Paddle',
            'baidu/ERNIE-4.5-300B-A47B-FP8-Paddle',
            'baidu/ERNIE-4.5-300B-A47B-2Bits-Paddle',
            'baidu/ERNIE-4.5-300B-A47B-W4A8C8-TP4-Paddle',
        ],
        'pattern': r'ERNIE-4\.5-300B-A47B',
    },
    'ERNIE-4.5-VL-424B-A47B': {
        'models': [
            'baidu/ERNIE-4.5-VL-424B-A47B-PT',
            'baidu/ERNIE-4.5-VL-424B-A47B-Base-PT',
            'baidu/ERNIE-4.5-VL-424B-A47B-Paddle',
            'baidu/ERNIE-4.5-VL-424B-A47B-Base-Paddle',
        ],
        'pattern': r'ERNIE-4\.5-VL-424B-A47B',
    },
    'PaddleOCR-VL': {
        'models': [
            'PaddlePaddle/PaddleOCR-VL',
        ],
        'pattern': r'PaddleOCR-VL',
    },
}


# æ„å»ºå®˜æ–¹ base_model çš„å¤§å°å†™æ— å…³æ˜ å°„ï¼Œä¾¿äºæ ‡å‡†åŒ–
CANONICAL_BASE_MODEL_MAP = {}
for group_info in OFFICIAL_MODEL_GROUPS.values():
    for model_id in group_info['models']:
        CANONICAL_BASE_MODEL_MAP[model_id.lower()] = model_id
        CANONICAL_BASE_MODEL_MAP[model_id.split('/')[-1].lower()] = model_id


def normalize_base_models(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, int]]:
    """
    æ ‡å‡†åŒ– base_modelï¼Œä¿®å¤ PaddleOCR-VL é”™å½’åˆ° ERNIE çš„é—®é¢˜

    - ç»Ÿä¸€å¤§å°å†™ã€è¡¥å…¨ publisher å‰ç¼€
    - å¯¹å®˜æ–¹åŸå§‹æ¨¡å‹æ¸…ç©º base_modelï¼Œé¿å…è¢«å½“æˆè¡ç”Ÿ
    - å°† PaddleOCR-VL ç›¸å…³æ¨¡å‹çš„ base_model å½’ä¸€åˆ°å®˜æ–¹ ID
    """
    normalized_df = df.copy()
    stats = {'canonicalized': 0, 'cleared_original': 0, 'paddleocr_fixed': 0}

    if 'base_model' not in normalized_df.columns:
        normalized_df['base_model'] = None
        return normalized_df, stats

    def clean_value(val):
        if pd.isna(val):
            return None
        val_str = str(val).strip()
        return None if val_str.lower() in ['', 'none', 'nan'] else val_str

    normalized_df['base_model'] = normalized_df['base_model'].apply(clean_value)

    # å®˜æ–¹åŸå§‹æ¨¡å‹ä¸åº”å¸¦ base_modelï¼Œé¿å…è®¡å…¥è¡ç”Ÿ
    if 'model_type' in normalized_df.columns:
        cleared_mask = (normalized_df['model_type'] == 'original') & normalized_df['base_model'].notna()
        stats['cleared_original'] += int(cleared_mask.sum())
        normalized_df.loc[normalized_df['model_type'] == 'original', 'base_model'] = None

    if 'data_source' in normalized_df.columns:
        source_mask = (normalized_df['data_source'] == 'original') & normalized_df['base_model'].notna()
        stats['cleared_original'] += int(source_mask.sum())
        normalized_df.loc[normalized_df['data_source'] == 'original', 'base_model'] = None

    def canonicalize(val):
        if not val:
            return None
        lower = val.lower()
        canonical = CANONICAL_BASE_MODEL_MAP.get(lower)
        if not canonical:
            bare = lower.split('/')[-1]
            canonical = CANONICAL_BASE_MODEL_MAP.get(bare)
        if canonical and canonical != val:
            stats['canonicalized'] += 1
            return canonical
        return val

    normalized_df['base_model'] = normalized_df['base_model'].apply(canonicalize)

    # ä¸“é—¨ä¿®å¤ PaddleOCR-VL è¢«è¯¯åˆ¤æˆ ERNIE-4.5 çš„æƒ…å†µ
    paddle_base = OFFICIAL_MODEL_GROUPS['PaddleOCR-VL']['models'][0]

    def is_paddleocr(row):
        name = str(row.get('model_name', '')).lower()
        category = str(row.get('model_category', '')).lower()
        base = str(row.get('base_model') or '').lower()
        publisher = str(row.get('publisher', '')).lower()
        return (
            'paddleocr-vl' in name
            or 'paddleocr-vl' in category
            or 'paddleocr-vl' in base
            or publisher == 'paddleocr-vl'
        )

    paddle_mask = normalized_df.apply(is_paddleocr, axis=1)
    base_col = normalized_df['base_model'].fillna('')
    wrong_base_mask = base_col.str.contains('ernie-4.5', case=False, na=False) | base_col.str.fullmatch('paddleocr-vl', case=False, na=False)
    fix_mask = paddle_mask & (normalized_df['base_model'].isna() | wrong_base_mask)
    if 'model_type' in normalized_df.columns:
        fix_mask = fix_mask & (normalized_df['model_type'] != 'original')
    if 'data_source' in normalized_df.columns:
        fix_mask = fix_mask & (normalized_df['data_source'] != 'original')
    stats['paddleocr_fixed'] = int(fix_mask.sum())
    normalized_df.loc[fix_mask, 'base_model'] = paddle_base

    return normalized_df, stats


def infer_base_model_from_name(model_name: str, publisher: str, full_model_id: str = None) -> Optional[str]:
    """
    æ ¹æ®æ¨¡å‹åç§°æ¨æ–­ base_model

    Args:
        model_name: æ¨¡å‹åç§°ï¼ˆä¸å«publisherï¼‰
        publisher: å‘å¸ƒè€…
        full_model_id: å®Œæ•´æ¨¡å‹IDï¼ˆpublisher/model_nameï¼‰ï¼Œå¯é€‰

    Returns:
        æ¨æ–­å‡ºçš„ base_modelï¼Œå¦‚æœæ— æ³•æ¨æ–­åˆ™è¿”å› None
    """
    if full_model_id is None:
        full_model_id = f"{publisher}/{model_name}"

    # å¦‚æœæ˜¯å®˜æ–¹æ¨¡å‹ï¼Œä¸éœ€è¦æ¨æ–­
    if publisher.lower() in ['baidu', 'paddlepaddle']:
        return None

    # å°†æ¨¡å‹åç§°è½¬ä¸ºå°å†™ä¾¿äºåŒ¹é…
    model_lower = full_model_id.lower()

    # æŒ‰ä¼˜å…ˆçº§åŒ¹é…ï¼ˆä»å…·ä½“åˆ°ä¸€èˆ¬ï¼‰
    # 1. å…ˆåŒ¹é… Thinking æ¨¡å‹ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
    if 'thinking' in model_lower:
        if '21b' in model_lower or '21-b' in model_lower:
            return 'baidu/ERNIE-4.5-21B-A3B-Thinking'
        elif 'vl' in model_lower and ('28b' in model_lower or '28-b' in model_lower):
            return 'baidu/ERNIE-4.5-VL-28B-A3B-Thinking'

    # 2. åŒ¹é…å…¶ä»–å°ºå¯¸æ¨¡å‹
    # VL-424B-A47B
    if 'vl' in model_lower and ('424b' in model_lower or '424-b' in model_lower):
        # ä¼˜å…ˆåŒ¹é… PT ç‰ˆæœ¬
        if 'paddle' in model_lower:
            return 'baidu/ERNIE-4.5-VL-424B-A47B-Paddle'
        else:
            return 'baidu/ERNIE-4.5-VL-424B-A47B-PT'

    # 300B-A47B
    if '300b' in model_lower or '300-b' in model_lower:
        if 'paddle' in model_lower:
            return 'baidu/ERNIE-4.5-300B-A47B-Paddle'
        else:
            return 'baidu/ERNIE-4.5-300B-A47B-PT'

    # VL-28B-A3B (éThinking)
    if 'vl' in model_lower and ('28b' in model_lower or '28-b' in model_lower):
        if 'paddle' in model_lower:
            return 'baidu/ERNIE-4.5-VL-28B-A3B-Paddle'
        else:
            return 'baidu/ERNIE-4.5-VL-28B-A3B-PT'

    # 21B-A3B (éThinking)
    if '21b' in model_lower or '21-b' in model_lower:
        if 'paddle' in model_lower:
            return 'baidu/ERNIE-4.5-21B-A3B-Paddle'
        else:
            return 'baidu/ERNIE-4.5-21B-A3B-PT'

    # 0.3B
    if '0.3b' in model_lower or '0-3b' in model_lower or '300m' in model_lower:
        if 'paddle' in model_lower:
            return 'baidu/ERNIE-4.5-0.3B-Paddle'
        else:
            return 'baidu/ERNIE-4.5-0.3B-PT'

    # PaddleOCR-VL
    if 'paddleocr' in model_lower and 'vl' in model_lower:
        return 'PaddlePaddle/PaddleOCR-VL'

    # æ— æ³•æ¨æ–­
    return None


def get_model_group(base_model: str) -> Optional[str]:
    """
    æ ¹æ® base_model ç¡®å®šå®ƒå±äºå“ªä¸ªåˆ†ç»„

    Args:
        base_model: åŸºç¡€æ¨¡å‹IDï¼ˆå¦‚ 'baidu/ERNIE-4.5-21B-A3B-PT'ï¼‰

    Returns:
        åˆ†ç»„åç§°ï¼Œå¦‚æœä¸å±äºä»»ä½•åˆ†ç»„åˆ™è¿”å› None
    """
    if not base_model:
        return None

    for group_name, group_info in OFFICIAL_MODEL_GROUPS.items():
        if base_model in group_info['models']:
            return group_name

    return None


def analyze_derivative_ecosystem(df: pd.DataFrame, infer_missing: bool = True) -> Dict:
    """
    åˆ†æè¡ç”Ÿæ¨¡å‹ç”Ÿæ€

    Args:
        df: åŒ…å«æ¨¡å‹æ•°æ®çš„ DataFrameï¼ˆå¿…é¡»åŒ…å« base_model, model_type åˆ—ï¼‰
        infer_missing: æ˜¯å¦æ¨æ–­ç¼ºå¤±çš„ base_model

    Returns:
        åˆ†æç»“æœå­—å…¸
    """
    # å¤åˆ¶æ•°æ®é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
    analysis_df, normalization_stats = normalize_base_models(df)

    if any(normalization_stats.values()):
        print(
            f"ğŸ”§ æ ‡å‡†åŒ– base_model | "
            f"æ¸…ç†å®˜æ–¹: {normalization_stats['cleared_original']} | "
            f"IDå½’ä¸€: {normalization_stats['canonicalized']} | "
            f"PaddleOCRä¿®æ­£: {normalization_stats['paddleocr_fixed']}"
        )

    # 1. æ¨æ–­ç¼ºå¤±çš„ base_model
    if infer_missing:
        print("ğŸ” æ¨æ–­ç¼ºå¤±çš„ base_model...")
        inferred_count = 0

        for idx, row in analysis_df.iterrows():
            # åªå¤„ç†æ²¡æœ‰ base_model çš„è®°å½•
            if pd.isna(row.get('base_model')) or not row.get('base_model'):
                model_name = row.get('model_name', '')
                publisher = row.get('publisher', '')

                # æ¨æ–­ base_model
                inferred_base = infer_base_model_from_name(model_name, publisher)

                if inferred_base:
                    analysis_df.at[idx, 'base_model'] = inferred_base
                    analysis_df.at[idx, 'base_model_inferred'] = True
                    inferred_count += 1

        print(f"  âœ… æˆåŠŸæ¨æ–­ {inferred_count} ä¸ªæ¨¡å‹çš„ base_model")

    # 2. æŒ‰åˆ†ç»„ç»Ÿè®¡
    print("\nğŸ“Š æŒ‰åˆ†ç»„ç»Ÿè®¡è¡ç”Ÿç”Ÿæ€...")

    # è¿‡æ»¤å‡ºæœ‰ base_model çš„è®°å½•ï¼ˆè¡ç”Ÿæ¨¡å‹ï¼‰
    derivatives = analysis_df[
        analysis_df['base_model'].notna() &
        (analysis_df['base_model'] != '') &
        (analysis_df['base_model'] != 'None')
    ].copy()

    print(f"  âœ… å…±æœ‰ {len(derivatives)} ä¸ªè¡ç”Ÿæ¨¡å‹")

    # æ·»åŠ åˆ†ç»„ä¿¡æ¯
    derivatives['model_group'] = derivatives['base_model'].apply(get_model_group)

    # ç»Ÿè®¡ç»“æœ
    results = {}

    for group_name in OFFICIAL_MODEL_GROUPS.keys():
        group_derivatives = derivatives[derivatives['model_group'] == group_name]

        if len(group_derivatives) == 0:
            results[group_name] = {
                'total': 0,
                'by_type': {},
                'by_data_source': {},
                'models': []
            }
            continue

        # æŒ‰ç±»å‹ç»Ÿè®¡
        type_counts = group_derivatives['model_type'].value_counts().to_dict()

        # æŒ‰æ•°æ®æ¥æºç»Ÿè®¡ï¼ˆå¦‚æœæœ‰ data_source åˆ—ï¼‰
        source_counts = {}
        if 'data_source' in group_derivatives.columns:
            source_counts = group_derivatives['data_source'].value_counts().to_dict()

        # è·å–æ ·æœ¬æ¨¡å‹
        sample_models = group_derivatives[['model_name', 'publisher', 'base_model', 'model_type', 'download_count']].head(10).to_dict('records')

        results[group_name] = {
            'total': len(group_derivatives),
            'by_type': type_counts,
            'by_data_source': source_counts,
            'models': sample_models,
            'base_models': group_derivatives['base_model'].unique().tolist()
        }

    # 3. æ€»ä½“ç»Ÿè®¡
    summary = {
        'total_derivatives': len(derivatives),
        'total_inferred': inferred_count if infer_missing else 0,
        'by_group': results,
        'overall_by_type': derivatives['model_type'].value_counts().to_dict(),
    }

    return summary


def print_analysis_report(analysis_result: Dict):
    """
    æ‰“å°åˆ†ææŠ¥å‘Š

    Args:
        analysis_result: analyze_derivative_ecosystem() çš„è¿”å›ç»“æœ
    """
    print("\n" + "="*80)
    print("ğŸ“Š ERNIE-4.5 è¡ç”Ÿæ¨¡å‹ç”Ÿæ€åˆ†ææŠ¥å‘Š")
    print("="*80)

    print(f"\nğŸ“ˆ æ€»ä½“ç»Ÿè®¡:")
    print(f"  - è¡ç”Ÿæ¨¡å‹æ€»æ•°: {analysis_result['total_derivatives']}")
    print(f"  - æ¨æ–­çš„ base_model: {analysis_result['total_inferred']}")

    print(f"\nğŸ“Š æ•´ä½“ç±»å‹åˆ†å¸ƒ:")
    for model_type, count in sorted(analysis_result['overall_by_type'].items(), key=lambda x: x[1], reverse=True):
        emoji = {
            'quantized': 'âš¡',
            'finetune': 'ğŸ”§',
            'adapter': 'ğŸ”Œ',
            'lora': 'ğŸ¯',
            'merge': 'ğŸ”€',
            'other': 'ğŸ“¦'
        }.get(model_type, 'ğŸ“¦')
        print(f"  {emoji} {model_type}: {count} ä¸ª")

    print(f"\n" + "="*80)
    print("ğŸ“‹ å„åˆ†ç»„è¯¦ç»†ç»Ÿè®¡")
    print("="*80)

    for group_name, group_data in analysis_result['by_group'].items():
        print(f"\nğŸ·ï¸  {group_name}")
        print(f"  æ€»è®¡: {group_data['total']} ä¸ªè¡ç”Ÿæ¨¡å‹")

        if group_data['total'] > 0:
            print(f"\n  æŒ‰ç±»å‹åˆ†å¸ƒ:")
            for model_type, count in sorted(group_data['by_type'].items(), key=lambda x: x[1], reverse=True):
                emoji = {
                    'quantized': 'âš¡',
                    'finetune': 'ğŸ”§',
                    'adapter': 'ğŸ”Œ',
                    'lora': 'ğŸ¯',
                    'merge': 'ğŸ”€',
                    'other': 'ğŸ“¦'
                }.get(model_type, 'ğŸ“¦')
                percentage = (count / group_data['total']) * 100
                print(f"    {emoji} {model_type}: {count} ä¸ª ({percentage:.1f}%)")

            if group_data['by_data_source']:
                print(f"\n  æŒ‰æ•°æ®æ¥æºåˆ†å¸ƒ:")
                for source, count in group_data['by_data_source'].items():
                    source_label = {
                        'search': 'æœç´¢å‘ç°',
                        'model_tree': 'Model Tree',
                        'both': 'æœç´¢+Model Tree',
                        None: 'æ¨æ–­'
                    }.get(source, source)
                    print(f"    - {source_label}: {count} ä¸ª")

            print(f"\n  åŒ…å«çš„å®˜æ–¹ base_model:")
            for base_model in group_data['base_models']:
                print(f"    - {base_model}")

            print(f"\n  æ ·æœ¬æ¨¡å‹ï¼ˆå‰5ä¸ªï¼‰:")
            for i, model in enumerate(group_data['models'][:5], 1):
                print(f"    {i}. {model['publisher']}/{model['model_name']}")
                downloads = model.get('download_count', 0)
                downloads_int = int(downloads) if pd.notna(downloads) else 0
                print(f"       ç±»å‹: {model['model_type']} | base: {model['base_model']} | ä¸‹è½½: {downloads_int:,}")

        print()


def export_analysis_to_excel(analysis_result: Dict, df: pd.DataFrame, output_path: str):
    """
    å¯¼å‡ºåˆ†æç»“æœåˆ° Excelï¼ˆå¤šä¸ªsheetï¼‰

    Args:
        analysis_result: åˆ†æç»“æœ
        df: åŸå§‹æ•°æ®
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
        # Sheet 1: æ€»ä½“ç»Ÿè®¡
        summary_data = {
            'æŒ‡æ ‡': ['è¡ç”Ÿæ¨¡å‹æ€»æ•°', 'æ¨æ–­çš„base_modelæ•°é‡'],
            'æ•°å€¼': [analysis_result['total_derivatives'], analysis_result['total_inferred']]
        }
        pd.DataFrame(summary_data).to_excel(writer, sheet_name='æ€»ä½“ç»Ÿè®¡', index=False)

        # Sheet 2: å„åˆ†ç»„ç»Ÿè®¡
        group_stats = []
        for group_name, group_data in analysis_result['by_group'].items():
            for model_type, count in group_data['by_type'].items():
                group_stats.append({
                    'åˆ†ç»„': group_name,
                    'æ¨¡å‹ç±»å‹': model_type,
                    'æ•°é‡': count,
                    'å æ¯”': f"{(count / group_data['total'] * 100):.1f}%" if group_data['total'] > 0 else "0%"
                })

        if group_stats:
            pd.DataFrame(group_stats).to_excel(writer, sheet_name='åˆ†ç»„ç»Ÿè®¡', index=False)

        # Sheet 3-N: æ¯ä¸ªåˆ†ç»„çš„è¯¦ç»†æ¨¡å‹åˆ—è¡¨
        derivatives = df[
            df['base_model'].notna() &
            (df['base_model'] != '') &
            (df['base_model'] != 'None')
        ].copy()

        derivatives['model_group'] = derivatives['base_model'].apply(get_model_group)

        for group_name in OFFICIAL_MODEL_GROUPS.keys():
            group_derivatives = derivatives[derivatives['model_group'] == group_name]

            if len(group_derivatives) > 0:
                # é€‰æ‹©é‡è¦åˆ—
                export_cols = ['model_name', 'publisher', 'base_model', 'model_type',
                             'download_count', 'data_source', 'model_category']
                available_cols = [col for col in export_cols if col in group_derivatives.columns]

                sheet_df = group_derivatives[available_cols].sort_values('download_count', ascending=False)

                # Excel sheet åç§°é•¿åº¦é™åˆ¶ä¸º31
                sheet_name = group_name[:28] + '...' if len(group_name) > 31 else group_name
                sheet_df.to_excel(writer, sheet_name=sheet_name, index=False)

    print(f"\nâœ… åˆ†æç»“æœå·²å¯¼å‡ºåˆ°: {output_path}")


if __name__ == "__main__":
    # æµ‹è¯•æ¨æ–­åŠŸèƒ½
    print("ğŸ§ª æµ‹è¯• base_model æ¨æ–­åŠŸèƒ½:\n")

    test_cases = [
        ("baidu_ERNIE-4.5-21B-A3B-PT-GGUF", "bartowski"),
        ("ERNIE-4.5-21B-A3B-Thinking-GGUF", "unsloth"),
        ("ERNIE-4.5-0.3B-PT-GGUF", "lmstudio-community"),
        ("ERNIE-4.5-VL-28B-A3B-Thinking-GGUF", "gabriellarson"),
        ("ERNIE-4.5-300B-A47B-PT-GGUF", "unsloth"),
        ("some-random-model", "user123"),
    ]

    for model_name, publisher in test_cases:
        inferred = infer_base_model_from_name(model_name, publisher)
        print(f"  {publisher}/{model_name}")
        print(f"    â†’ {inferred if inferred else 'æ— æ³•æ¨æ–­'}\n")
