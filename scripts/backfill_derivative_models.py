#!/usr/bin/env python3
"""
å›å¡«è¡ç”Ÿæ¨¡å‹æ•°æ®åˆ°åˆ›å»ºæ—¥æœŸ

å¯¹äº2026-01-16è·å–çš„AI Studioå’ŒModelScopeçš„æ–°å¢è¡ç”Ÿæ¨¡å‹ï¼ˆæ­¤å‰æœªè·å–è¿‡çš„ï¼‰ï¼Œ
ä¼˜å…ˆä½¿ç”¨created_atï¼Œå¤‡é€‰last_modifiedï¼Œå°†æ•°æ®è®°å½•å›å¡«åˆ°æ¨¡å‹çš„åˆ›å»ºæ—¥æœŸã€‚

é¿å…ç»Ÿè®¡å½“å‘¨æ–°å¢æ—¶è®°å½•è¿‡å¤šçš„æ¨¡å‹ã€‚
"""

import sqlite3
import sys
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from ernie_tracker.config import DB_PATH, DATA_TABLE


def backfill_derivative_models():
    """å›å¡«è¡ç”Ÿæ¨¡å‹æ•°æ®åˆ°åˆ›å»ºæ—¥æœŸ"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()

    target_date = '2026-01-16'
    target_repos = ['AI Studio', 'ModelScope']
    target_categories = ['ernie-4.5', 'paddleocr-vl']

    print("ğŸš€ å¼€å§‹å›å¡«è¡ç”Ÿæ¨¡å‹æ•°æ®...")
    print(f"ç›®æ ‡æ—¥æœŸ: {target_date}")
    print(f"ç›®æ ‡å¹³å°: {', '.join(target_repos)}")
    print(f"ç›®æ ‡åˆ†ç±»: {', '.join(target_categories)}")
    print()

    # Step 1: é¦–å…ˆè¿è¡Œ backfill_model_category ç¡®ä¿ model_category å­—æ®µå·²å¡«å……
    print("Step 1: æ£€æŸ¥å¹¶å¡«å…… model_category å­—æ®µ...")
    try:
        from scripts.backfill_model_category import backfill_model_category as fill_category
        fill_category()
        print("âœ… model_category å­—æ®µæ£€æŸ¥å®Œæˆ\n")
    except Exception as e:
        print(f"âš ï¸  å¡«å…… model_category æ—¶å‡ºé”™: {e}")
        print("ç»§ç»­æ‰§è¡Œ...\n")

    # Step 2: è·å–åœ¨ target_date ä¹‹å‰çš„æ‰€æœ‰ (repo, model_name) ç»„åˆ
    print("Step 2: è¯†åˆ«å†å²ä¸Šå·²æœ‰çš„æ¨¡å‹...")
    cursor.execute(f"""
        SELECT DISTINCT repo, model_name
        FROM {DATA_TABLE}
        WHERE date < ?
        AND repo IN ({','.join(['?' for _ in target_repos])})
    """, [target_date] + target_repos)

    historical_models = set()
    for row in cursor.fetchall():
        historical_models.add((row[0], row[1]))

    print(f"âœ… å‘ç° {len(historical_models)} ä¸ªå†å²æ¨¡å‹\n")

    # Step 3: è·å– target_date çš„æ‰€æœ‰è¡ç”Ÿæ¨¡å‹è®°å½•ï¼ˆå»é‡ï¼‰
    print("Step 3: è·å–ç›®æ ‡æ—¥æœŸçš„è¡ç”Ÿæ¨¡å‹...")
    cursor.execute(f"""
        SELECT
            MAX(rowid) as rowid,
            repo,
            model_name,
            publisher,
            download_count,
            model_type,
            model_category,
            tags,
            base_model,
            data_source,
            likes,
            library_name,
            pipeline_tag,
            created_at,
            last_modified,
            fetched_at,
            base_model_from_api,
            search_keyword,
            url
        FROM {DATA_TABLE}
        WHERE date = ?
        AND repo IN ({','.join(['?' for _ in target_repos])})
        AND model_category IN ({','.join(['?' for _ in target_categories])})
        GROUP BY repo, model_name
    """, [target_date] + target_repos + target_categories)

    target_records = cursor.fetchall()
    print(f"âœ… ç›®æ ‡æ—¥æœŸæœ‰ {len(target_records)} ä¸ªç›¸å…³æ¨¡å‹\n")

    # Step 4: è¯†åˆ«æ–°å¢çš„è¡ç”Ÿæ¨¡å‹
    print("Step 4: è¯†åˆ«æ–°å¢çš„è¡ç”Ÿæ¨¡å‹...")
    new_derivative_models = []

    for record in target_records:
        rowid = record[0]
        repo = record[1]
        model_name = record[2]

        # æ£€æŸ¥æ˜¯å¦ä¸ºæ–°å¢æ¨¡å‹ï¼ˆä¸åœ¨å†å²è®°å½•ä¸­ï¼‰
        if (repo, model_name) not in historical_models:
            new_derivative_models.append(record)

    print(f"âœ… å‘ç° {len(new_derivative_models)} ä¸ªæ–°å¢è¡ç”Ÿæ¨¡å‹\n")

    if len(new_derivative_models) == 0:
        print("âŒ æ²¡æœ‰å‘ç°éœ€è¦å›å¡«çš„æ–°å¢è¡ç”Ÿæ¨¡å‹")
        conn.close()
        return

    # Step 5: ç¡®å®šå›å¡«æ—¥æœŸå¹¶æ’å…¥è®°å½•
    print("Step 5: å¼€å§‹å›å¡«æ•°æ®...")
    backfilled_count = 0
    skipped_count = 0

    for record in new_derivative_models:
        (
            rowid,
            repo,
            model_name,
            publisher,
            download_count,
            model_type,
            model_category,
            tags,
            base_model,
            data_source,
            likes,
            library_name,
            pipeline_tag,
            created_at,
            last_modified,
            fetched_at,
            base_model_from_api,
            search_keyword,
            url
        ) = record

        # ç¡®å®šå›å¡«æ—¥æœŸï¼šä¼˜å…ˆä½¿ç”¨ created_atï¼Œå¤‡é€‰ last_modified
        backfill_date = created_at if created_at else last_modified

        if not backfill_date:
            print(f"âš ï¸  è·³è¿‡ {repo} - {model_name}: æ²¡æœ‰åˆ›å»ºæ—¶é—´æˆ–æ›´æ–°æ—¶é—´")
            skipped_count += 1
            continue

        # ç¡®ä¿å›å¡«æ—¥æœŸåœ¨ç›®æ ‡æ—¥æœŸä¹‹å‰
        if backfill_date >= target_date:
            print(f"âš ï¸  è·³è¿‡ {repo} - {model_name}: åˆ›å»ºæ—¶é—´ {backfill_date} ä¸åœ¨ç›®æ ‡æ—¥æœŸä¹‹å‰")
            skipped_count += 1
            continue

        # æ£€æŸ¥è¯¥æ—¥æœŸæ˜¯å¦å·²æœ‰è®°å½•
        cursor.execute(f"""
            SELECT COUNT(*)
            FROM {DATA_TABLE}
            WHERE date = ? AND repo = ? AND model_name = ?
        """, [backfill_date, repo, model_name])

        if cursor.fetchone()[0] > 0:
            print(f"âš ï¸  è·³è¿‡ {repo} - {model_name}: æ—¥æœŸ {backfill_date} å·²æœ‰è®°å½•")
            skipped_count += 1
            continue

        # æ’å…¥å›å¡«è®°å½•ï¼ˆä¸‹è½½é‡ä¸º0ï¼‰
        cursor.execute(f"""
            INSERT INTO {DATA_TABLE} (
                date, repo, model_name, publisher, download_count,
                model_type, model_category, tags, base_model, data_source,
                likes, library_name, pipeline_tag,
                created_at, last_modified, fetched_at,
                base_model_from_api, search_keyword, url
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, [
            backfill_date, repo, model_name, publisher, '0',
            model_type, model_category, tags, base_model, data_source,
            likes, library_name, pipeline_tag,
            created_at, last_modified, fetched_at,
            base_model_from_api, search_keyword, url
        ])

        backfilled_count += 1
        print(f"âœ… å›å¡«: {repo} - {model_name} -> {backfill_date}")

    # æäº¤äº‹åŠ¡
    conn.commit()

    # éªŒè¯ç»“æœ
    print("\n" + "="*60)
    print("âœ… å›å¡«å®Œæˆï¼")
    print("="*60)
    print(f"æˆåŠŸå›å¡«: {backfilled_count} æ¡è®°å½•")
    print(f"è·³è¿‡è®°å½•: {skipped_count} æ¡")

    # æ˜¾ç¤ºæŒ‰å¹³å°ç»Ÿè®¡çš„ç»“æœ
    print("\n" + "="*60)
    print("æŒ‰å¹³å°ç»Ÿè®¡å›å¡«æƒ…å†µ:")
    print("="*60)

    for repo in target_repos:
        cursor.execute(f"""
            SELECT COUNT(*)
            FROM {DATA_TABLE}
            WHERE date < ? AND repo = ? AND download_count = '0'
        """, [target_date, repo])

        zero_count = cursor.fetchone()[0]
        print(f"  {repo:20s}: {zero_count} æ¡å›å¡«è®°å½•")

    conn.close()

    print("\nâœ… æ‰€æœ‰æ“ä½œå·²å®Œæˆ")


if __name__ == '__main__':
    print("="*60)
    print("è¡ç”Ÿæ¨¡å‹æ•°æ®å›å¡«å·¥å…·")
    print("="*60)
    print()

    try:
        backfill_derivative_models()
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
